{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from transformers import BertConfig, BertTokenizerFast\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import BertForDiffusion, DiffusionLM, ConditionalDiffusionLM\n",
    "from data_utils import load_qqp_dataset_and_tokenizer_from_disk, QQPParaphraseDataset, load_split_qqp_dataset_and_tokenizer_from_disk\n",
    "from noise_schedule import get_named_beta_schedule\n",
    "from train_utils import train_conditional, evaluate_conditional\n",
    "from metric_utils import calculate_bleu, calculate_rouge\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataset args\n",
    "max_len = 32\n",
    "\n",
    "# training args\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda:1\")\n",
    "lr = 1e-4\n",
    "num_epoch = 30\n",
    "weight_decay = 0\n",
    "num_warmup_steps = 100\n",
    "\n",
    "# model args\n",
    "word_embedding_dim = 128\n",
    "# hidden_size, num_hidden_layers, num_attention_heads, intermediate_size = 768, 12, 12, 3072\n",
    "hidden_size, num_hidden_layers, num_attention_heads, intermediate_size = 512, 4, 8, 2048\n",
    "\n",
    "max_position_embeddings = max_len\n",
    "\n",
    "encoder_type = 'from-scratch'\n",
    "noise_schedule = 'sqrt'\n",
    "emb_type = 'bit'\n",
    "use_shared_weight=True\n",
    "lm_head_bias=False\n",
    "add_emb_noise=False\n",
    "self_condition=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 15672\n",
      "Training set size: 120940\n",
      "Evaluation set size: 13438\n"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset, tokenizer = load_split_qqp_dataset_and_tokenizer_from_disk(data_path=\"data\")\n",
    "\n",
    "# tokenized_qqp_train, tokenized_qqp_eval, tokenizer = load_qqp_dataset_and_tokenizer_from_disk(data_path=\"data\")\n",
    "\n",
    "rev_tokenizer = {v: k for k, v in tokenizer.items()}\n",
    "\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
    "\n",
    "# train_dataset = QQPParaphraseDataset(dataset=tokenized_qqp_train, random_swap=True)\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "# eval_dataset = QQPParaphraseDataset(dataset=tokenized_qqp_eval, random_swap=False)\n",
    "print(\"Evaluation set size:\", len(eval_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"T\": 2000,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 32,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 15672\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=len(tokenizer), hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=max_position_embeddings, pad_token_id=tokenizer['[PAD]'])\n",
    "\n",
    "config.T = 2000\n",
    "if emb_type in ['learned', 'randn']:\n",
    "    config.word_embedding_dim = word_embedding_dim\n",
    "\n",
    "print(config)\n",
    "\n",
    "betas = torch.Tensor(get_named_beta_schedule(schedule_name=noise_schedule, num_diffusion_timesteps=config.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using bit word embedding\n",
      "set word_embedding_dim to: 14\n",
      "Diffusion model #parameters:\n",
      "38886414\n",
      "Diffusion model #trainable parameters\n",
      "38886414\n"
     ]
    }
   ],
   "source": [
    "diffusion_model = ConditionalDiffusionLM(config=config, betas=betas, use_shared_weight=use_shared_weight, lm_head_bias=lm_head_bias, add_emb_noise=add_emb_noise, conditional_gen=True,self_condition=self_condition, encoder_type=encoder_type, encoder_name_or_path='bert-base-uncased', emb_type=emb_type).to(device)\n",
    "\n",
    "print(\"Diffusion model #parameters:\")\n",
    "print(sum([p.numel() for p in diffusion_model.parameters()]))\n",
    "\n",
    "print(\"Diffusion model #trainable parameters\")\n",
    "print(sum([p.numel() for p in filter(lambda p:p.requires_grad, diffusion_model.parameters())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.load_state_dict(torch.load(\"checkpoints/20221103_0634\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57d5decd77c41c3807181853949dfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.08785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0878, device='cuda:1')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_conditional(diffusion_model=diffusion_model, dataloader=eval_dataloader,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f5025b89e44963934485d3be4cb02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "generated_questions_mbr1_ddim500 = diffusion_model.generate(\n",
    "    dataset = eval_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=500,\n",
    "    eta=0,\n",
    "    mbr=1,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr1_ddim500, eval_dataset, rev_tokenizer)\n",
    "print(sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"]))\n",
    "print(sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr1_ddim500, eval_dataset, rev_tokenizer)\n",
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]\n",
    "print(sum(rouge_l_f)/len(rouge_l_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some ways to get rid off addiction to WhatsApp ?\n",
      "How do I get rid of my WhatsApp addiction ?\n",
      "How do I get rid addiction on WhatsApp ?\n"
     ]
    }
   ],
   "source": [
    "i = 84\n",
    "src_question = [rev_tokenizer[id.item()] for id in eval_dataset[i]['question1_input_ids']]\n",
    "src_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src_question))\n",
    "print(\" \".join(src_question))\n",
    "\n",
    "tgt_question = [rev_tokenizer[id.item()] for id in eval_dataset[i]['question2_input_ids']]\n",
    "tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "print(\" \".join(tgt_question))\n",
    "\n",
    "#print(\" \".join(generated_questions_mbr1_ddim200[i]))\n",
    "print(\" \".join(generated_questions_mbr10_ddim2[i]))\n",
    "#print(\" \".join(generated_questions_mbr5_ddim20[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
