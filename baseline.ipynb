{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "import random\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, BartForConditionalGeneration, BartTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/qqp_train_dict\", \"rb\") as f:\n",
    "    qqp_train_dict = pickle.load(f)\n",
    "\n",
    "with open(\"data/qqp_eval_dict\", \"rb\") as f:\n",
    "    qqp_eval_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question1_text', 'question2_text', 'question1_wordlst', 'question2_wordlst', 'question1_input_ids_bert', 'question1_token_type_ids_bert', 'question1_attention_mask_bert', 'question2_input_ids_bert', 'question2_token_type_ids_bert', 'question2_attention_mask_bert', 'question1_input_ids', 'question2_input_ids', 'question1_attention_mask', 'question2_attention_mask'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qqp_eval_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "qqp_train_dict = {'question1_text': qqp_train_dict['question1_text'],\n",
    "                  'question2_text': qqp_train_dict['question2_text']}\n",
    "qqp_eval_dict = {'question1_text': qqp_eval_dict['question1_text'],\n",
    "                  'question2_text': qqp_eval_dict['question2_text']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "qqp_train_raw = datasets.Dataset.from_dict(qqp_train_dict)\n",
    "qqp_eval_raw = datasets.Dataset.from_dict(qqp_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question1_text', 'question2_text'],\n",
       "    num_rows: 13438\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qqp_eval_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_func(examples):\n",
    "    question1 = examples['question1_text']\n",
    "    question2 = examples['question2_text']\n",
    "    tokenized_question1 = tokenizer(question1, max_length=32, padding='max_length', truncation=True,)\n",
    "    tokenized_question2 = tokenizer(question2, max_length=32, padding='max_length', truncation=True,)\n",
    "    res = {}\n",
    "    for k,v in tokenized_question1.items():\n",
    "        res['question1_'+k] = v\n",
    "    for k,v in tokenized_question2.items():\n",
    "        res['question2_'+k] = v\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407fc5a32a6546c980b666d487fe079d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10e797eea6744a2a168bdc4ca188668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qqp_train_processed = qqp_train_raw.map(preprocess_func, batched=True)\n",
    "qqp_eval_processed = qqp_eval_raw.map(preprocess_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question1_text', 'question2_text', 'question1_input_ids', 'question1_attention_mask', 'question2_input_ids', 'question2_attention_mask'],\n",
       "    num_rows: 120940\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qqp_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QQPParaphraseDataset(Dataset):\n",
    "    def __init__(self, dataset, random_swap=False):\n",
    "        \"\"\"\n",
    "        QQP paraphrase dataset using torch.utils.data.Dataset\n",
    "        :param dataset: huggingface dataset\n",
    "        :param random_swap: randomly swap question1 and question2\n",
    "        \"\"\"\n",
    "        longtensor_keys = ['question1_input_ids',\n",
    "                           'question2_input_ids',]\n",
    "        tensor_keys = ['question1_attention_mask',\n",
    "                       'question2_attention_mask']\n",
    "\n",
    "        dict_dataset = {}\n",
    "        for key in longtensor_keys:\n",
    "            dict_dataset[key] = torch.LongTensor(dataset[key])\n",
    "        for key in tensor_keys:\n",
    "            dict_dataset[key] = torch.Tensor(dataset[key])\n",
    "        self.dataset = dict_dataset\n",
    "        self.random_swap = random_swap\n",
    "\n",
    "    @staticmethod\n",
    "    def flip_key(key_str):\n",
    "        # used to swap 1 and 2\n",
    "        # ord('1') + ord('2') = 99\n",
    "        return key_str[:8] + chr(99-ord(key_str[8])) + key_str[9:]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return_keys = ['question1_input_ids',\n",
    "                       'question2_input_ids',\n",
    "                       'question1_attention_mask',\n",
    "                       'question2_attention_mask']\n",
    "\n",
    "        if self.random_swap and random.random() < 0.5:\n",
    "            tmp = {k: self.dataset[self.flip_key(k)][item] for k in return_keys}\n",
    "        else:\n",
    "            tmp = {k: self.dataset[k][item] for k in return_keys}\n",
    "\n",
    "        return {\"input_ids\": tmp['question1_input_ids'],\n",
    "                \"attention_mask\": tmp['question1_attention_mask'],\n",
    "                \"labels\": tmp['question2_input_ids']}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset['question1_input_ids'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = QQPParaphraseDataset(qqp_train_processed, random_swap=True)\n",
    "eval_dataset = QQPParaphraseDataset(qqp_eval_processed, random_swap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_epoch = 10\n",
    "lr = 5e-5\n",
    "weight_decay = 1e-3\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda:0\")\n",
    "num_warmup_steps = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139420416"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in bart_model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"facebook/bart-base\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": false,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 128,\n",
       "      \"min_length\": 12,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_cnn\": {\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_xsum\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 62,\n",
       "      \"min_length\": 11,\n",
       "      \"num_beams\": 6\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.19.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in bart_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in bart_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,\n",
    "                                            num_warmup_steps=num_warmup_steps,\n",
    "                                            num_training_steps=num_epoch * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, scheduler=None, num_epoch=1, verbose=False, print_steps=200):\n",
    "    progress_bar = tqdm(range(num_epoch*len(dataloader)))\n",
    "    device = model.device\n",
    "    for epoch in range(num_epoch):\n",
    "        training_loss = 0\n",
    "        sample_cnt = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            bs = batch['input_ids'].shape[0]\n",
    "            sample_cnt += bs\n",
    "            training_loss += loss.cpu() * bs\n",
    "\n",
    "            if verbose and step % print_steps == print_steps-1:\n",
    "                # print training loss\n",
    "                training_loss /= sample_cnt\n",
    "                sample_cnt = 0\n",
    "                print('step:', step+1, ' training loss={:.4f}'.format(training_loss))\n",
    "                training_loss = 0\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, verbose=True):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            bs = batch['input_ids'].shape[0]\n",
    "            test_loss += bs*outputs.loss\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        test_loss /= len(dataloader.dataset)\n",
    "        if verbose:\n",
    "            print('eval loss={:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    print(\"epoch:\",epoch+1)\n",
    "    train(model=bart_model, dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler ,verbose=True)\n",
    "    evaluate(model=bart_model, dataloader=eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_batch = next(iter(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Is \"\"Pokemon Ranger and The Temple of The Sea\"\" considered appropriate for kids?\"',\n",
       " 'What is the best textbook for Hebrew?',\n",
       " 'How do I take control on masturbation?',\n",
       " 'Who is your favourite female movie director and why?',\n",
       " 'Do ghost actually exists?',\n",
       " 'Which is the best institute in Mumbai for doing Financial Modeling certification course?',\n",
       " 'What is exam pattern of MH CET MBA?',\n",
       " 'What is the difference between a porn figure and a prostitute?',\n",
       " '\"Why my \\'\\'i\"\" is different than yours?\"',\n",
       " 'How do you become more masculine?',\n",
       " 'How can improve my managerial skills?',\n",
       " 'Why do people ask question on Quora that can be easily and definitively answered by Googling?',\n",
       " 'How did you find a job abroad?',\n",
       " 'Why would a boy love a girl?',\n",
       " 'What do you hate about school?',\n",
       " 'What are the consequences of lying about your ethnicity on your college applications?',\n",
       " 'What are compounds? What are some examples?',\n",
       " 'What would you do if you woke up to find that nuclear war had started?',\n",
       " 'How can one without any financial knowledge learn about stock trading and investment?',\n",
       " 'What do you think of the first US presidential debate?',\n",
       " 'How does one avoid existential depression?',\n",
       " 'If Hillary Clinton wins, would she nominate Barack Obama for SCOTUS? Would he even be interested in being a Supreme Court Justice?',\n",
       " 'I had 5 lakhs in my savings account can I deposit 2.5 now in my account?',\n",
       " 'What is periodic table?',\n",
       " 'What can be the best books for starters?',\n",
       " 'What is the easiest and most painless suicide method?',\n",
       " 'How do Airbnb differentiate itself from craigslist?',\n",
       " 'Does green tea really help in losing weight?',\n",
       " 'How do I improve my pullup strength?',\n",
       " 'What are some advantages and disadvantages of computer?',\n",
       " 'What is it that you want from your life?',\n",
       " 'How do we deal with people who are habitually late?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(eval_batch['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Is \"\"Pokémon Ranger and The Temple of The Sea\"\" considered childish?\"',\n",
       " \"What's the best self study book to learn Hebrew?\",\n",
       " 'How do I control on masturbation?',\n",
       " 'Who is the best female movie director?',\n",
       " 'Does ghost really exist?',\n",
       " 'Which is the best institute in Mumbai from where a fresher can learn financial modeling?',\n",
       " 'What is the exam pattern of MH CET MBA?',\n",
       " 'What is a difference between a prostitute and a porn star?',\n",
       " '\"Why my \\'\\' I \"\" is different than yours?\"',\n",
       " 'How can one become more masculine?',\n",
       " 'How do you improve your managerial skills?',\n",
       " 'Why do so many people ask questions on Quora that can be easily answered by any number of legitimate sources on the Web? Have they not heard of',\n",
       " 'How do I find a job abroad?',\n",
       " 'Why do boys love girls?',\n",
       " 'Why do you hate school?',\n",
       " 'Can I lie about my ethnicity to top college admissions?',\n",
       " 'What are some examples of compounds?',\n",
       " 'What would you do, if nuclear war began?',\n",
       " 'How should I start learning about stock trading and start investing in stocks?',\n",
       " 'Who do you think has dominated in the first us Presidential candidate debate 2016?',\n",
       " 'How can I get over this existential depression?',\n",
       " 'If Hillary Clinton wins the Presidency, do you think she will nominate Barack Obama for the Supreme Court?',\n",
       " 'Can I deposit 2.5 lakh in my wife’s, son’s, daughter’s, and my account?',\n",
       " 'What is the significance of the periodic table?',\n",
       " 'What are some good books for starters?',\n",
       " 'What is the most painless way to do a suicide?',\n",
       " 'How did Airbnb differentiate itself from Craigslist in the early days?',\n",
       " 'Does drinking green tea for weight loss really help? Does it have any adverse effects on your skin?',\n",
       " 'What are some tips to improve your pull-ups?',\n",
       " 'What are the advantages and disadvantages of computer?',\n",
       " 'What you want to do in your life?',\n",
       " 'How do I handle people who are always late?']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(eval_batch['labels'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_output = bart_model.generate(eval_batch['input_ids'].to(device), num_beams=4, max_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Is \"\"Pokemon Ranger and The Temple of The Sea\"\" a good anime?\"',\n",
       " 'What is the best book for learning Hebrew?',\n",
       " 'How can I stop masturbation?',\n",
       " 'Who is your favorite female movie director and why?',\n",
       " 'Do spirits really exist?',\n",
       " 'Which is the best institute for financial modelling in Mumbai?',\n",
       " 'What is the exam pattern of MH CET MBA?',\n",
       " 'What is the difference between a prostitute and a porn star?',\n",
       " '\"Why my \"\"i\"\" is different than yours?\"',\n",
       " 'How can I be more masculine?',\n",
       " 'How can I improve my managerial skills?',\n",
       " 'Why do some people ask questions on Quora that could easily be answered by using a search engine?',\n",
       " 'How can I find a job abroad?',\n",
       " 'Why do boys love girls?',\n",
       " 'What do you hate most about school?',\n",
       " 'What are the consequences of lying about ethnicity in college admissions?',\n",
       " 'What are some examples of compounds?',\n",
       " 'What would you do if you woke up in the middle of a nuclear war?',\n",
       " 'What is the best way to learn about stock trading?',\n",
       " 'What do you think of the first presidential debate between Hillary and Trump?',\n",
       " 'How do I overcome existential depression?',\n",
       " 'If Hillary Clinton wins, would she consider Barack Obama for SCOTUS?',\n",
       " 'I had 5 lakhs in my savings account can I deposit 2.5 now in my account?',\n",
       " 'What is a periodic table?',\n",
       " 'What are the best books to read?',\n",
       " 'What is the least painful and best way to commit suicide?',\n",
       " 'How does Airbnb differ from Craigslist?',\n",
       " \"Why is Lipton's green tea helpful for losing weight?\",\n",
       " 'How do I increase my pullup strength?',\n",
       " 'What are the advantages and disadvantages of computers?',\n",
       " 'What do you want from life?',\n",
       " 'How do I deal with people who are late?']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generate_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'Ġis', 'Ġthe', 'Ġbest', 'Ġbook', 'Ġfor', 'Ġlearning', 'ĠHebrew', '?']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokenizer.convert_ids_to_tokens(generate_output[1], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'Ġis', 'Ġthe', 'Ġbest', 'Ġtextbook', 'Ġfor', 'ĠHebrew', '?']\n"
     ]
    }
   ],
   "source": [
    "src = tokenizer.convert_ids_to_tokens(eval_batch['input_ids'][1], skip_special_tokens=True)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   113,  6209, 41039, 46145, 23151,     8,    20,  9660,     9,\n",
       "           20,  3939, 48149,  1687,  3901,    13,  1159,  1917,     2,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_batch['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 113, 6209, 41039, 46145, 23151, 8, 20, 9660, 9, 20, 3939, 48149, 1687, 3901, 13, 1159, 1917, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(qqp_eval_raw[0]['question1_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s><s>\"Is \"\"Pokemon Ranger and The Temple of The Sea\"\" a good anime?\"</s><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generate_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    0,   113,  6209, 41039, 46145, 23151,     8,    20,  9660,     9,\n",
       "            20,  3939, 48149,  1687,  3901,    13,  1159,  1917,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  2264,    16,     5,   275, 31046,    13, 27428,   116,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  6179,   109,    38,   185,   797,    15, 44473,   116,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0, 12375,    16,   110,  5548,  2182,  1569,   736,     8,   596,\n",
       "           116,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  8275, 15934,   888,  8785,   116,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0, 32251,    16,     5,   275, 14619,    11,  5729,    13,   608,\n",
       "          2108,  7192,   154, 12930,   768,   116,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  2264,    16, 10743,  6184,     9, 24294, 39034, 25330,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  2264,    16,     5,  2249,   227,    10, 13971,  1955,     8,\n",
       "            10, 36289,   116,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,   113,  7608,   127, 12801,   118, 48149,    16,   430,    87,\n",
       "         14314,  1917,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  6179,   109,    47,   555,    55, 37360,   116,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  6179,    64,  1477,   127, 27424,  2417,   116,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  7608,   109,    82,  1394,   864,    15,  3232,  4330,    14,\n",
       "            64,    28,  2773,     8, 35144,  7173,    30,  2381,  2154,  1527,\n",
       "           116,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([   0, 6179,  222,   47,  465,   10,  633, 5358,  116,    2,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([   0, 7608,   74,   10, 2143,  657,   10, 1816,  116,    2,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([   0, 2264,  109,   47, 4157,   59,  334,  116,    2,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([    0,  2264,    32,     5,  4914,     9,  6480,    59,   110, 23848,\n",
       "            15,   110,  1564,  2975,   116,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  2264,    32, 18291,   116,   653,    32,   103,  7721,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  2264,    74,    47,   109,   114,    47, 13356,    62,     7,\n",
       "           465,    14,  1748,   997,    56,   554,   116,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([   0, 6179,   64,   65,  396,  143,  613, 2655, 1532,   59,  388, 1446,\n",
       "            8,  915,  116,    2,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([   0, 2264,  109,   47,  206,    9,    5,   78,  382, 1939, 2625,  116,\n",
       "            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([    0,  6179,   473,    65,  1877, 27499,  6943,   116,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  1106,  5141,  2235,  2693,     6,    74,    79, 22159,  4282,\n",
       "          1284,    13,  4998, 24678,   116, 11258,    37,   190,    28,  2509,\n",
       "            11,   145,    10,  2124,   837,  1659,   116,     2,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([   0,  100,   56,  195, 8305,   29,   11,  127, 4522, 1316,   64,   38,\n",
       "         8068,  132,    4,  245,  122,   11,  127, 1316,  116,    2,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([    0,  2264,    16, 27185,  2103,   116,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([   0, 2264,   64,   28,    5,  275, 2799,   13, 9770,  116,    2,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([    0,  2264,    16,     5, 18815,     8,   144,  2400,  1672,  4260,\n",
       "          5448,   116,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  6179,   109, 16315, 26162,  1495,    31, 19656, 48535,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0, 27847,  2272,  6845,   269,   244,    11,  2086,  2408,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([   0, 6179,  109,   38, 1477,  127, 2999,  658, 2707,  116,    2,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([    0,  2264,    32,   103, 12340,     8, 38940,     9,  3034,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([   0, 2264,   16,   24,   14,   47,  236,   31,  110,  301,  116,    2,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([    0,  6179,   109,    52,   432,    19,    82,    54,    32, 10870,\n",
       "         13851,   628,   116,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(eval_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0,   113,  6209, 41039, 46145, 23151,     8,    20,  9660,\n",
       "             9,    20,  3939, 48149,    10,   205, 28805,  1917,     2,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    16,     5,   275,  1040,    13,  2239, 27428,\n",
       "           116,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  6179,    64,    38,   912, 44473,   116,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0, 12375,    16,   110,  2674,  2182,  1569,   736,     8,\n",
       "           596,   116,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  8275, 11656,   269,  5152,   116,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0, 32251,    16,     5,   275, 14619,    13,   613, 26471,\n",
       "            11,  5729,   116,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    16,     5, 10743,  6184,     9, 24294, 39034,\n",
       "         25330,   116,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    16,     5,  2249,   227,    10, 36289,     8,\n",
       "            10, 13971,   999,   116,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,   113,  7608,   127, 41039,   118, 48149,    16,   430,\n",
       "            87, 14314,  1917,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  6179,    64,    38,    28,    55, 37360,   116,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  6179,    64,    38,  1477,   127, 27424,  2417,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  7608,   109,   103,    82,  1394,  1142,    15,  3232,\n",
       "          4330,    14,   115,  2773,    28,  7173,    30,   634,    10,  1707,\n",
       "          3819,   116,     2],\n",
       "        [    2,     0,  6179,    64,    38,   465,    10,   633,  5358,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  7608,   109,  2786,   657,  1972,   116,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,   109,    47,  4157,   144,    59,   334,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    32,     5,  4914,     9,  6480,    59, 23848,\n",
       "            11,  1564, 18054,   116,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    32,   103,  7721,     9, 18291,   116,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    74,    47,   109,   114,    47, 13356,    62,\n",
       "            11,     5,  1692,     9,    10,  1748,   997,   116,     2,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    16,     5,   275,   169,     7,  1532,    59,\n",
       "           388,  1446,   116,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,   109,    47,   206,     9,     5,    78,  1939,\n",
       "          2625,   227,  5141,     8,   140,   116,     2,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  6179,   109,    38,  6647, 27499,  6943,   116,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  1106,  5141,  2235,  2693,     6,    74,    79,  1701,\n",
       "          4282,  1284,    13,  4998, 24678,   116,     2,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,   100,    56,   195,  8305,    29,    11,   127,  4522,\n",
       "          1316,    64,    38,  8068,   132,     4,   245,   122,    11,   127,\n",
       "          1316,   116,     2],\n",
       "        [    2,     0,  2264,    16,    10, 27185,  2103,   116,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    32,     5,   275,  2799,     7,  1166,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    16,     5,   513,  8661,     8,   275,   169,\n",
       "             7,  6225,  4260,   116,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  6179,   473, 16315, 10356,    31, 37439,   116,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  7608,    16,  5991,  9216,    18,  2272,  6845,  7163,\n",
       "            13,  2086,  2408,   116,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  6179,   109,    38,   712,   127,  2999,   658,  2707,\n",
       "           116,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,    32,     5, 12340,     8, 38940,     9,  7796,\n",
       "           116,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  2264,   109,    47,   236,    31,   301,   116,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1],\n",
       "        [    2,     0,  6179,   109,    38,   432,    19,    82,    54,    32,\n",
       "           628,   116,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb1e593391c4ef8a3e96054a81eba90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_inputs = []\n",
    "reference_outputs = []\n",
    "generated_outputs = []\n",
    "\n",
    "for eval_batch in tqdm(eval_dataloader):\n",
    "    src_inputs += [tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True) for input_ids in eval_batch['input_ids']]\n",
    "    reference_outputs += [tokenizer.convert_ids_to_tokens(labels, skip_special_tokens=True) for labels in eval_batch['labels']]\n",
    "    generate_output = bart_model.generate(eval_batch['input_ids'].to(device), num_beams=4, max_length=32)\n",
    "    generated_outputs += [tokenizer.convert_ids_to_tokens(outputs, skip_special_tokens=True) for outputs in generate_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13438"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleu_score = []\n",
    "self_bleu_score = []\n",
    "\n",
    "for src, ref, generate in zip(src_inputs, reference_outputs, generated_outputs):\n",
    "    bleu_score.append(sentence_bleu([ref], generate))\n",
    "    self_bleu_score.append(sentence_bleu([src], generate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23770649604085697"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_score)/len(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3532578247938156"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(self_bleu_score)/len(self_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
