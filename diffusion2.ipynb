{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import DiffusionLM\n",
    "from data_utils import load_rocstories_dataset_and_tokenizer, RocstoriesDataset\n",
    "from noise_schedule import get_named_beta_schedule\n",
    "from train_utils import train, evaluate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataset args\n",
    "max_len = 72    # maximum length of input_ids\n",
    "vocab_threshold = 10    # occurrence time < threshold token as [UNK]\n",
    "test_size = 0.1     # size of evaluation dataset\n",
    "\n",
    "# training args\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda:1\")\n",
    "lr = 2e-4\n",
    "num_epoch = 50\n",
    "weight_decay = 0\n",
    "num_warmup_steps = 100\n",
    "\n",
    "# model args\n",
    "word_embedding_dim = 128\n",
    "hidden_size = 512\n",
    "num_hidden_layers = 4\n",
    "num_attention_heads = 8\n",
    "intermediate_size = 2048\n",
    "max_position_embeddings = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: roc_stories/all\n",
      "Reusing dataset roc_stories (/home/dingyizhou/.cache/huggingface/datasets/wza___roc_stories/all/2.1.0/43e2851d9f31e08e4b2dd07a8057ed7a64cbb25cc7105d09856c14e638695506)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76559e52846f47ce828688b856127d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8af443f84b412bb491c6fe0fd3439b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c900598fe3496684e341957fa042b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_rocstories_dataset, tokenizer = load_rocstories_dataset_and_tokenizer(max_len=max_len, vocab_threshold=vocab_threshold)\n",
    "\n",
    "rev_tokenizer = {v: k for k, v in tokenizer.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 88344\n",
      "Evaluation set size: 9817\n"
     ]
    }
   ],
   "source": [
    "train_set, eval_set = train_test_split(tokenized_rocstories_dataset, test_size=test_size, shuffle=True)\n",
    "\n",
    "train_dataset = RocstoriesDataset(data_lst=train_set['input_ids'], attention_mask_lst=train_set['attention_mask'])\n",
    "print(\"Training set size:\",len(train_dataset))\n",
    "eval_dataset = RocstoriesDataset(data_lst=eval_set['input_ids'], attention_mask_lst=eval_set['attention_mask'])\n",
    "print(\"Evaluation set size:\", len(eval_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"T\": 2000,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 72,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 11831,\n",
      "  \"word_embedding_dim\": 128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=len(tokenizer), hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=max_position_embeddings, pad_token_id=tokenizer['[PAD]'])\n",
    "\n",
    "config.T = 2000\n",
    "config.word_embedding_dim = word_embedding_dim\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "betas = torch.Tensor(get_named_beta_schedule(schedule_name=\"sqrt\", num_diffusion_timesteps=config.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion model #parameters:\n",
      "17369015\n"
     ]
    }
   ],
   "source": [
    "diffusion_model = DiffusionLM(config=config, betas=betas, use_attention_mask=False, use_shared_weight=False, add_emb_noise=True).to(device)\n",
    "\n",
    "print(\"Diffusion model #parameters:\")\n",
    "print(sum([p.numel() for p in diffusion_model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(diffusion_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_epoch*len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b038510e5ff4eda8d2fa4e6a54c2ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138050 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 100\n",
      "mse  training loss=0.8261\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=9.0203\n",
      "step: 200\n",
      "mse  training loss=0.5848\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=7.2288\n",
      "step: 300\n",
      "mse  training loss=0.4985\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=5.1360\n",
      "step: 400\n",
      "mse  training loss=0.4493\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=3.4085\n",
      "step: 500\n",
      "mse  training loss=0.4211\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=2.3304\n",
      "step: 600\n",
      "mse  training loss=0.3963\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=1.7090\n",
      "step: 700\n",
      "mse  training loss=0.3929\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=1.3688\n",
      "step: 800\n",
      "mse  training loss=0.3707\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=1.1454\n",
      "step: 900\n",
      "mse  training loss=0.3646\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.9873\n",
      "step: 1000\n",
      "mse  training loss=0.3515\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.8733\n",
      "step: 1100\n",
      "mse  training loss=0.3384\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.7763\n",
      "step: 1200\n",
      "mse  training loss=0.3320\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.6963\n",
      "step: 1300\n",
      "mse  training loss=0.3286\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.6306\n",
      "step: 1400\n",
      "mse  training loss=0.3205\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.5780\n",
      "step: 1500\n",
      "mse  training loss=0.3135\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.5265\n",
      "step: 1600\n",
      "mse  training loss=0.3089\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.4917\n",
      "step: 1700\n",
      "mse  training loss=0.3017\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.4498\n",
      "step: 1800\n",
      "mse  training loss=0.2956\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.4199\n",
      "step: 1900\n",
      "mse  training loss=0.2931\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.3864\n",
      "step: 2000\n",
      "mse  training loss=0.2891\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.3590\n",
      "step: 2100\n",
      "mse  training loss=0.2753\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.3450\n",
      "step: 2200\n",
      "mse  training loss=0.2790\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.3222\n",
      "step: 2300\n",
      "mse  training loss=0.2823\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.3009\n",
      "step: 2400\n",
      "mse  training loss=0.2739\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.2801\n",
      "step: 2500\n",
      "mse  training loss=0.2690\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.2652\n",
      "step: 2600\n",
      "mse  training loss=0.2686\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.2500\n",
      "step: 2700\n",
      "mse  training loss=0.2572\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.2393\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b6869bd15a4473bc44b2879c0ffc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.4686\n",
      "epoch: 2\n",
      "step: 100\n",
      "mse  training loss=0.2582\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.2154\n",
      "step: 200\n",
      "mse  training loss=0.2602\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.2033\n",
      "step: 300\n",
      "mse  training loss=0.2609\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1945\n",
      "step: 400\n",
      "mse  training loss=0.2569\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1798\n",
      "step: 500\n",
      "mse  training loss=0.2528\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1702\n",
      "step: 600\n",
      "mse  training loss=0.2525\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1668\n",
      "step: 700\n",
      "mse  training loss=0.2451\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1573\n",
      "step: 800\n",
      "mse  training loss=0.2443\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1509\n",
      "step: 900\n",
      "mse  training loss=0.2484\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1451\n",
      "step: 1000\n",
      "mse  training loss=0.2409\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1399\n",
      "step: 1100\n",
      "mse  training loss=0.2513\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1298\n",
      "step: 1200\n",
      "mse  training loss=0.2483\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1253\n",
      "step: 1300\n",
      "mse  training loss=0.2371\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1221\n",
      "step: 1400\n",
      "mse  training loss=0.2354\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1174\n",
      "step: 1500\n",
      "mse  training loss=0.2383\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1121\n",
      "step: 1600\n",
      "mse  training loss=0.2409\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1096\n",
      "step: 1700\n",
      "mse  training loss=0.2317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.1044\n",
      "step: 1800\n",
      "mse  training loss=0.2334\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0979\n",
      "step: 1900\n",
      "mse  training loss=0.2313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0931\n",
      "step: 2000\n",
      "mse  training loss=0.2321\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0866\n",
      "step: 2100\n",
      "mse  training loss=0.2309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0857\n",
      "step: 2200\n",
      "mse  training loss=0.2283\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0840\n",
      "step: 2300\n",
      "mse  training loss=0.2275\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0788\n",
      "step: 2400\n",
      "mse  training loss=0.2241\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0772\n",
      "step: 2500\n",
      "mse  training loss=0.2276\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0743\n",
      "step: 2600\n",
      "mse  training loss=0.2239\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0697\n",
      "step: 2700\n",
      "mse  training loss=0.2222\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f93ee8897e2466c8fd010bd02d08979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.2783\n",
      "epoch: 3\n",
      "step: 100\n",
      "mse  training loss=0.2262\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0593\n",
      "step: 200\n",
      "mse  training loss=0.2246\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0586\n",
      "step: 300\n",
      "mse  training loss=0.2217\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0550\n",
      "step: 400\n",
      "mse  training loss=0.2180\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0532\n",
      "step: 500\n",
      "mse  training loss=0.2181\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0517\n",
      "step: 600\n",
      "mse  training loss=0.2213\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0497\n",
      "step: 700\n",
      "mse  training loss=0.2110\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0473\n",
      "step: 800\n",
      "mse  training loss=0.2150\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0481\n",
      "step: 900\n",
      "mse  training loss=0.2145\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0445\n",
      "step: 1000\n",
      "mse  training loss=0.2185\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0441\n",
      "step: 1100\n",
      "mse  training loss=0.2084\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0409\n",
      "step: 1200\n",
      "mse  training loss=0.2104\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0391\n",
      "step: 1300\n",
      "mse  training loss=0.2156\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0373\n",
      "step: 1400\n",
      "mse  training loss=0.2082\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0381\n",
      "step: 1500\n",
      "mse  training loss=0.2080\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0350\n",
      "step: 1600\n",
      "mse  training loss=0.2058\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0352\n",
      "step: 1700\n",
      "mse  training loss=0.2070\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0318\n",
      "step: 1800\n",
      "mse  training loss=0.2045\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0315\n",
      "step: 1900\n",
      "mse  training loss=0.2068\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0299\n",
      "step: 2000\n",
      "mse  training loss=0.2120\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0299\n",
      "step: 2100\n",
      "mse  training loss=0.2085\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0280\n",
      "step: 2200\n",
      "mse  training loss=0.2083\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0267\n",
      "step: 2300\n",
      "mse  training loss=0.2051\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0263\n",
      "step: 2400\n",
      "mse  training loss=0.2089\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0249\n",
      "step: 2500\n",
      "mse  training loss=0.2030\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0254\n",
      "step: 2600\n",
      "mse  training loss=0.2065\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0235\n",
      "step: 2700\n",
      "mse  training loss=0.2020\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c19c864fd44d5faf1a433f3d642c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.2164\n",
      "epoch: 4\n",
      "step: 100\n",
      "mse  training loss=0.2016\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0206\n",
      "step: 200\n",
      "mse  training loss=0.1990\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0194\n",
      "step: 300\n",
      "mse  training loss=0.1993\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0187\n",
      "step: 400\n",
      "mse  training loss=0.1960\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0177\n",
      "step: 500\n",
      "mse  training loss=0.2017\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0172\n",
      "step: 600\n",
      "mse  training loss=0.1964\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0160\n",
      "step: 700\n",
      "mse  training loss=0.1936\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0169\n",
      "step: 800\n",
      "mse  training loss=0.1934\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0151\n",
      "step: 900\n",
      "mse  training loss=0.1941\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0151\n",
      "step: 1000\n",
      "mse  training loss=0.1929\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0145\n",
      "step: 1100\n",
      "mse  training loss=0.1882\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0131\n",
      "step: 1200\n",
      "mse  training loss=0.1944\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0131\n",
      "step: 1300\n",
      "mse  training loss=0.1928\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0128\n",
      "step: 1400\n",
      "mse  training loss=0.1941\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0119\n",
      "step: 1500\n",
      "mse  training loss=0.1902\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0127\n",
      "step: 1600\n",
      "mse  training loss=0.1933\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0111\n",
      "step: 1700\n",
      "mse  training loss=0.1896\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0114\n",
      "step: 1800\n",
      "mse  training loss=0.1886\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0102\n",
      "step: 1900\n",
      "mse  training loss=0.1866\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0102\n",
      "step: 2000\n",
      "mse  training loss=0.1915\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0105\n",
      "step: 2100\n",
      "mse  training loss=0.1867\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0094\n",
      "step: 2200\n",
      "mse  training loss=0.1880\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0094\n",
      "step: 2300\n",
      "mse  training loss=0.1901\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0087\n",
      "step: 2400\n",
      "mse  training loss=0.1825\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0084\n",
      "step: 2500\n",
      "mse  training loss=0.1864\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0079\n",
      "step: 2600\n",
      "mse  training loss=0.1834\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0086\n",
      "step: 2700\n",
      "mse  training loss=0.1829\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfefefe88e34e778b9edd35a7146d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.1830\n",
      "epoch: 5\n",
      "step: 100\n",
      "mse  training loss=0.1812\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0065\n",
      "step: 200\n",
      "mse  training loss=0.1827\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0060\n",
      "step: 300\n",
      "mse  training loss=0.1818\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0064\n",
      "step: 400\n",
      "mse  training loss=0.1828\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0062\n",
      "step: 500\n",
      "mse  training loss=0.1749\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0064\n",
      "step: 600\n",
      "mse  training loss=0.1782\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0058\n",
      "step: 700\n",
      "mse  training loss=0.1811\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0052\n",
      "step: 800\n",
      "mse  training loss=0.1779\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0059\n",
      "step: 900\n",
      "mse  training loss=0.1797\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0057\n",
      "step: 1000\n",
      "mse  training loss=0.1804\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0053\n",
      "step: 1100\n",
      "mse  training loss=0.1767\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0049\n",
      "step: 1200\n",
      "mse  training loss=0.1782\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0049\n",
      "step: 1300\n",
      "mse  training loss=0.1721\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0045\n",
      "step: 1400\n",
      "mse  training loss=0.1735\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0048\n",
      "step: 1500\n",
      "mse  training loss=0.1758\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0034\n",
      "step: 1600\n",
      "mse  training loss=0.1751\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0048\n",
      "step: 1700\n",
      "mse  training loss=0.1746\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0041\n",
      "step: 1800\n",
      "mse  training loss=0.1740\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0035\n",
      "step: 1900\n",
      "mse  training loss=0.1735\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0040\n",
      "step: 2000\n",
      "mse  training loss=0.1753\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0044\n",
      "step: 2100\n",
      "mse  training loss=0.1697\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0034\n",
      "step: 2200\n",
      "mse  training loss=0.1720\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0033\n",
      "step: 2300\n",
      "mse  training loss=0.1729\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0029\n",
      "step: 2400\n",
      "mse  training loss=0.1685\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0033\n",
      "step: 2500\n",
      "mse  training loss=0.1659\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0031\n",
      "step: 2600\n",
      "mse  training loss=0.1737\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0033\n",
      "step: 2700\n",
      "mse  training loss=0.1710\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c707e340e584c9a89034a018bc9d5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.1683\n",
      "epoch: 6\n",
      "step: 100\n",
      "mse  training loss=0.1708\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0023\n",
      "step: 200\n",
      "mse  training loss=0.1700\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0024\n",
      "step: 300\n",
      "mse  training loss=0.1699\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0023\n",
      "step: 400\n",
      "mse  training loss=0.1698\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0021\n",
      "step: 500\n",
      "mse  training loss=0.1703\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0024\n",
      "step: 600\n",
      "mse  training loss=0.1683\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0021\n",
      "step: 700\n",
      "mse  training loss=0.1653\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0021\n",
      "step: 800\n",
      "mse  training loss=0.1702\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0018\n",
      "step: 900\n",
      "mse  training loss=0.1657\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0019\n",
      "step: 1000\n",
      "mse  training loss=0.1677\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0022\n",
      "step: 1100\n",
      "mse  training loss=0.1714\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0021\n",
      "step: 1200\n",
      "mse  training loss=0.1636\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0021\n",
      "step: 1300\n",
      "mse  training loss=0.1655\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0018\n",
      "step: 1400\n",
      "mse  training loss=0.1680\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0021\n",
      "step: 1500\n",
      "mse  training loss=0.1657\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0020\n",
      "step: 1600\n",
      "mse  training loss=0.1669\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0020\n",
      "step: 1700\n",
      "mse  training loss=0.1651\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0018\n",
      "step: 1800\n",
      "mse  training loss=0.1616\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0017\n",
      "step: 1900\n",
      "mse  training loss=0.1645\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0020\n",
      "step: 2000\n",
      "mse  training loss=0.1592\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0019\n",
      "step: 2100\n",
      "mse  training loss=0.1584\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0015\n",
      "step: 2200\n",
      "mse  training loss=0.1591\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0015\n",
      "step: 2300\n",
      "mse  training loss=0.1586\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2400\n",
      "mse  training loss=0.1575\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0015\n",
      "step: 2500\n",
      "mse  training loss=0.1573\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2600\n",
      "mse  training loss=0.1578\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2700\n",
      "mse  training loss=0.1591\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6dd4eba728462981395ffa5bd336e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.1539\n",
      "epoch: 7\n",
      "step: 100\n",
      "mse  training loss=0.1558\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 200\n",
      "mse  training loss=0.1544\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 300\n",
      "mse  training loss=0.1593\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 400\n",
      "mse  training loss=0.1603\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 500\n",
      "mse  training loss=0.1535\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.1559\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 700\n",
      "mse  training loss=0.1551\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.1543\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.1552\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.1540\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.1504\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 1200\n",
      "mse  training loss=0.1525\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.1547\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 1400\n",
      "mse  training loss=0.1529\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 1500\n",
      "mse  training loss=0.1510\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.1521\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.1486\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.1496\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.1471\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 2000\n",
      "mse  training loss=0.1474\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 2100\n",
      "mse  training loss=0.1498\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 2200\n",
      "mse  training loss=0.1545\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.1519\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 2400\n",
      "mse  training loss=0.1463\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 2500\n",
      "mse  training loss=0.1507\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 2600\n",
      "mse  training loss=0.1502\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 2700\n",
      "mse  training loss=0.1473\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331a684d3e474b89929464a4d1a664cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.1426\n",
      "epoch: 8\n",
      "step: 100\n",
      "mse  training loss=0.1470\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 200\n",
      "mse  training loss=0.1456\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 300\n",
      "mse  training loss=0.1480\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 400\n",
      "mse  training loss=0.1465\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 500\n",
      "mse  training loss=0.1446\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 600\n",
      "mse  training loss=0.1447\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 700\n",
      "mse  training loss=0.1454\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 800\n",
      "mse  training loss=0.1455\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 900\n",
      "mse  training loss=0.1427\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1000\n",
      "mse  training loss=0.1432\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 1100\n",
      "mse  training loss=0.1449\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 1200\n",
      "mse  training loss=0.1402\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 1300\n",
      "mse  training loss=0.1421\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 1400\n",
      "mse  training loss=0.1429\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 1500\n",
      "mse  training loss=0.1418\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 1600\n",
      "mse  training loss=0.1419\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 1700\n",
      "mse  training loss=0.1387\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1800\n",
      "mse  training loss=0.1405\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 1900\n",
      "mse  training loss=0.1416\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2000\n",
      "mse  training loss=0.1413\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 2100\n",
      "mse  training loss=0.1435\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2200\n",
      "mse  training loss=0.1370\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2300\n",
      "mse  training loss=0.1384\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2400\n",
      "mse  training loss=0.1384\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2500\n",
      "mse  training loss=0.1373\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2600\n",
      "mse  training loss=0.1374\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2700\n",
      "mse  training loss=0.1360\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b8151153ca4d94958eab60a6e0806b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.1325\n",
      "epoch: 9\n",
      "step: 100\n",
      "mse  training loss=0.1348\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 200\n",
      "mse  training loss=0.1338\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 300\n",
      "mse  training loss=0.1353\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 400\n",
      "mse  training loss=0.1356\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 500\n",
      "mse  training loss=0.1321\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 600\n",
      "mse  training loss=0.1334\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 700\n",
      "mse  training loss=0.1313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 800\n",
      "mse  training loss=0.1341\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 900\n",
      "mse  training loss=0.1298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1000\n",
      "mse  training loss=0.1330\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1100\n",
      "mse  training loss=0.1302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1200\n",
      "mse  training loss=0.1291\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1300\n",
      "mse  training loss=0.1321\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1400\n",
      "mse  training loss=0.1284\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1500\n",
      "mse  training loss=0.1337\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1600\n",
      "mse  training loss=0.1278\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1700\n",
      "mse  training loss=0.1279\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1800\n",
      "mse  training loss=0.1273\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1900\n",
      "mse  training loss=0.1280\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2000\n",
      "mse  training loss=0.1263\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2100\n",
      "mse  training loss=0.1300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 2200\n",
      "mse  training loss=0.1289\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2300\n",
      "mse  training loss=0.1279\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2400\n",
      "mse  training loss=0.1294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2500\n",
      "mse  training loss=0.1266\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2600\n",
      "mse  training loss=0.1256\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2700\n",
      "mse  training loss=0.1246\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fdf1fe3f6a4b9083af82e7aa0b45cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.1216\n",
      "epoch: 10\n",
      "step: 100\n",
      "mse  training loss=0.1263\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 200\n",
      "mse  training loss=0.1270\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 300\n",
      "mse  training loss=0.1206\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 400\n",
      "mse  training loss=0.1210\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 500\n",
      "mse  training loss=0.1207\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 600\n",
      "mse  training loss=0.1233\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 700\n",
      "mse  training loss=0.1219\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 800\n",
      "mse  training loss=0.1195\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 900\n",
      "mse  training loss=0.1223\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1000\n",
      "mse  training loss=0.1187\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1100\n",
      "mse  training loss=0.1201\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1200\n",
      "mse  training loss=0.1215\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1300\n",
      "mse  training loss=0.1183\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1400\n",
      "mse  training loss=0.1166\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1500\n",
      "mse  training loss=0.1190\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1600\n",
      "mse  training loss=0.1178\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1700\n",
      "mse  training loss=0.1176\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1800\n",
      "mse  training loss=0.1149\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 1900\n",
      "mse  training loss=0.1149\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2000\n",
      "mse  training loss=0.1127\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2100\n",
      "mse  training loss=0.1142\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2200\n",
      "mse  training loss=0.1147\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2300\n",
      "mse  training loss=0.1160\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2400\n",
      "mse  training loss=0.1124\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2500\n",
      "mse  training loss=0.1139\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2600\n",
      "mse  training loss=0.1152\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 2700\n",
      "mse  training loss=0.1111\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7864e59b4d4ef896e0fa428c2f8953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.1075\n",
      "epoch: 11\n",
      "step: 100\n",
      "mse  training loss=0.1083\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 200\n",
      "mse  training loss=0.1115\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0002\n",
      "step: 300\n",
      "mse  training loss=0.1098\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 400\n",
      "mse  training loss=0.1091\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 500\n",
      "mse  training loss=0.1092\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 600\n",
      "mse  training loss=0.1076\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 700\n",
      "mse  training loss=0.1089\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 800\n",
      "mse  training loss=0.1056\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 900\n",
      "mse  training loss=0.1090\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1000\n",
      "mse  training loss=0.1065\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1100\n",
      "mse  training loss=0.1078\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1200\n",
      "mse  training loss=0.1055\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1300\n",
      "mse  training loss=0.1059\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1400\n",
      "mse  training loss=0.1057\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1500\n",
      "mse  training loss=0.1038\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1600\n",
      "mse  training loss=0.1049\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 1700\n",
      "mse  training loss=0.1017\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0003\n",
      "step: 1800\n",
      "mse  training loss=0.1022\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 1900\n",
      "mse  training loss=0.1026\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 2000\n",
      "mse  training loss=0.1004\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 2100\n",
      "mse  training loss=0.1015\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 2200\n",
      "mse  training loss=0.1015\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 2300\n",
      "mse  training loss=0.1001\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 2400\n",
      "mse  training loss=0.0995\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0004\n",
      "step: 2500\n",
      "mse  training loss=0.1013\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 2600\n",
      "mse  training loss=0.0980\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 2700\n",
      "mse  training loss=0.0964\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af304dcb7bed435daaec14ede4d1cecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0958\n",
      "epoch: 12\n",
      "step: 100\n",
      "mse  training loss=0.0966\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 200\n",
      "mse  training loss=0.0968\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 300\n",
      "mse  training loss=0.0964\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 400\n",
      "mse  training loss=0.0942\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 500\n",
      "mse  training loss=0.0944\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 600\n",
      "mse  training loss=0.0954\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 700\n",
      "mse  training loss=0.0941\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 800\n",
      "mse  training loss=0.0922\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0005\n",
      "step: 900\n",
      "mse  training loss=0.0924\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 1000\n",
      "mse  training loss=0.0927\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 1100\n",
      "mse  training loss=0.0926\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 1200\n",
      "mse  training loss=0.0893\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 1300\n",
      "mse  training loss=0.0897\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 1400\n",
      "mse  training loss=0.0905\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 1500\n",
      "mse  training loss=0.0898\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 1600\n",
      "mse  training loss=0.0892\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0006\n",
      "step: 1700\n",
      "mse  training loss=0.0872\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 1800\n",
      "mse  training loss=0.0874\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 1900\n",
      "mse  training loss=0.0863\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 2000\n",
      "mse  training loss=0.0882\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 2100\n",
      "mse  training loss=0.0862\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 2200\n",
      "mse  training loss=0.0859\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 2300\n",
      "mse  training loss=0.0855\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 2400\n",
      "mse  training loss=0.0854\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0834\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 2600\n",
      "mse  training loss=0.0841\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2700\n",
      "mse  training loss=0.0838\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd112967306f4ccc8b306d46a0840eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0829\n",
      "epoch: 13\n",
      "step: 100\n",
      "mse  training loss=0.0832\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0007\n",
      "step: 200\n",
      "mse  training loss=0.0824\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0824\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0812\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0806\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0805\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 700\n",
      "mse  training loss=0.0798\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0779\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0790\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0779\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0766\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0783\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0782\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0759\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0769\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0760\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1700\n",
      "mse  training loss=0.0761\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1800\n",
      "mse  training loss=0.0759\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1900\n",
      "mse  training loss=0.0747\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2000\n",
      "mse  training loss=0.0741\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2100\n",
      "mse  training loss=0.0731\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2200\n",
      "mse  training loss=0.0737\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2300\n",
      "mse  training loss=0.0727\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2400\n",
      "mse  training loss=0.0724\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2500\n",
      "mse  training loss=0.0724\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2600\n",
      "mse  training loss=0.0711\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2700\n",
      "mse  training loss=0.0717\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ba864c35cb409b85d4cba766917490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0710\n",
      "epoch: 14\n",
      "step: 100\n",
      "mse  training loss=0.0700\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 200\n",
      "mse  training loss=0.0709\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 300\n",
      "mse  training loss=0.0702\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 400\n",
      "mse  training loss=0.0704\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 500\n",
      "mse  training loss=0.0690\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 600\n",
      "mse  training loss=0.0697\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 700\n",
      "mse  training loss=0.0690\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 800\n",
      "mse  training loss=0.0692\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 900\n",
      "mse  training loss=0.0687\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1000\n",
      "mse  training loss=0.0681\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1100\n",
      "mse  training loss=0.0671\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1200\n",
      "mse  training loss=0.0673\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1300\n",
      "mse  training loss=0.0675\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1400\n",
      "mse  training loss=0.0669\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1500\n",
      "mse  training loss=0.0662\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1600\n",
      "mse  training loss=0.0660\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1700\n",
      "mse  training loss=0.0648\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1800\n",
      "mse  training loss=0.0649\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1900\n",
      "mse  training loss=0.0654\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2000\n",
      "mse  training loss=0.0652\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2100\n",
      "mse  training loss=0.0645\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2200\n",
      "mse  training loss=0.0641\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2300\n",
      "mse  training loss=0.0638\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2400\n",
      "mse  training loss=0.0638\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2500\n",
      "mse  training loss=0.0633\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2600\n",
      "mse  training loss=0.0630\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2700\n",
      "mse  training loss=0.0626\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafdd41194594cc782f61f83fa3c476c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0624\n",
      "epoch: 15\n",
      "step: 100\n",
      "mse  training loss=0.0618\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 200\n",
      "mse  training loss=0.0621\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 300\n",
      "mse  training loss=0.0609\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 400\n",
      "mse  training loss=0.0607\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 500\n",
      "mse  training loss=0.0612\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 600\n",
      "mse  training loss=0.0608\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 700\n",
      "mse  training loss=0.0598\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 800\n",
      "mse  training loss=0.0612\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 900\n",
      "mse  training loss=0.0600\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1000\n",
      "mse  training loss=0.0600\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1100\n",
      "mse  training loss=0.0600\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1200\n",
      "mse  training loss=0.0604\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1300\n",
      "mse  training loss=0.0596\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1400\n",
      "mse  training loss=0.0592\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1500\n",
      "mse  training loss=0.0587\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1600\n",
      "mse  training loss=0.0586\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1700\n",
      "mse  training loss=0.0576\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1800\n",
      "mse  training loss=0.0582\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1900\n",
      "mse  training loss=0.0573\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2000\n",
      "mse  training loss=0.0570\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2100\n",
      "mse  training loss=0.0584\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2200\n",
      "mse  training loss=0.0577\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2300\n",
      "mse  training loss=0.0576\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2400\n",
      "mse  training loss=0.0569\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2500\n",
      "mse  training loss=0.0569\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2600\n",
      "mse  training loss=0.0565\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2700\n",
      "mse  training loss=0.0570\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6795cd97654de3b596dfa514633185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0572\n",
      "epoch: 16\n",
      "step: 100\n",
      "mse  training loss=0.0561\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 200\n",
      "mse  training loss=0.0556\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 300\n",
      "mse  training loss=0.0557\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 400\n",
      "mse  training loss=0.0553\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 500\n",
      "mse  training loss=0.0551\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 600\n",
      "mse  training loss=0.0554\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 700\n",
      "mse  training loss=0.0550\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 800\n",
      "mse  training loss=0.0555\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0014\n",
      "step: 900\n",
      "mse  training loss=0.0549\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1000\n",
      "mse  training loss=0.0538\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1100\n",
      "mse  training loss=0.0537\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1200\n",
      "mse  training loss=0.0542\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1300\n",
      "mse  training loss=0.0538\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1400\n",
      "mse  training loss=0.0546\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1500\n",
      "mse  training loss=0.0541\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1600\n",
      "mse  training loss=0.0530\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1700\n",
      "mse  training loss=0.0536\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1800\n",
      "mse  training loss=0.0524\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1900\n",
      "mse  training loss=0.0529\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2000\n",
      "mse  training loss=0.0527\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2100\n",
      "mse  training loss=0.0520\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2200\n",
      "mse  training loss=0.0520\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2300\n",
      "mse  training loss=0.0525\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2400\n",
      "mse  training loss=0.0523\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2500\n",
      "mse  training loss=0.0518\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2600\n",
      "mse  training loss=0.0517\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2700\n",
      "mse  training loss=0.0517\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37413f523ff94a078256d22764447445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0521\n",
      "epoch: 17\n",
      "step: 100\n",
      "mse  training loss=0.0515\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 200\n",
      "mse  training loss=0.0505\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 300\n",
      "mse  training loss=0.0509\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 400\n",
      "mse  training loss=0.0514\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 500\n",
      "mse  training loss=0.0510\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 600\n",
      "mse  training loss=0.0511\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 700\n",
      "mse  training loss=0.0498\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 800\n",
      "mse  training loss=0.0504\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 900\n",
      "mse  training loss=0.0501\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1000\n",
      "mse  training loss=0.0500\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1100\n",
      "mse  training loss=0.0506\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1200\n",
      "mse  training loss=0.0497\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1300\n",
      "mse  training loss=0.0498\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1400\n",
      "mse  training loss=0.0492\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1500\n",
      "mse  training loss=0.0499\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1600\n",
      "mse  training loss=0.0488\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1700\n",
      "mse  training loss=0.0487\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1800\n",
      "mse  training loss=0.0494\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1900\n",
      "mse  training loss=0.0481\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2000\n",
      "mse  training loss=0.0489\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2100\n",
      "mse  training loss=0.0484\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2200\n",
      "mse  training loss=0.0480\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2300\n",
      "mse  training loss=0.0483\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2400\n",
      "mse  training loss=0.0481\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2500\n",
      "mse  training loss=0.0473\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2600\n",
      "mse  training loss=0.0481\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2700\n",
      "mse  training loss=0.0475\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2298cd3224d040b1aae91cc84522c631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0481\n",
      "epoch: 18\n",
      "step: 100\n",
      "mse  training loss=0.0474\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 200\n",
      "mse  training loss=0.0471\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 300\n",
      "mse  training loss=0.0472\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 400\n",
      "mse  training loss=0.0471\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 500\n",
      "mse  training loss=0.0465\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 600\n",
      "mse  training loss=0.0466\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 700\n",
      "mse  training loss=0.0465\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 800\n",
      "mse  training loss=0.0463\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 900\n",
      "mse  training loss=0.0462\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1000\n",
      "mse  training loss=0.0462\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1100\n",
      "mse  training loss=0.0459\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1200\n",
      "mse  training loss=0.0458\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1300\n",
      "mse  training loss=0.0454\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1400\n",
      "mse  training loss=0.0457\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1500\n",
      "mse  training loss=0.0458\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1600\n",
      "mse  training loss=0.0454\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1700\n",
      "mse  training loss=0.0456\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1800\n",
      "mse  training loss=0.0454\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1900\n",
      "mse  training loss=0.0449\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2000\n",
      "mse  training loss=0.0444\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2100\n",
      "mse  training loss=0.0452\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2200\n",
      "mse  training loss=0.0446\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2300\n",
      "mse  training loss=0.0448\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2400\n",
      "mse  training loss=0.0448\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2500\n",
      "mse  training loss=0.0443\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2600\n",
      "mse  training loss=0.0446\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2700\n",
      "mse  training loss=0.0444\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3339fe098b64679a377e1e9a7b71cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0451\n",
      "epoch: 19\n",
      "step: 100\n",
      "mse  training loss=0.0440\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 200\n",
      "mse  training loss=0.0444\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 300\n",
      "mse  training loss=0.0440\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 400\n",
      "mse  training loss=0.0444\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 500\n",
      "mse  training loss=0.0439\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 600\n",
      "mse  training loss=0.0435\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 700\n",
      "mse  training loss=0.0436\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 800\n",
      "mse  training loss=0.0434\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 900\n",
      "mse  training loss=0.0434\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1000\n",
      "mse  training loss=0.0428\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1100\n",
      "mse  training loss=0.0432\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1200\n",
      "mse  training loss=0.0432\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1300\n",
      "mse  training loss=0.0430\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1400\n",
      "mse  training loss=0.0422\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1500\n",
      "mse  training loss=0.0433\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 1600\n",
      "mse  training loss=0.0432\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1700\n",
      "mse  training loss=0.0428\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1800\n",
      "mse  training loss=0.0430\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1900\n",
      "mse  training loss=0.0427\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2000\n",
      "mse  training loss=0.0423\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2100\n",
      "mse  training loss=0.0423\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2200\n",
      "mse  training loss=0.0425\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2300\n",
      "mse  training loss=0.0419\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2400\n",
      "mse  training loss=0.0424\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2500\n",
      "mse  training loss=0.0412\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2600\n",
      "mse  training loss=0.0418\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0013\n",
      "step: 2700\n",
      "mse  training loss=0.0422\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab82fa3aab594c70a435dc05c91f2e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0432\n",
      "epoch: 20\n",
      "step: 100\n",
      "mse  training loss=0.0419\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 200\n",
      "mse  training loss=0.0415\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 300\n",
      "mse  training loss=0.0417\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 400\n",
      "mse  training loss=0.0417\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 500\n",
      "mse  training loss=0.0415\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 600\n",
      "mse  training loss=0.0411\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 700\n",
      "mse  training loss=0.0412\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 800\n",
      "mse  training loss=0.0412\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 900\n",
      "mse  training loss=0.0416\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1000\n",
      "mse  training loss=0.0411\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1100\n",
      "mse  training loss=0.0406\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1200\n",
      "mse  training loss=0.0405\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1300\n",
      "mse  training loss=0.0412\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1400\n",
      "mse  training loss=0.0406\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1500\n",
      "mse  training loss=0.0407\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1600\n",
      "mse  training loss=0.0409\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1700\n",
      "mse  training loss=0.0406\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1800\n",
      "mse  training loss=0.0411\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1900\n",
      "mse  training loss=0.0411\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2000\n",
      "mse  training loss=0.0407\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2100\n",
      "mse  training loss=0.0410\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2200\n",
      "mse  training loss=0.0397\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2300\n",
      "mse  training loss=0.0406\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2400\n",
      "mse  training loss=0.0406\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2500\n",
      "mse  training loss=0.0403\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2600\n",
      "mse  training loss=0.0402\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2700\n",
      "mse  training loss=0.0399\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dca117bbe1c47f48d1f074635607288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0412\n",
      "epoch: 21\n",
      "step: 100\n",
      "mse  training loss=0.0400\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 200\n",
      "mse  training loss=0.0402\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 300\n",
      "mse  training loss=0.0393\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 400\n",
      "mse  training loss=0.0401\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 500\n",
      "mse  training loss=0.0390\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 600\n",
      "mse  training loss=0.0398\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 700\n",
      "mse  training loss=0.0397\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 800\n",
      "mse  training loss=0.0400\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 900\n",
      "mse  training loss=0.0397\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1000\n",
      "mse  training loss=0.0398\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1100\n",
      "mse  training loss=0.0395\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1200\n",
      "mse  training loss=0.0392\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1300\n",
      "mse  training loss=0.0395\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1400\n",
      "mse  training loss=0.0389\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1500\n",
      "mse  training loss=0.0391\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1600\n",
      "mse  training loss=0.0397\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1700\n",
      "mse  training loss=0.0392\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1800\n",
      "mse  training loss=0.0392\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 1900\n",
      "mse  training loss=0.0389\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2000\n",
      "mse  training loss=0.0390\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2100\n",
      "mse  training loss=0.0395\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2200\n",
      "mse  training loss=0.0387\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2300\n",
      "mse  training loss=0.0386\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2400\n",
      "mse  training loss=0.0390\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2500\n",
      "mse  training loss=0.0387\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0012\n",
      "step: 2600\n",
      "mse  training loss=0.0388\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2700\n",
      "mse  training loss=0.0387\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52d513f41d148e2ae7798a6cdeaf9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0397\n",
      "epoch: 22\n",
      "step: 100\n",
      "mse  training loss=0.0388\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 200\n",
      "mse  training loss=0.0385\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 300\n",
      "mse  training loss=0.0384\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 400\n",
      "mse  training loss=0.0383\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 500\n",
      "mse  training loss=0.0387\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 600\n",
      "mse  training loss=0.0386\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 700\n",
      "mse  training loss=0.0385\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 800\n",
      "mse  training loss=0.0383\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 900\n",
      "mse  training loss=0.0381\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1000\n",
      "mse  training loss=0.0385\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1100\n",
      "mse  training loss=0.0381\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1200\n",
      "mse  training loss=0.0378\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1300\n",
      "mse  training loss=0.0382\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1400\n",
      "mse  training loss=0.0374\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1500\n",
      "mse  training loss=0.0376\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1600\n",
      "mse  training loss=0.0378\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1700\n",
      "mse  training loss=0.0376\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1800\n",
      "mse  training loss=0.0379\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1900\n",
      "mse  training loss=0.0379\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2000\n",
      "mse  training loss=0.0378\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2100\n",
      "mse  training loss=0.0375\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2200\n",
      "mse  training loss=0.0377\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2300\n",
      "mse  training loss=0.0376\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2400\n",
      "mse  training loss=0.0376\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2500\n",
      "mse  training loss=0.0377\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2600\n",
      "mse  training loss=0.0373\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2700\n",
      "mse  training loss=0.0376\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f020aa5c7a7496cbf7adcd14ef9a297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0383\n",
      "epoch: 23\n",
      "step: 100\n",
      "mse  training loss=0.0375\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 200\n",
      "mse  training loss=0.0377\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 300\n",
      "mse  training loss=0.0370\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 400\n",
      "mse  training loss=0.0372\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 500\n",
      "mse  training loss=0.0366\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 600\n",
      "mse  training loss=0.0371\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 700\n",
      "mse  training loss=0.0374\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 800\n",
      "mse  training loss=0.0371\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 900\n",
      "mse  training loss=0.0370\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1000\n",
      "mse  training loss=0.0371\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1100\n",
      "mse  training loss=0.0370\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1200\n",
      "mse  training loss=0.0371\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1300\n",
      "mse  training loss=0.0370\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1400\n",
      "mse  training loss=0.0370\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1500\n",
      "mse  training loss=0.0370\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1600\n",
      "mse  training loss=0.0368\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1700\n",
      "mse  training loss=0.0369\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1800\n",
      "mse  training loss=0.0369\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1900\n",
      "mse  training loss=0.0364\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2000\n",
      "mse  training loss=0.0368\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2100\n",
      "mse  training loss=0.0366\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2200\n",
      "mse  training loss=0.0367\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2300\n",
      "mse  training loss=0.0365\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2400\n",
      "mse  training loss=0.0362\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2500\n",
      "mse  training loss=0.0363\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2600\n",
      "mse  training loss=0.0364\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 2700\n",
      "mse  training loss=0.0363\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a10d2356d544f36bd41e42e6d922f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0373\n",
      "epoch: 24\n",
      "step: 100\n",
      "mse  training loss=0.0358\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 200\n",
      "mse  training loss=0.0362\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 300\n",
      "mse  training loss=0.0363\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 400\n",
      "mse  training loss=0.0359\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 500\n",
      "mse  training loss=0.0359\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 600\n",
      "mse  training loss=0.0362\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 700\n",
      "mse  training loss=0.0359\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 800\n",
      "mse  training loss=0.0366\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 900\n",
      "mse  training loss=0.0356\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1000\n",
      "mse  training loss=0.0363\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1100\n",
      "mse  training loss=0.0360\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1200\n",
      "mse  training loss=0.0361\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1300\n",
      "mse  training loss=0.0362\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1400\n",
      "mse  training loss=0.0358\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0011\n",
      "step: 1500\n",
      "mse  training loss=0.0358\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1600\n",
      "mse  training loss=0.0362\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1700\n",
      "mse  training loss=0.0355\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1800\n",
      "mse  training loss=0.0358\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1900\n",
      "mse  training loss=0.0358\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2000\n",
      "mse  training loss=0.0357\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2100\n",
      "mse  training loss=0.0358\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2200\n",
      "mse  training loss=0.0358\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2300\n",
      "mse  training loss=0.0357\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2400\n",
      "mse  training loss=0.0350\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2500\n",
      "mse  training loss=0.0361\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2600\n",
      "mse  training loss=0.0355\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2700\n",
      "mse  training loss=0.0355\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5980c6c22248149f448388ef6aab58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0366\n",
      "epoch: 25\n",
      "step: 100\n",
      "mse  training loss=0.0355\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 200\n",
      "mse  training loss=0.0354\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 300\n",
      "mse  training loss=0.0353\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 400\n",
      "mse  training loss=0.0359\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 500\n",
      "mse  training loss=0.0354\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 600\n",
      "mse  training loss=0.0357\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 700\n",
      "mse  training loss=0.0351\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 800\n",
      "mse  training loss=0.0350\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 900\n",
      "mse  training loss=0.0355\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1000\n",
      "mse  training loss=0.0349\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1100\n",
      "mse  training loss=0.0354\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1200\n",
      "mse  training loss=0.0353\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1300\n",
      "mse  training loss=0.0349\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1400\n",
      "mse  training loss=0.0352\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1500\n",
      "mse  training loss=0.0349\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1600\n",
      "mse  training loss=0.0349\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1700\n",
      "mse  training loss=0.0348\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1800\n",
      "mse  training loss=0.0350\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1900\n",
      "mse  training loss=0.0344\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2000\n",
      "mse  training loss=0.0350\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2100\n",
      "mse  training loss=0.0350\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2200\n",
      "mse  training loss=0.0351\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2300\n",
      "mse  training loss=0.0348\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2400\n",
      "mse  training loss=0.0344\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2500\n",
      "mse  training loss=0.0354\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2600\n",
      "mse  training loss=0.0350\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2700\n",
      "mse  training loss=0.0351\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65d90b2992547c88c5e21351284f64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0359\n",
      "epoch: 26\n",
      "step: 100\n",
      "mse  training loss=0.0351\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 200\n",
      "mse  training loss=0.0345\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 300\n",
      "mse  training loss=0.0344\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 400\n",
      "mse  training loss=0.0348\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 500\n",
      "mse  training loss=0.0346\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 600\n",
      "mse  training loss=0.0349\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 700\n",
      "mse  training loss=0.0340\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 800\n",
      "mse  training loss=0.0340\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 900\n",
      "mse  training loss=0.0344\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1000\n",
      "mse  training loss=0.0344\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1100\n",
      "mse  training loss=0.0348\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1200\n",
      "mse  training loss=0.0346\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1300\n",
      "mse  training loss=0.0342\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1400\n",
      "mse  training loss=0.0345\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1500\n",
      "mse  training loss=0.0345\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1600\n",
      "mse  training loss=0.0346\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1700\n",
      "mse  training loss=0.0344\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1800\n",
      "mse  training loss=0.0344\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1900\n",
      "mse  training loss=0.0344\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2000\n",
      "mse  training loss=0.0347\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2100\n",
      "mse  training loss=0.0339\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2200\n",
      "mse  training loss=0.0344\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2300\n",
      "mse  training loss=0.0339\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2400\n",
      "mse  training loss=0.0340\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2500\n",
      "mse  training loss=0.0342\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2600\n",
      "mse  training loss=0.0336\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2700\n",
      "mse  training loss=0.0347\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c8fbf5b8c944468e53aea649fb2656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0351\n",
      "epoch: 27\n",
      "step: 100\n",
      "mse  training loss=0.0345\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 200\n",
      "mse  training loss=0.0343\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 300\n",
      "mse  training loss=0.0340\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 400\n",
      "mse  training loss=0.0340\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 500\n",
      "mse  training loss=0.0336\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 600\n",
      "mse  training loss=0.0338\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 700\n",
      "mse  training loss=0.0338\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 800\n",
      "mse  training loss=0.0341\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 900\n",
      "mse  training loss=0.0332\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1000\n",
      "mse  training loss=0.0337\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1100\n",
      "mse  training loss=0.0337\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1200\n",
      "mse  training loss=0.0335\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1300\n",
      "mse  training loss=0.0339\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1400\n",
      "mse  training loss=0.0334\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1500\n",
      "mse  training loss=0.0341\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1600\n",
      "mse  training loss=0.0339\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1700\n",
      "mse  training loss=0.0336\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0339\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1900\n",
      "mse  training loss=0.0339\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2000\n",
      "mse  training loss=0.0338\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2100\n",
      "mse  training loss=0.0339\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2200\n",
      "mse  training loss=0.0333\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2300\n",
      "mse  training loss=0.0332\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2400\n",
      "mse  training loss=0.0332\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2500\n",
      "mse  training loss=0.0343\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2600\n",
      "mse  training loss=0.0337\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 2700\n",
      "mse  training loss=0.0330\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3415d86641bd49c1ad113d3f337c1e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0346\n",
      "epoch: 28\n",
      "step: 100\n",
      "mse  training loss=0.0333\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 200\n",
      "mse  training loss=0.0338\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0340\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 400\n",
      "mse  training loss=0.0337\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 500\n",
      "mse  training loss=0.0336\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 600\n",
      "mse  training loss=0.0336\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0329\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0336\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0337\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1000\n",
      "mse  training loss=0.0329\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0328\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0331\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1300\n",
      "mse  training loss=0.0334\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0332\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0332\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1600\n",
      "mse  training loss=0.0330\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0331\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0327\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 1900\n",
      "mse  training loss=0.0331\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0330\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0333\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0331\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0329\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0330\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0332\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0327\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0329\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364c77b106bb4db6b39a99bca4a82d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0341\n",
      "epoch: 29\n",
      "step: 100\n",
      "mse  training loss=0.0329\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0331\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0331\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0010\n",
      "step: 400\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 500\n",
      "mse  training loss=0.0326\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0329\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0328\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0328\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0329\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0330\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0330\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0325\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0327\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0328\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0332\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0325\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0328\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0327\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0325\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0325\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0322\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0326\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0326\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0325\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540202214f3744a38788a4bddc940e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0334\n",
      "epoch: 30\n",
      "step: 100\n",
      "mse  training loss=0.0328\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0325\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0327\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0322\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 500\n",
      "mse  training loss=0.0322\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0325\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0322\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0328\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0324\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0321\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0326\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0320\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0322\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0327\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0324\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0319\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0323\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0324\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0321\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0321\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0315\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9bd3c47dec43c5ac8eda04af4e66c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0330\n",
      "epoch: 31\n",
      "step: 100\n",
      "mse  training loss=0.0320\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0320\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 500\n",
      "mse  training loss=0.0320\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0320\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0320\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0319\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0319\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0321\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0319\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0321\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0319\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f037011286324155b34f8c5bcebfbfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0326\n",
      "epoch: 32\n",
      "step: 100\n",
      "mse  training loss=0.0319\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0314\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0314\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 500\n",
      "mse  training loss=0.0315\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0320\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0315\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0319\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0318\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0315\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0315\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0319\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0314\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0314\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0315\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0314\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e693914aa5458983dfba3e40813023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0324\n",
      "epoch: 33\n",
      "step: 100\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0315\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 500\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0310\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0316\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0317\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0314\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa8dc2f0c8c4f2594fb93c59beee589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0322\n",
      "epoch: 34\n",
      "step: 100\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 500\n",
      "mse  training loss=0.0310\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0310\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0314\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0310\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0313\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0310\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0310\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0310\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ec9c22a50d42198555b2669910b9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0317\n",
      "epoch: 35\n",
      "step: 100\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0312\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 500\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0311\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0310\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0309\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0314\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3cc1e45ea041afadd56c51d8853637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0319\n",
      "epoch: 36\n",
      "step: 100\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 500\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0310\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2700\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf9dcad18d94d78b93c55a982c290bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0314\n",
      "epoch: 37\n",
      "step: 100\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0308\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 500\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1000\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0306\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f2f50074bb4e018e0a70bf6039565f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0315\n",
      "epoch: 38\n",
      "step: 100\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0307\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1200\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1400\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1700\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2400\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2600\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88af6e9cf9ed4ffca6dbaf77c51e5b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0312\n",
      "epoch: 39\n",
      "step: 100\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0304\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2400\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b47ae0e7aa74f93b2cb0f7ca422ad68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0310\n",
      "epoch: 40\n",
      "step: 100\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 200\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1300\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1400\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0305\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2700\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5308d67b564f7d9311766c502381ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0309\n",
      "epoch: 41\n",
      "step: 100\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 300\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 700\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1500\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2400\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2700\n",
      "mse  training loss=0.0303\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5858676c535946aabb951575b79bf5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0309\n",
      "epoch: 42\n",
      "step: 100\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 200\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 700\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 900\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1100\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2200\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5654b6e0c33846e9bb9185a6289e15aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0308\n",
      "epoch: 43\n",
      "step: 100\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 200\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1400\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2700\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb126c8b25446f3aaaa7331604a2fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0308\n",
      "epoch: 44\n",
      "step: 100\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 200\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 600\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 700\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1400\n",
      "mse  training loss=0.0293\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2400\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2500\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2700\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007ce7f2c1f4469aa8d15d1f2abc5235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0306\n",
      "epoch: 45\n",
      "step: 100\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 200\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 700\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1400\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1600\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2400\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2700\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1fac29dcaa42bc844bdd94859f7060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0307\n",
      "epoch: 46\n",
      "step: 100\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 200\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 700\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1400\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0302\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0293\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2700\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe44eb307b694cd3931af4cd54754aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0306\n",
      "epoch: 47\n",
      "step: 100\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 200\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0293\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 700\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1400\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2400\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0300\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0293\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bdd68dc43a4c638a2273c1c3686c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0306\n",
      "epoch: 48\n",
      "step: 100\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 700\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 800\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1400\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0293\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2000\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2400\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0293\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2700\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e181cfdaba54b0e84b0cc39e4bbeee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0305\n",
      "epoch: 49\n",
      "step: 100\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 200\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0293\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 400\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 700\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1400\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1800\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 1900\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2100\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2300\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2400\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2dd0751cfb46a881e6dcf85be3a8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0308\n",
      "epoch: 50\n",
      "step: 100\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 200\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 300\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 400\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 500\n",
      "mse  training loss=0.0293\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 600\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 700\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 800\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 900\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1000\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1100\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1200\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1300\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1400\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1500\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1600\n",
      "mse  training loss=0.0301\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1700\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1800\n",
      "mse  training loss=0.0295\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 1900\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2000\n",
      "mse  training loss=0.0293\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2100\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2200\n",
      "mse  training loss=0.0299\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2300\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2400\n",
      "mse  training loss=0.0294\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2500\n",
      "mse  training loss=0.0296\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n",
      "step: 2600\n",
      "mse  training loss=0.0297\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0009\n",
      "step: 2700\n",
      "mse  training loss=0.0298\n",
      "L_T  training loss=0.0000\n",
      "rounding  training loss=0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dff833fddd499cadfeca28d62f682d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.0307\n"
     ]
    }
   ],
   "source": [
    "loss_terms_dict_lst = []\n",
    "loss_terms_weights = {'mse':1, 'L_T':1, 'rounding': 1}\n",
    "progress_bar = tqdm(range(num_epoch*len(train_dataloader)))\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    print(\"epoch:\",epoch+1)\n",
    "    loss_terms_dict_lst.append(train(diffusion_model=diffusion_model, dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler ,progress_bar=progress_bar, loss_terms_weights=loss_terms_weights ,verbose=True))\n",
    "    evaluate(diffusion_model=diffusion_model, dataloader=eval_dataloader, loss_terms_weights=loss_terms_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_terms_dict = {'mse':[], 'L_T':[], 'rounding':[]}\n",
    "for key in loss_terms_dict_lst[0].keys():\n",
    "    for ep in range(num_epoch):\n",
    "        loss_terms_dict[key] += loss_terms_dict_lst[ep][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_terms_dict['mse'], label='mse')\n",
    "plt.plot(loss_terms_dict['rounding'], label='rounding')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(diffusion_model.state_dict(), \"checkpoints/roc_unshared_dim128_embnoise.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_T = torch.randn(size=(batch_size, max_len, word_embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a35b832b46483fb182dc148ed84295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logits, hidden_states = diffusion_model.sample(x_T.to(device), return_hidden_states=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "lessons [UNK] Bobby laying grew teacher . brother children roommate threw his fan calm and videos hurt everything bags It at arrived thought beginning close worth York It friends Unfortunately came became order lots and shirt contest Jenny that do pass bathroom . never day city . purchase showed After full go . in had an stove to each and an to run very Two Alice home a his room brother of\n",
      "step: 1500\n",
      "puppy dinner moving of , and to Then But my would single 's sunny offered amazed big of Tina got friends so haircut mall burned went really driving However she would proudly n't an month several be how for hour road very . [END] lost her the [PAD] a [PAD] found got [PAD] [PAD] found every [PAD] got each got [PAD] challenged not to [PAD] would that tried all [PAD] all bar\n",
      "step: 1800\n",
      "[START] account this mother being bottle instead She put my her getting . watched . woman we vet it a There so been had down love for put and friends would decided to all few No broke they . fence stage best . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] not all [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] would [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "step: 1900\n",
      "[START] was ordered had , , instead broken But them from his . much goal Tom different this has a There and been . area . , a friends her an it she had with closer I an onto was singing birthday . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] all [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "step: 1950\n",
      "[START] camping needed his 's , caught could game she was his . shirt I Tom buy that has a loved and been a got . , a friends I an it to had with closer not So has [UNK] ! would . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "step: 1980\n",
      "[START] was ordered his work , few great apartment the drunk his . [UNK] I Tom have friends asked a loved and pounds a area . He a friends much an it a they with at to his of ! towards would . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "step: 1990\n",
      "[START] for ordered his work , few great apartment the tried his . ! . Tom have friends asked a She and pounds a got . He agreed that much up it a they with at to his of was girls would . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "step: 1995\n",
      "[START] for ordered his man , trouble . apartment the out his . house . Tom have friends asked a She and list a work . He agreed that much up it a they the in to his of was to would . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "step: 1996\n",
      "[START] for ordered his friends , trouble . apartment the out his first much . Tom have friends asked a She and pounds a The . He agreed that much up it a they the head to his of was to would . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "step: 1997\n",
      "[START] for ordered his friends , trouble . apartment the out his first house . Tom have friends asked a She and pounds a work . He agreed friends much up it a they the head to his of was to would . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "step: 1998\n",
      "[START] for ordered his friends , trouble . apartment the out his first house . Tom have friends asked a table and pounds a get . He agreed friends much up it a an the head to his of was she would . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "step: 1999\n",
      "[START] for ordered his friends , high . apartment the out his first house . Tom have dogs asked a table and done a get . He agreed friends much up it a band the head to his of was she would . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "for step in [0,1500,1800,1900,1950,1980,1990,1995,1996,1997,1998,1999]:\n",
    "    hidden_state = hidden_states[step][sample_idx]\n",
    "    with torch.no_grad():\n",
    "        hidden_logits = diffusion_model.lm_head(hidden_state)\n",
    "        sampled_ids = torch.argmax(hidden_logits,dim=-1).cpu()\n",
    "        sampled_seq = [rev_tokenizer[token_id.item()] for token_id in sampled_ids]\n",
    "        print(\"step:\", step)\n",
    "        print(\" \".join(sampled_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_idx: 0\n",
      "[START] for ordered his friends , high . apartment the out his first house . Tom have dogs asked a table and done a get . He agreed friends much up it a band the head to his of was she would . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 1\n",
      "[START] The girls cat cat who , son his His next class . was Thanksgiving to horse about [UNK] on his playing noise . They was mother started summer . friends the like . His work and until set his the next day . . The of [UNK] and the day of the a new both next . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 2\n",
      "[START] A wife the . wanted to [UNK] . She was party moved to take , day . She decided to mom her . up on into and finally , too . The would a would and the [UNK] more . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 3\n",
      "[START] Ann loved man had off . She bought It was service at the night . The hitting was [UNK] and could . They was They every find their it them . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 4\n",
      "[START] Before had at wanted the college . But she is a a , while ground . So apples was that , her way the a . They right all is so was their finally up was . When all this in at and won stop night . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 5\n",
      "[START] Dan made what outside getting lot for a trip to day . Finally , the . her best over . Every . at the is and could . together was back all and it some I spent her no few to it smiled just to get just great her . . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 6\n",
      "[START] Bill was working drive hard . needed was made to a beach . The said them the right , she date of lots [UNK] her . He was father and asked 's very in the . His [UNK] get way to go [UNK] over and off to his to wear . He ended with her they as a did fun now it . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 7\n",
      "[START] my had a to get my friends . He I could of [UNK] finished . They was not to sleep on how all by car in the about hour . He at a around the . for in them . of a [UNK] and money night . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 8\n",
      "[START] There man driving new the street . But to went drove to work . restaurant for the park come I decided a lot in a got and , . He did ran on a big , he and was it got the end , were drive to ! [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 9\n",
      "[START] Kelly was upset a with work one a [UNK] to go and . She decided to pick at to on the Ashley . His hot to be I . All do up down with got there fell . She 's up a better was so [UNK] more able two of . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 10\n",
      "[START] George went to the year money has [UNK] later by famous . slowly a little him about to the inside . thought many people so Jon to his to play of hard work first up away . The next was would but a new he than he . The . At . ! and is his when at . year [UNK] . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 11\n",
      "[START] Mary loved she old always to school movie to the . She spent car drove her to enough to her mom . was to let her was n't want when to play . She enjoyed a . That There for she to [UNK] the her . one [UNK] I was was happy , she would it she of her puppy ! [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 12\n",
      "[START] Liz a for a to work 's was . Suddenly only to check the in the always pizza see but boy . She could to try her doctor . saw and laughed could up the doctor and her a dress and her . She . rode then moved take set home a great her . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 13\n",
      "[START] I decided to read my on with class . At a before a great of and one . I was my long It . I to . I came - it . When started to not want my next it of it and again . I had that went so but I 's again . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 14\n",
      "[START] Tom was to play a on his . One decided to with it day . He tried something for money a He was and said his year - . He had [UNK] until and . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 15\n",
      "[START] Warren went to the dog for store on one [UNK] to . She opened very [UNK] it started , the grandma over . As all of a so the went go and . The store last dog his . 's they to refused and were their [UNK] new of the . got were they a bought the door . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 16\n",
      "[START] [UNK] always if . a at her beach . He sat some . It and was inside up . [UNK] off one called very the and bought his the he was . perfect , was more her own . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 17\n",
      "[START] Sam is his to . new his was very a It . His took to a very . . He . to for to no the with back back . He his took parents sick . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 18\n",
      "[START] The [UNK] the a [UNK] football in dream . She needed all noticed to [UNK] her time them was . 's to get to broke and to house he out . watched was sad they could . The second a again felt with his . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 19\n",
      "[START] Jane had a book was one window town . We asked gave nervous long to I after were and worried never . the home . My their [UNK] by milk . The kids the , he home but being felt small . ate for just to back employee and as better have her later . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 20\n",
      "[START] I just a them to . . Once of a tomorrow and man were looking in the movie on I met my . He had finally ticket . Unfortunately took it looked I put an to play the ! from the movie . and arrived and was their got and with hour . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 21\n",
      "[START] Tom is family to play with school . They . She wanted to get decided some [UNK] 's the time had was . . my , my very more of the were sure are back an . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 22\n",
      "[START] Tom was the were at more on summer . He asked almost a to high with . He went to his head him for Gwen him . on home . He wanted to play his first to house was caught [UNK] river continued the other to . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 23\n",
      "[START] Mary just mouth told dance with late baby [UNK] they . She tried not all what worked . old they By doing a bought Jill the Finally her a driver . her dog the dating was mom time said he immediately had from the . Gina ! . Everyone - paid was her at laughed and the time . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 24\n",
      "[START] . failed an for cookies . It that the first where to buy . Then on some happy was time only car looked came in to at she had of her . . Tom were n't good they a . She did n't so money was out on and was the back her . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 25\n",
      "[START] My has been dog [UNK] on a . staying in the been for all the [UNK] and I with the [UNK] . He called the it to get . To and had to [UNK] from wondered on The park . The a the girl to go to play a perfect away . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 26\n",
      "[START] [UNK] was a the - a daughter work . He all a special the bright . He asked was to his it for him the everything ! she saw back . it , to wait very for use his it . He made was best as his into one would than pay . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 27\n",
      "[START] I used the to having on a while shopping n't local would . decided to them and liked all the into by . It were . on was one up and went reading blanket to . As finally store walk got through took it The way at the was happy they got even [UNK] 's 's to . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 28\n",
      "[START] [UNK] was trying died the as it . I . I worked at about accident the morning . They got a lost and went asked all house at the a . . It ended the ignored to play get days . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 29\n",
      "[START] Tom was walking the to the pick on the line She wanted going to the crowd though and by . When had there told the baby water . She called he showed . Tom her . , for they was was not to . in him . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 30\n",
      "[START] My wife her went that would her only was would grandmother his . The first from him and . a People . watch [UNK] an met and [UNK] . The So the next the took . They the few eight ! [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 31\n",
      "[START] Kate [UNK] the to the old in in it . She do that [UNK] the hour the was to to mom died . When then were time to be getting the we her her . During , I later out had hour , had The I a Tammy and as the more had . on . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for sample_idx in range(32):\n",
    "    hidden_state = hidden_states[-1][sample_idx]\n",
    "    with torch.no_grad():\n",
    "        hidden_logits = diffusion_model.lm_head(hidden_state)\n",
    "        sampled_ids = torch.argmax(hidden_logits,dim=-1).cpu()\n",
    "        sampled_seq = [rev_tokenizer[token_id.item()] for token_id in sampled_ids]\n",
    "        print(\"sample_idx:\", sample_idx)\n",
    "        print(\" \".join(sampled_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a240a34ebf4aed90764ff2cb07ce66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logits, hidden_states = diffusion_model.sample(x_T.to(device),clamp='rounding', return_hidden_states=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_idx: 0\n",
      "[START] I wanted of friend Jenny . They to a . . . Then I picked and to to . One was able . Tom class to a the . He decided to [UNK] to not it and the . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 1\n",
      "[START] Tom and a took to for in test . a decided an the box called [UNK] [UNK] was . to the food it his morning . . She bought the [UNK] . The the a the . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 2\n",
      "[START] Today was n't for on her friends a . told thought in a to . she . to did n't children had . He were to a sick . His [UNK] , . She the date new . night the the of wait friends to an up to and not and the his . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 3\n",
      "[START] Eric was a a boat with to his went out He had to buy night at a date . of of a water for it . . He he made at again . But in so he the away [UNK] home . his house it . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 4\n",
      "[START] Mary wanted to a lot the a . She was test . . the . the was no people on . They to a He . She Tina at the it it the and a got . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 5\n",
      "[START] The was very to something . She her tried out him it . . . It from to the scared car . When out it . She were let and time friend there off . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 6\n",
      "[START] I wanted to eat I my . He I wanted it the this was was . . [UNK] month with to the it . up told had a food and . The middle the had . the my a in of . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 7\n",
      "[START] I was been eat . . The job , was not a new and . When . Finally she I all . She the in in the store . One day the face in go to . she to her lost it . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 8\n",
      "[START] . went to a at work than She asked . the teacher them . a top . a went . It stopped . was in the dog girlfriend [UNK] . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 9\n",
      "[START] There was a a with the store . I noticed the each She way . . in . When for school to [UNK] [UNK] her . I had sick . She had lost to the door in and there [UNK] The . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 10\n",
      "[START] friends needed went was . He day her little at her . She . I to the the . The and got man . . Unfortunately a small . the the able time the a . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 11\n",
      "[START] One and up from a great [UNK] one of to trip . He was week was about a large . The the . She next , he could asked me was It him from how to [UNK] . . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 12\n",
      "[START] I wanted to wear . . He was on my money buy . . I decided to the and I the . He . He is it set through his family . I could a what in the . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 13\n",
      "[START] Sam had a new his . She got had to . He were in . tried every and liked . to . . off was an him . She with could the . the and was all . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 14\n",
      "[START] Tom was never to . . He decided he had down a game in . to get all . Finally him to make work . He it and the broke told he an the . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 15\n",
      "[START] Tom was a play . He had . He a found to the not on money a . He started the . . was his mother to Tom his going the was for a job . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 16\n",
      "[START] Tom was at . decided . He told He a trip too to get . When he more his up to it . was [UNK] to get better n't . He were to make out . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 17\n",
      "[START] Jane wanted to [UNK] [UNK] . She . she was . The with her to back . When the friends all so . She the use . for to 's her to buy to get her the , . She asked two a the and ! [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 18\n",
      "[START] I kept to make a the [UNK] . He cut time a a long . He went to the pizza that it though . to a had it . She was of her . . I found the had baby finally them . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 19\n",
      "[START] Dan was walking not that to on the They . He was when to to a up . he could . . After had for to his was . He had was he , the food . He sat Eric of it to a better [UNK] . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 20\n",
      "[START] Bob met his on the class . He decided to his and to water . They he to one the have to waited could . . The was was a at was for his . John . . . had the in then into . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 21\n",
      "[START] The man His friends . . His of went to his help . He pulled out . He the him . He then . His got to do . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 22\n",
      "[START] Sam was was a get pet . in . She could arrived to get a She wanted to not of a . store . She broke excited would local have some and . [UNK] took . ran to to to that . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 23\n",
      "[START] Bob was up [UNK] at my friends He . He to [UNK] . He started I to him . He . His told school all too It another and were the water was . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 24\n",
      "[START] I was his a with . I I friend . I was of go to take the walk I . I and over to make the to got and went . over down . I was like had me . I it better to help and and the ! [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 25\n",
      "[START] Tim ran was [UNK] for He [UNK] They home . It the with The got it started set the doctor together . job . Tim ate would this . friends . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 26\n",
      "[START] I loved to have in . I my new her test . I found His . get the . The got last was n't know . . I to the friend a [UNK] . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 27\n",
      "[START] I had a new the a and . He told she to take . He was a lot . I was would the different became making . They was but how [UNK] . and at to [UNK] it . the but . was had her , not to eat the before again . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 28\n",
      "[START] We had a new new . She decided to see the . We tried in the [UNK] some He [UNK] . She a so eyes . there the to play for [UNK] to her She was [UNK] . the and . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 29\n",
      "[START] I was a home wanted at their . I found test . . I it to show on the . I found place of . On how they decided to a lot it . They the I was with people I was to . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 30\n",
      "[START] I has a great my of She to a baby in . When I started I decided to . When I went . , in a new mother on off . He . He I broke him . got came and the . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sample_idx: 31\n",
      "[START] My and a hungry . He was work to try new books . nervous a bought . He was He got could he . He started he looked a . He is . He him . [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for sample_idx in range(32):\n",
    "    hidden_state = hidden_states[-1][sample_idx]\n",
    "    with torch.no_grad():\n",
    "        hidden_logits = diffusion_model.lm_head(hidden_state)\n",
    "        sampled_ids = torch.argmax(hidden_logits,dim=-1).cpu()\n",
    "        sampled_seq = [rev_tokenizer[token_id.item()] for token_id in sampled_ids]\n",
    "        print(\"sample_idx:\", sample_idx)\n",
    "        print(\" \".join(sampled_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4805,  0.3161,  1.1531,  ...,  1.6092,  0.0171, -0.0755],\n",
       "        [ 0.5296, -0.2127,  0.8692,  ..., -1.3619,  0.0922,  0.2312],\n",
       "        [-0.5909, -0.2637, -0.1425,  ...,  0.2933, -0.0284,  0.2060],\n",
       "        ...,\n",
       "        [-0.0487, -0.1270, -0.7429,  ..., -0.4486, -0.2888,  0.6939],\n",
       "        [-0.0635,  0.0507, -0.6597,  ..., -0.0198,  0.6503,  1.5029],\n",
       "        [ 0.2038, -0.0694,  0.1111,  ...,  1.4222, -0.0120, -0.0297]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.word_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4805,  0.3161,  1.1531,  ...,  1.6092,  0.0171, -0.0755],\n",
       "        [ 0.5296, -0.2127,  0.8692,  ..., -1.3619,  0.0922,  0.2312],\n",
       "        [-0.5909, -0.2637, -0.1425,  ...,  0.2933, -0.0284,  0.2060],\n",
       "        ...,\n",
       "        [-0.0487, -0.1270, -0.7429,  ..., -0.4486, -0.2888,  0.6939],\n",
       "        [-0.0635,  0.0507, -0.6597,  ..., -0.0198,  0.6503,  1.5029],\n",
       "        [ 0.2038, -0.0694,  0.1111,  ...,  1.4222, -0.0120, -0.0297]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.lm_head.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-2.8335e+00,  5.0157e-01,  1.5772e+00,  1.6008e+00,  1.3920e+00,\n",
       "        -5.0213e-01,  6.7412e-01,  1.5496e+00,  1.5888e+00, -1.4500e+00,\n",
       "         5.5731e-02,  1.8086e+00,  4.3691e-01, -4.1515e-01,  1.5522e+00,\n",
       "         1.0511e+00,  1.1348e+00, -1.7812e+00, -2.4732e+00,  1.0588e-01,\n",
       "         9.5429e-01,  5.0374e-01, -1.8397e+00,  1.2936e+00, -8.1339e-01,\n",
       "         1.4371e+00,  2.4678e-01, -4.5192e-02,  5.2514e-01, -1.9127e+00,\n",
       "         9.4692e-01,  1.6614e+00,  1.1508e+00,  1.1107e+00,  7.1305e-01,\n",
       "        -1.9485e+00,  1.5000e+00, -5.3676e-01,  2.8657e+00,  1.0546e+00,\n",
       "         1.1212e+00,  1.6808e+00,  1.0841e+00, -8.2300e-01, -3.2293e-01,\n",
       "         6.3604e-01,  2.0501e+00,  1.4522e+00,  1.5419e+00,  2.6134e+00,\n",
       "         6.9575e-01,  1.0382e+00,  9.5937e-01,  1.4109e+00,  5.3004e-01,\n",
       "        -9.3019e-01,  2.7558e-01, -1.3236e+00,  5.6241e-01, -6.0610e-01,\n",
       "         1.2124e+00,  1.5894e+00, -1.8690e+00, -8.2687e-01,  2.2712e+00,\n",
       "         2.2624e-01, -1.3299e+00,  5.6966e-01, -4.6062e-01,  4.5010e-01,\n",
       "         1.3629e+00, -5.4711e-01, -1.5696e+00,  7.0217e-01, -5.5777e-01,\n",
       "         3.8315e-01,  9.4528e-01, -5.9141e-01,  4.6345e-01,  8.8182e-01,\n",
       "        -2.6876e+00, -5.6862e-01, -4.8524e-01,  4.5892e-01, -1.3430e+00,\n",
       "        -1.6164e+00, -1.4113e+00, -1.3470e+00, -2.0702e+00, -1.3548e+00,\n",
       "        -9.9693e-01, -1.6185e-01,  7.6383e-01, -1.4749e+00,  2.0719e+00,\n",
       "         8.0032e-01,  8.6660e-01,  1.1376e+00, -1.6291e+00, -7.7805e-01,\n",
       "         5.3737e-01, -1.3718e+00,  8.4901e-02, -8.5533e-02,  1.0057e+00,\n",
       "        -1.0040e+00,  4.9087e-01,  1.2737e+00, -2.9288e+00, -2.1728e-01,\n",
       "        -1.3429e+00, -7.0048e-01,  1.5404e+00,  1.0897e+00,  6.7441e-01,\n",
       "        -1.4411e-01,  6.0059e-02, -2.0268e+00,  8.3244e-01,  1.0802e+00,\n",
       "         6.5404e-01,  5.9723e-01, -1.1090e+00, -3.4025e-01, -1.0054e+00,\n",
       "        -9.8894e-01, -3.9088e-01, -6.0450e-01, -1.4241e+00,  2.1113e-01,\n",
       "         9.1996e-01, -1.8373e+00, -2.8899e-01, -1.8865e+00, -1.1191e+00,\n",
       "         1.1221e+00, -1.3348e+00, -2.7002e-03,  7.7816e-01,  4.1012e-02,\n",
       "        -4.2724e-01, -3.9356e-01, -2.7554e+00, -1.7569e+00,  2.3960e-01,\n",
       "         1.5146e-01, -1.3053e+00, -8.6386e-01, -6.8802e-01, -8.3301e-02,\n",
       "         9.8618e-01, -2.0417e-01,  3.5691e-01, -1.8749e+00,  7.6795e-01,\n",
       "        -1.1983e+00, -1.5048e+00, -3.9729e-01,  6.9326e-01, -7.7732e-01,\n",
       "        -1.5475e+00, -1.9242e+00, -7.8463e-01,  7.0572e-01, -2.6570e-01,\n",
       "         1.4531e-01, -3.4811e-01, -1.3445e+00, -1.4703e-01,  2.9809e-01,\n",
       "         6.1516e-01,  3.8777e-02,  1.3243e-02, -1.0308e+00, -1.6038e-01,\n",
       "        -7.1201e-02, -1.0376e-01,  6.2565e-01, -4.4684e-01, -2.6118e+00,\n",
       "        -7.1432e-01, -1.2570e-02, -1.1770e+00, -1.3927e+00, -1.3348e+00,\n",
       "        -5.6228e-01, -1.7833e+00, -1.4223e-01,  3.5739e-01, -6.6882e-01,\n",
       "         1.0440e+00, -1.8172e+00, -8.6076e-01, -5.5429e-01, -1.0419e+00,\n",
       "        -3.7528e+00, -1.4577e+00,  5.7526e-01,  3.3782e-01, -4.0617e-01,\n",
       "        -9.6479e-01, -3.4319e+00, -9.5857e-01, -7.1425e-01, -4.3030e-04,\n",
       "         9.3452e-01, -8.1979e-01, -2.0845e+00, -2.1850e+00, -1.3638e+00,\n",
       "        -1.3210e+00, -2.2036e+00, -2.3111e+00, -1.9984e+00, -1.8145e+00,\n",
       "         9.4418e-01, -1.9896e-01, -1.8790e+00, -9.1025e-01, -1.2268e+00,\n",
       "        -4.9423e-01, -6.7755e-02, -5.7858e-01, -5.4315e-01, -2.9178e+00,\n",
       "        -3.7566e+00, -2.5723e+00, -1.5466e+00, -1.3510e+00, -7.3740e-01,\n",
       "        -1.9426e-01,  3.9274e-01,  1.2333e-01, -3.8853e-01, -8.5306e-01,\n",
       "         7.4171e-02,  4.5697e-01, -3.2739e-01, -2.8805e-01, -2.2042e+00,\n",
       "        -4.0190e+00, -3.4379e-01, -8.7806e-02, -2.5532e+00,  3.1618e-01,\n",
       "         6.5170e-01, -1.0394e+00,  5.0058e-01, -4.6297e-01, -1.9093e+00,\n",
       "         7.8203e-01, -2.0534e+00, -1.4265e+00, -2.1781e+00, -1.3439e+00,\n",
       "        -1.9399e+00, -2.6506e+00, -7.1419e-01, -1.5612e+00, -2.9797e+00,\n",
       "        -2.0295e+00, -1.4312e+00, -5.6637e-01, -1.5874e+00, -7.9946e-01,\n",
       "        -3.6092e+00, -7.7117e-01, -3.9399e+00, -1.1828e+00, -1.6165e+00,\n",
       "        -1.2906e+00, -1.2515e+00, -3.3629e+00, -1.9938e+00, -2.7308e+00,\n",
       "        -1.9073e+00, -1.2921e+00, -1.4498e+00, -1.0907e+00, -2.4394e+00,\n",
       "        -5.4170e-01, -2.1831e+00, -6.1007e-01, -1.4152e+00, -2.6084e+00,\n",
       "        -1.7505e+00, -1.3081e+00, -1.7612e+00, -1.0941e+00, -2.0629e+00,\n",
       "        -8.5384e-01, -1.7661e+00, -1.9475e+00, -2.9224e+00, -1.7038e+00,\n",
       "        -1.4701e+00, -2.5262e+00, -1.2744e+00, -4.4667e-01, -1.3217e+00,\n",
       "        -3.1183e+00, -1.1188e+00, -1.4234e+00, -1.9425e+00, -1.8161e+00,\n",
       "        -4.0489e+00, -2.0867e+00, -2.4542e+00, -1.2808e+00, -1.2399e+00,\n",
       "        -1.9826e+00, -2.3199e-01, -2.3471e+00, -2.5313e+00, -1.6153e+00,\n",
       "        -1.0908e+00, -8.2953e-01, -1.3417e+00, -1.7246e+00, -2.0912e+00,\n",
       "        -2.7096e+00, -3.0469e+00, -3.6121e+00, -1.5514e+00, -5.9606e-03,\n",
       "        -2.8614e+00, -1.2573e+00, -4.3402e+00, -3.9453e+00, -1.7405e+00,\n",
       "        -3.9487e+00, -2.7703e+00, -2.3417e+00, -2.5577e+00, -1.1830e+00,\n",
       "        -2.6377e+00, -2.4248e+00, -2.6259e+00,  3.6496e-01, -2.3771e+00,\n",
       "        -2.0801e+00, -1.8870e+00, -2.5537e+00, -2.2866e+00, -4.1079e+00,\n",
       "        -3.9963e+00, -3.4932e+00, -1.7439e+00, -2.6052e+00, -4.0375e+00,\n",
       "        -2.9681e+00, -1.3747e+00, -1.8802e+00, -3.7175e+00, -3.3081e+00,\n",
       "        -9.2179e-01, -1.3275e+00, -1.1257e+00, -2.7357e+00, -2.3828e+00,\n",
       "        -9.3175e-01, -1.9274e+00, -1.5867e+00, -1.1147e+00, -7.6200e-01,\n",
       "        -2.1962e+00, -1.8848e+00, -2.3732e+00, -3.4889e+00, -4.1384e+00,\n",
       "        -2.2290e+00, -1.7332e+00, -2.1837e+00, -2.6069e+00, -1.2044e+00,\n",
       "        -8.2778e-01, -2.6367e+00, -8.4770e-01, -1.6148e+00, -1.6831e+00,\n",
       "        -4.5943e+00, -1.2034e+00, -2.4183e+00, -2.3753e+00, -3.7105e+00,\n",
       "        -2.4834e+00, -2.2856e+00, -2.6213e+00, -1.4602e+00, -1.6130e+00,\n",
       "        -1.9254e+00, -2.1063e+00, -4.3608e+00, -1.6700e+00, -1.2499e+00,\n",
       "        -2.5361e+00, -4.3832e+00, -3.0208e+00, -3.1845e+00, -4.4427e+00,\n",
       "        -2.7613e+00, -4.1703e+00, -2.3649e+00, -2.5874e+00, -2.0153e+00,\n",
       "        -1.7273e+00, -2.3063e+00, -1.3617e+00, -3.0173e+00, -2.1452e+00,\n",
       "        -1.6906e+00, -1.9868e+00, -1.2620e+00, -2.3252e+00, -1.7039e+00,\n",
       "        -2.4212e+00, -2.8511e+00, -3.5061e+00, -1.3828e+00, -1.9664e+00,\n",
       "        -2.0361e+00, -1.8747e+00, -2.5511e+00, -3.7154e+00, -1.8020e+00,\n",
       "        -4.6550e+00, -1.7998e+00, -2.3642e+00, -3.7618e+00, -1.8935e+00,\n",
       "        -2.2817e+00, -2.2474e+00, -1.6058e+00, -1.5675e+00, -2.0471e+00,\n",
       "        -2.2614e+00, -2.8964e+00, -4.5400e+00, -1.0528e+00, -3.2491e+00,\n",
       "        -2.6021e+00, -2.2602e+00, -1.9737e+00, -2.8077e+00, -2.0911e+00,\n",
       "        -4.0410e+00, -3.9345e+00, -2.3128e+00, -3.4482e+00, -3.6833e+00,\n",
       "        -3.3486e+00, -2.9493e+00, -1.8903e+00, -2.1389e+00, -3.8594e+00,\n",
       "        -2.4647e+00, -3.0747e+00, -1.2360e+00, -1.7662e+00, -2.7395e+00,\n",
       "        -1.8549e+00, -1.6536e+00, -3.7132e+00, -1.7886e+00, -4.5437e+00,\n",
       "        -1.6937e+00, -2.9825e+00, -4.5432e+00, -4.1007e+00, -1.6267e+00,\n",
       "        -2.6280e+00, -1.0781e+00, -2.0220e+00, -1.9750e+00, -1.1990e+00,\n",
       "        -4.7474e+00, -3.2102e+00, -2.8281e+00, -1.6788e+00, -2.5252e+00,\n",
       "        -3.1834e+00, -2.9289e+00, -3.5403e+00, -2.9582e+00, -2.7041e+00,\n",
       "        -2.3035e+00, -3.9000e+00, -2.3645e+00, -2.4400e+00, -1.9191e+00,\n",
       "        -2.1079e+00, -1.9131e+00, -3.0416e+00, -2.3296e+00, -4.1500e+00,\n",
       "        -3.3054e+00, -3.1896e+00, -4.3133e-01, -3.8614e+00, -2.3017e+00,\n",
       "        -4.8613e+00, -3.9741e+00, -3.8339e+00, -4.5165e+00, -2.3310e+00,\n",
       "        -2.2540e+00, -2.7221e+00, -3.2100e+00, -1.6091e+00, -4.1768e+00,\n",
       "        -3.0610e+00, -2.0844e+00, -2.5142e+00, -2.8306e+00, -4.5360e+00,\n",
       "        -1.5588e+00, -2.0280e+00, -3.2742e+00, -2.5652e+00, -2.5459e+00,\n",
       "        -2.1031e+00, -2.6466e+00, -1.9467e+00, -3.0616e+00, -1.7983e+00,\n",
       "        -3.6613e+00, -2.2137e+00, -2.1014e+00, -3.0301e+00, -2.2945e+00,\n",
       "        -3.6151e+00, -2.7995e+00, -3.1679e+00, -2.2704e+00, -2.8943e+00,\n",
       "        -2.0538e+00, -4.5383e+00, -2.0479e+00, -1.9903e+00, -3.9666e+00,\n",
       "        -3.4430e+00, -3.8404e+00, -1.8635e+00, -3.3508e+00, -4.6083e+00,\n",
       "        -2.3946e+00, -2.7838e+00, -2.4690e+00, -3.3001e+00, -2.0036e+00,\n",
       "        -2.5141e+00, -4.3397e+00, -4.9336e+00, -3.1887e+00, -2.9167e+00,\n",
       "        -1.6836e+00, -3.3450e+00, -2.7772e+00, -3.3577e+00, -2.0345e+00,\n",
       "        -4.1052e+00, -2.9950e+00, -2.8716e+00, -3.1962e+00, -3.9509e+00,\n",
       "        -3.0432e+00, -4.5293e+00, -4.5469e+00, -2.1368e+00, -2.1847e+00,\n",
       "        -1.8866e+00, -3.7102e+00, -2.1992e+00, -3.6429e+00, -2.5369e+00,\n",
       "        -4.7829e+00, -3.4652e+00, -4.1615e+00, -3.9211e+00, -3.5090e+00,\n",
       "        -3.6083e+00, -3.1238e+00, -3.6082e+00, -4.5608e+00, -3.5816e+00,\n",
       "        -1.9059e+00, -2.1987e+00, -2.5102e+00, -2.1509e+00, -3.0419e+00,\n",
       "        -3.8750e+00, -3.6326e+00, -3.4164e+00, -2.0998e+00, -2.5855e+00,\n",
       "        -3.0427e+00, -4.4575e+00, -4.0995e+00, -2.9442e+00, -4.2341e+00,\n",
       "        -3.9964e+00, -3.8931e+00, -3.4028e+00, -3.6887e+00, -2.6763e+00,\n",
       "        -2.3494e+00, -2.1675e+00, -4.9130e+00, -4.4052e+00, -3.1095e+00,\n",
       "        -2.9470e+00, -2.7092e+00, -3.5804e+00, -4.6113e+00, -2.4906e+00,\n",
       "        -3.3263e+00, -3.0976e+00, -2.6910e+00, -2.5950e+00, -1.9130e+00,\n",
       "        -2.5434e+00, -3.8552e+00, -3.6204e+00, -3.8119e+00, -2.1912e+00,\n",
       "        -2.7422e+00, -2.6224e+00, -2.6419e+00, -1.6286e+00, -3.0092e+00,\n",
       "        -5.1869e+00, -3.9929e+00, -2.9831e+00, -3.0060e+00, -3.0391e+00,\n",
       "        -2.5550e+00, -3.5607e+00, -4.6013e+00, -1.8766e+00, -3.3064e+00,\n",
       "        -2.8879e+00, -4.2628e+00, -2.1335e+00, -1.9767e+00, -2.3016e+00,\n",
       "        -3.6773e+00, -2.7352e+00, -4.1618e+00, -4.2181e+00, -3.4700e+00,\n",
       "        -4.1694e+00, -4.6110e+00, -2.8701e+00, -3.1055e+00, -3.2489e+00,\n",
       "        -2.8509e+00, -3.6041e+00, -3.5940e+00, -3.9794e+00, -4.6304e+00,\n",
       "        -3.5159e+00, -3.0029e+00, -4.5128e+00, -4.6606e+00, -3.5947e+00,\n",
       "        -4.5871e+00, -4.5473e+00, -3.1719e+00, -2.2715e+00, -3.1729e+00,\n",
       "        -4.0133e+00, -2.4119e+00, -3.9412e+00, -2.6230e+00, -5.0294e+00,\n",
       "        -2.0307e+00, -2.2761e+00, -2.7396e+00, -2.6841e+00, -3.7883e+00,\n",
       "        -3.4622e+00, -4.6920e+00, -4.8420e+00, -4.1335e-01, -3.4119e+00,\n",
       "        -3.1464e+00, -4.0306e+00, -4.7648e+00, -2.8270e+00, -4.1204e+00,\n",
       "        -4.1506e+00, -2.2267e+00, -3.7764e+00, -4.5837e+00, -2.7150e+00,\n",
       "        -2.5725e+00, -3.1202e+00, -1.9317e+00, -2.5411e+00, -3.1199e+00,\n",
       "        -3.1412e+00, -4.3624e+00, -3.0445e+00, -3.7331e+00, -3.8825e+00,\n",
       "        -3.6094e+00, -3.4235e+00, -2.8566e+00, -3.0366e+00, -4.2405e+00,\n",
       "        -3.6809e+00, -3.4225e+00, -1.9355e+00, -4.0616e+00, -3.8930e+00,\n",
       "        -3.5757e+00, -2.5918e+00, -2.6241e+00, -4.2135e+00, -3.6030e+00,\n",
       "        -4.2619e+00, -3.6649e+00, -2.4681e+00, -3.7224e+00, -4.5054e+00,\n",
       "        -2.4313e+00, -2.9899e+00, -3.5174e+00, -2.7677e+00, -3.8334e+00,\n",
       "        -3.7711e+00, -4.2779e+00, -2.6658e+00, -2.5213e+00, -2.7998e+00,\n",
       "        -2.5516e+00, -3.1868e+00, -3.6262e+00, -4.0500e+00, -3.4187e+00,\n",
       "        -3.3731e+00, -1.8804e+00, -4.5441e+00, -2.4697e+00, -2.9717e+00,\n",
       "        -3.5118e+00, -3.0892e+00, -4.5394e+00, -3.5155e+00, -2.7962e+00,\n",
       "        -2.9037e+00, -4.4065e+00, -4.2273e+00, -3.2356e+00, -3.6919e+00,\n",
       "        -2.5755e+00, -2.5562e+00, -4.3311e+00, -3.4382e+00, -2.3069e+00,\n",
       "        -2.6129e+00, -3.9947e+00, -3.4868e+00, -4.5265e+00, -4.6033e+00,\n",
       "        -3.1843e+00, -4.8873e+00, -4.6450e+00, -3.4928e+00, -3.7801e+00,\n",
       "        -3.8480e+00, -4.7150e+00, -3.9130e+00, -3.2871e+00, -4.7148e+00,\n",
       "        -4.7698e+00, -4.7023e+00, -2.5782e+00, -4.4779e+00, -3.1360e+00,\n",
       "        -4.2380e+00, -3.1940e+00, -2.4523e+00, -3.7252e+00, -2.8732e+00,\n",
       "        -3.1500e+00, -4.5918e+00, -4.0865e+00, -4.7496e+00, -3.4463e+00,\n",
       "        -4.0994e+00, -3.0482e+00, -3.1675e+00, -2.3563e+00, -1.6128e+00,\n",
       "        -3.0279e+00, -3.4138e+00, -4.2751e+00, -4.2508e+00, -4.1215e+00,\n",
       "        -3.0036e+00, -4.5261e+00, -3.9788e+00, -2.2237e+00, -4.6167e+00,\n",
       "        -4.0559e+00, -4.2746e+00, -4.3217e+00, -3.9878e+00, -4.6136e+00,\n",
       "        -4.3877e+00, -4.4322e+00, -4.5973e+00, -4.6240e+00, -4.9145e+00,\n",
       "        -4.5074e+00, -2.3799e+00, -3.8066e+00, -4.0739e+00, -4.3559e+00,\n",
       "        -2.7800e+00, -4.6403e+00, -2.8581e+00, -4.6443e+00, -4.6963e+00,\n",
       "        -2.4241e+00, -4.9349e+00, -2.9784e+00, -4.4874e+00, -2.8595e+00,\n",
       "        -3.4782e+00, -2.9508e+00, -1.7011e+00, -3.5671e+00, -4.0069e+00,\n",
       "        -3.4072e+00, -2.8525e+00, -4.0406e+00, -3.5668e+00, -2.5182e+00,\n",
       "        -3.4022e+00, -4.7975e+00, -3.9713e+00, -3.1810e+00, -3.6592e+00,\n",
       "        -4.5556e+00, -3.0373e+00, -3.0051e+00, -3.6484e+00, -2.7423e+00,\n",
       "        -3.6332e+00, -3.9374e+00], device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.lm_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11831, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.word_embeddings.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11831, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(diffusion_model.word_embeddings.weight.data,dim=1,keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 72, 11831])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 72, 128])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    normed_emb = diffusion_model.word_embeddings.weight.data/torch.norm(diffusion_model.word_embeddings.weight.data,dim=1,keepdim=True)\n",
    "    sim = torch.matmul(hidden_states[-1], normed_emb.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 72, 11831])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True], device='cuda:1')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(sim[0],dim=-1)==torch.argmax(logits[0],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 72, 128])\n",
      "torch.Size([128, 11831])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states[-1].shape)\n",
    "print(diffusion_model.word_embeddings.weight.data.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dist = torch.dist(hidden_states[-1].unsqueeze(-1), diffusion_model.word_embeddings.weight.data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    l2_dist = torch.sum((hidden_states[-1].cpu().unsqueeze(-1)-diffusion_model.word_embeddings.weight.data.T.cpu())**2,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 72, 11831])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(l2_dist[0],dim=-1)==torch.argmax(logits[0].cpu(),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in eval_dataloader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        loss_terms = diffusion_model(input_ids, attention_mask)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse tensor(0.0490, device='cuda:1')\n",
      "L_T tensor(6.3241e-08, device='cuda:1')\n",
      "rounding tensor(0.0051, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "for key, val in loss_terms.items():\n",
    "    print(key,loss_terms[key].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learned_embbeding = diffusion_model.word_embeddings.weight.data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learned_rounding = diffusion_model.lm_head.weight.data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_input_ids = torch.LongTensor(range(config.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([852, 852])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    emb = diffusion_model.word_embeddings(test_input_ids.to(device))\n",
    "    rounded = diffusion_model.lm_head(emb)\n",
    "    print(rounded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(852)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.argmax(rounded,dim=1).cpu()==test_input_ids).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
