{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from transformers import BertConfig, BertTokenizerFast\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import BertForDiffusion, DiffusionLM, ConditionalDiffusionLM\n",
    "from data_utils import load_qqp_dataset_and_tokenizer_from_disk, QQPParaphraseDataset, load_split_qqp_dataset_and_tokenizer_from_disk\n",
    "from noise_schedule import get_named_beta_schedule\n",
    "from train_utils import train_conditional, evaluate_conditional\n",
    "from metric_utils import calculate_bleu, calculate_rouge\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataset args\n",
    "max_len = 32\n",
    "\n",
    "# training args\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda:1\")\n",
    "lr = 1e-4\n",
    "num_epoch = 30\n",
    "weight_decay = 0\n",
    "num_warmup_steps = 100\n",
    "\n",
    "# model args\n",
    "word_embedding_dim = 128\n",
    "# hidden_size = 768\n",
    "# num_hidden_layers = 12\n",
    "# num_attention_heads = 12\n",
    "# intermediate_size = 3072\n",
    "hidden_size = 512\n",
    "num_hidden_layers = 4\n",
    "num_attention_heads = 8\n",
    "intermediate_size = 2048\n",
    "max_position_embeddings = max_len\n",
    "\n",
    "encoder_type = 'from-scratch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 15672\n",
      "Training set size: 120940\n",
      "Evaluation set size: 13438\n"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset, tokenizer = load_split_qqp_dataset_and_tokenizer_from_disk(data_path=\"data\")\n",
    "\n",
    "# tokenized_qqp_train, tokenized_qqp_eval, tokenizer = load_qqp_dataset_and_tokenizer_from_disk(data_path=\"data\")\n",
    "\n",
    "rev_tokenizer = {v: k for k, v in tokenizer.items()}\n",
    "\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
    "\n",
    "# train_dataset = QQPParaphraseDataset(dataset=tokenized_qqp_train, random_swap=True)\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "# eval_dataset = QQPParaphraseDataset(dataset=tokenized_qqp_eval, random_swap=False)\n",
    "print(\"Evaluation set size:\", len(eval_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"T\": 2000,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 32,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 15672\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=len(tokenizer), hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=max_position_embeddings, pad_token_id=tokenizer['[PAD]'])\n",
    "\n",
    "config.T = 2000\n",
    "# comment next line if using bit word embedding\n",
    "#config.word_embedding_dim = word_embedding_dim\n",
    "\n",
    "print(config)\n",
    "\n",
    "betas = torch.Tensor(get_named_beta_schedule(schedule_name=\"cosine\", num_diffusion_timesteps=config.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using bit word embedding\n",
      "set word_embedding_dim to: 14\n",
      "Diffusion model #parameters:\n",
      "38886414\n",
      "Diffusion model #trainable parameters\n",
      "38886414\n"
     ]
    }
   ],
   "source": [
    "diffusion_model = ConditionalDiffusionLM(config=config, betas=betas, use_shared_weight=True, lm_head_bias=False, add_emb_noise=False, conditional_gen=True, encoder_type=encoder_type, encoder_name_or_path='bert-base-uncased', emb_type='bit').to(device)\n",
    "\n",
    "print(\"Diffusion model #parameters:\")\n",
    "print(sum([p.numel() for p in diffusion_model.parameters()]))\n",
    "\n",
    "print(\"Diffusion model #trainable parameters\")\n",
    "print(sum([p.numel() for p in filter(lambda p:p.requires_grad, diffusion_model.parameters())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p:p.requires_grad, diffusion_model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_epoch*len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train loop\n",
    "loss_terms_dict_lst = []\n",
    "progress_bar = tqdm(range(num_epoch*len(train_dataloader)))\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    print(\"epoch:\",epoch+1)\n",
    "    loss_terms_dict_lst.append(train_conditional(diffusion_model=diffusion_model, dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler ,progress_bar=progress_bar ,verbose=True))\n",
    "    evaluate_conditional(diffusion_model=diffusion_model, dataloader=eval_dataloader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(diffusion_model.state_dict(), \"checkpoints/conditional_from_scratch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['alphas_bar', 'alphas_bar_prev'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diffusion_model.load_state_dict(torch.load(\"checkpoints/20221015_2026\"))\n",
    "diffusion_model.load_state_dict(torch.load(\"checkpoints/20220906_0519\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774ffe570ac14eb89694d8fcef592931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.07535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0753, device='cuda:1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_conditional(diffusion_model=diffusion_model, dataloader=eval_dataloader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc7c1dd919b41779373bd844ea7578f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "generated_questions_mbr5_ddim20 = diffusion_model.generate(\n",
    "    dataset = eval_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=20,\n",
    "    eta=0,\n",
    "    mbr=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15155854506376149\n",
      "0.22320761914790413\n"
     ]
    }
   ],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr5_ddim20, eval_dataset, rev_tokenizer)\n",
    "print(sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"]))\n",
    "print(sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5461694549349757\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr5_ddim20, eval_dataset, rev_tokenizer)\n",
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]\n",
    "print(sum(rouge_l_f)/len(rouge_l_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some ways to help a teenager overcome depression ?\n",
      "How does anyone overcome depression ?\n",
      "How do I overcome depression ?\n"
     ]
    }
   ],
   "source": [
    "i = 51\n",
    "src_question = [rev_tokenizer[id.item()] for id in eval_dataset[i]['question1_input_ids']]\n",
    "src_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src_question))\n",
    "print(\" \".join(src_question))\n",
    "\n",
    "tgt_question = [rev_tokenizer[id.item()] for id in eval_dataset[i]['question2_input_ids']]\n",
    "tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "print(\" \".join(tgt_question))\n",
    "\n",
    "#print(\" \".join(generated_questions_mbr1_ddim200[i]))\n",
    "print(\" \".join(generated_questions_mbr15_ddim2[i]))\n",
    "#print(\" \".join(generated_questions_mbr5_ddim20[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7342301b021940aa883544b8a31e64eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "generated_questions_mbr5 = diffusion_model.generate(\n",
    "    dataset = eval_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=200,\n",
    "    eta=0,\n",
    "    mbr=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18039911311842954\n",
      "0.2751321938279151\n"
     ]
    }
   ],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr5, eval_dataset, rev_tokenizer)\n",
    "print(sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"]))\n",
    "print(sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5874787212343898\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr5, eval_dataset, rev_tokenizer)\n",
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]\n",
    "print(sum(rouge_l_f)/len(rouge_l_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_batch = next(iter(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "generated_questions = []\n",
    "bs = eval_batch['question1_input_ids'].shape[0]\n",
    "mbr = 5\n",
    "if mbr > 1:     # using MBR decoding\n",
    "    batch_questions = []\n",
    "    for cnt in range(mbr):\n",
    "        x_T = torch.randn(size=(bs,\n",
    "                                diffusion_model.config.max_position_embeddings,\n",
    "                                diffusion_model.config.word_embedding_dim))\n",
    "        final_hidden_state = diffusion_model.ddim_sample(x_T.to(device),\n",
    "                                              sampling_timesteps=200,\n",
    "                                              eta=0,\n",
    "                                              src_ids=eval_batch['question1_input_ids'].to(device),\n",
    "                                              src_attention_mask=eval_batch['question1_attention_mask'].to(device),\n",
    "                                              return_hidden_states=False,\n",
    "                                              verbose=False\n",
    "                                              )\n",
    "        sampled_ids = diffusion_model.rounding(final_hidden_state).cpu()\n",
    "        questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "        batch_questions.append([list(filter(lambda x: x not in ['[PAD]', '[START]', '[END]'], question)) for question in questions])\n",
    "    # batch_questions [mbr, bs, question]\n",
    "    for batch_ind in range(bs):\n",
    "        candidates = [one_generation[batch_ind] for one_generation in batch_questions]      # [mbr, question]\n",
    "        bleu_scores = torch.zeros(mbr)\n",
    "        for candidate_ind, candidate in enumerate(candidates):\n",
    "            for ref_ind, ref in enumerate(candidates):\n",
    "                if ref_ind != candidate_ind:\n",
    "                    bleu_scores[candidate_ind] += sentence_bleu([ref], candidate)\n",
    "        select_ind = torch.argmax(bleu_scores).item()\n",
    "        generated_questions.append(batch_questions[select_ind][batch_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_T = torch.randn(size=(batch_size, max_len, diffusion_model.config.word_embedding_dim))\n",
    "    final_hidden_state, hidden_states = diffusion_model.sample(x_T.to(device),\n",
    "                                        src_ids=eval_batch['question1_input_ids'].to(device),\n",
    "                                        src_attention_mask=eval_batch['question1_attention_mask'].to(device),\n",
    "                                        return_hidden_states=True,\n",
    "                                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_T = torch.randn(size=(batch_size, max_len, diffusion_model.config.word_embedding_dim))\n",
    "    final_hidden_state_ddim, hidden_states_ddim = diffusion_model.ddim_sample(x_T.to(device), sampling_timesteps=200, src_ids=eval_batch['question1_input_ids'].to(device),src_attention_mask=eval_batch['question1_attention_mask'].to(device), return_hidden_states=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# src_questions = bert_tokenizer.batch_decode(eval_batch['question1_input_ids_bert'], skip_special_tokens=True)\n",
    "src_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in eval_batch['question1_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(src_questions[sample_idx])\n",
    "    print(\" \".join(src_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# target_questions = bert_tokenizer.batch_decode(eval_batch['question2_input_ids_bert'], skip_special_tokens=True)\n",
    "target_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in eval_batch['question2_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(target_questions[sample_idx])\n",
    "    print(\" \".join(target_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sampled_ids = diffusion_model.rounding(hidden_states[-1])\n",
    "    generated_questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"sample_idx:\", sample_idx)\n",
    "    print(\" \".join(generated_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sampled_ids = diffusion_model.rounding(hidden_states_ddim[-1])\n",
    "    generated_questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"sample_idx:\", sample_idx)\n",
    "    print(\" \".join(generated_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_states_ddim[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_idx = 63\n",
    "for step in [1000,1900,1940,1980,1990,1993,1994,1995,1996,1997,1998,-1]:\n",
    "    hidden_state = hidden_states[step][sample_idx]\n",
    "    with torch.no_grad():\n",
    "        sampled_ids = diffusion_model.rounding(hidden_state)\n",
    "        sampled_seq = [rev_tokenizer[token_id.item()] for token_id in sampled_ids]\n",
    "        print(\"step:\", step)\n",
    "        print(\" \".join(sampled_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_idx = 63\n",
    "for step in [0,150,180,190,195,197,198,-1]:\n",
    "    hidden_state = hidden_states_ddim[step][sample_idx]\n",
    "    with torch.no_grad():\n",
    "        sampled_ids = diffusion_model.rounding(hidden_state)\n",
    "        sampled_seq = [rev_tokenizer[token_id.item()] for token_id in sampled_ids]\n",
    "        print(\"step:\", step)\n",
    "        print(\" \".join(sampled_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_hidden_state2, hidden_states2 = diffusion_model.sample(x_T.to(device),\n",
    "                                                           src_ids=train_batch['question1_input_ids'].to(device),\n",
    "                                                           src_attention_mask=train_batch['question1_attention_mask'].to(device),\n",
    "                                                           return_hidden_states=True,\n",
    "                                                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# src_questions = bert_tokenizer.batch_decode(eval_batch['question1_input_ids_bert'], skip_special_tokens=True)\n",
    "src_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in train_batch['question1_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(src_questions[sample_idx])\n",
    "    print(\" \".join(src_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# target_questions = bert_tokenizer.batch_decode(eval_batch['question2_input_ids_bert'], skip_special_tokens=True)\n",
    "target_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in train_batch['question2_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(target_questions[sample_idx])\n",
    "    print(\" \".join(target_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sampled_ids = diffusion_model.rounding(hidden_states2[-1])\n",
    "    generated_questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"sample_idx:\", sample_idx)\n",
    "    print(\" \".join(generated_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(eval_batch['question1_input_ids_bert'][0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer.convert_ids_to_tokens(eval_batch['question1_input_ids_bert'][59], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], target_questions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], generated_questions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bleu_score = []\n",
    "for target, generate in zip(target_questions, generated_questions):\n",
    "    bleu_score.append(sentence_bleu([list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], target))],\n",
    "    list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], generate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum(bleu_score)/len(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bleu_score = []\n",
    "for src, generate in zip(src_questions, generated_questions):\n",
    "    bleu_score.append(sentence_bleu([list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src))],\n",
    "                                    list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], generate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum(bleu_score)/len(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_dataset, rest_dataset = torch.utils.data.random_split(eval_dataset, [500, 12938])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generated_questions_mbr1 = diffusion_model.generate(\n",
    "    dataset = small_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=200,\n",
    "    eta=0,\n",
    "    mbr=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generated_questions_mbr1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_dataset[0]['question1_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[START]',\n",
       " 'Which',\n",
       " 'fruits',\n",
       " 'or',\n",
       " 'vegetables',\n",
       " 'should',\n",
       " 'be',\n",
       " 'eaten',\n",
       " 'regularly',\n",
       " 'to',\n",
       " 'get',\n",
       " 'vitamins',\n",
       " '?',\n",
       " '[END]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rev_tokenizer[i.item()] for i in small_dataset[0]['question2_input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bleu(generated_questions, dataset, rev_tokenizer):\n",
    "    \"\"\"\n",
    "    calculate BLEU metric\n",
    "    :param generated_questions: list[token_list]\n",
    "    :param dataset: pytorch dataset\n",
    "    :param rev_tokenizer: token_id to token dict\n",
    "    :return: {\"bleu\": val_list, \"self_bleu\": val_list}\n",
    "    \"\"\"\n",
    "    bleu, self_bleu = [],[]\n",
    "    for ind, sample in enumerate(dataset):\n",
    "        src_question = [rev_tokenizer[id.item()] for id in sample['question1_input_ids']]\n",
    "        src_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src_question))\n",
    "        tgt_question = [rev_tokenizer[id.item()] for id in sample['question2_input_ids']]\n",
    "        tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "        bleu.append(sentence_bleu([tgt_question], generated_questions[ind]))\n",
    "        self_bleu.append(sentence_bleu([src_question], generated_questions[ind]))\n",
    "\n",
    "    return {\"bleu\": bleu, \"self_bleu\": self_bleu}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr1, small_dataset, rev_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15005539419638644"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2198190329545948"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which is the better fruits to get eaten at critics ?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(generated_questions_mbr1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_rouge(generated_questions, dataset, rev_tokenizer):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    for ind, sample in enumerate(dataset):\n",
    "        tgt_question = [rev_tokenizer[id.item()] for id in sample['question2_input_ids']]\n",
    "        tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "        rouge_scores += rouge.get_scores(\" \".join(generated_questions[ind]), \" \".join(tgt_question))\n",
    "\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr1, small_dataset, rev_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.5, 'p': 0.5454545454545454, 'f': 0.5217391254442345},\n",
       " 'rouge-2': {'r': 0.09090909090909091, 'p': 0.1, 'f': 0.09523809024943337},\n",
       " 'rouge-l': {'r': 0.4166666666666667,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.434782603705104}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
