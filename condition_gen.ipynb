{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from transformers import BertConfig, BertTokenizerFast\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import BertForDiffusion, DiffusionLM, ConditionalDiffusionLM\n",
    "from data_utils import load_qqp_dataset_and_tokenizer_from_disk, QQPParaphraseDataset, load_split_qqp_dataset_and_tokenizer_from_disk\n",
    "from noise_schedule import get_named_beta_schedule\n",
    "from train_utils import train_conditional, evaluate_conditional\n",
    "from metric_utils import calculate_bleu, calculate_rouge\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataset args\n",
    "max_len = 32\n",
    "\n",
    "# training args\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda:1\")\n",
    "lr = 1e-4\n",
    "num_epoch = 30\n",
    "weight_decay = 0\n",
    "num_warmup_steps = 100\n",
    "\n",
    "# model args\n",
    "word_embedding_dim = 128\n",
    "# hidden_size = 768\n",
    "# num_hidden_layers = 12\n",
    "# num_attention_heads = 12\n",
    "# intermediate_size = 3072\n",
    "hidden_size = 512\n",
    "num_hidden_layers = 4\n",
    "num_attention_heads = 8\n",
    "intermediate_size = 2048\n",
    "max_position_embeddings = max_len\n",
    "\n",
    "encoder_type = 'from-scratch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 15672\n",
      "Training set size: 120940\n",
      "Evaluation set size: 13438\n"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset, tokenizer = load_split_qqp_dataset_and_tokenizer_from_disk(data_path=\"data\")\n",
    "\n",
    "# tokenized_qqp_train, tokenized_qqp_eval, tokenizer = load_qqp_dataset_and_tokenizer_from_disk(data_path=\"data\")\n",
    "\n",
    "rev_tokenizer = {v: k for k, v in tokenizer.items()}\n",
    "\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
    "\n",
    "# train_dataset = QQPParaphraseDataset(dataset=tokenized_qqp_train, random_swap=True)\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "# eval_dataset = QQPParaphraseDataset(dataset=tokenized_qqp_eval, random_swap=False)\n",
    "print(\"Evaluation set size:\", len(eval_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"T\": 2000,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 32,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 15672\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=len(tokenizer), hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=max_position_embeddings, pad_token_id=tokenizer['[PAD]'])\n",
    "\n",
    "config.T = 2000\n",
    "# comment next line if using bit word embedding\n",
    "#config.word_embedding_dim = word_embedding_dim\n",
    "\n",
    "print(config)\n",
    "\n",
    "betas = torch.Tensor(get_named_beta_schedule(schedule_name=\"cosine\", num_diffusion_timesteps=config.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using bit word embedding\n",
      "set word_embedding_dim to: 14\n",
      "Diffusion model #parameters:\n",
      "38886414\n",
      "Diffusion model #trainable parameters\n",
      "38886414\n"
     ]
    }
   ],
   "source": [
    "diffusion_model = ConditionalDiffusionLM(config=config, betas=betas, use_shared_weight=True, lm_head_bias=False, add_emb_noise=False, conditional_gen=True, encoder_type=encoder_type, encoder_name_or_path='bert-base-uncased', emb_type='bit').to(device)\n",
    "\n",
    "print(\"Diffusion model #parameters:\")\n",
    "print(sum([p.numel() for p in diffusion_model.parameters()]))\n",
    "\n",
    "print(\"Diffusion model #trainable parameters\")\n",
    "print(sum([p.numel() for p in filter(lambda p:p.requires_grad, diffusion_model.parameters())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p:p.requires_grad, diffusion_model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_epoch*len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train loop\n",
    "loss_terms_dict_lst = []\n",
    "progress_bar = tqdm(range(num_epoch*len(train_dataloader)))\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    print(\"epoch:\",epoch+1)\n",
    "    loss_terms_dict_lst.append(train_conditional(diffusion_model=diffusion_model, dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler ,progress_bar=progress_bar ,verbose=True))\n",
    "    evaluate_conditional(diffusion_model=diffusion_model, dataloader=eval_dataloader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(diffusion_model.state_dict(), \"checkpoints/conditional_from_scratch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['alphas_bar', 'alphas_bar_prev'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diffusion_model.load_state_dict(torch.load(\"checkpoints/20221015_2026\"))\n",
    "diffusion_model.load_state_dict(torch.load(\"checkpoints/20220906_0519\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774ffe570ac14eb89694d8fcef592931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.07535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0753, device='cuda:1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_conditional(diffusion_model=diffusion_model, dataloader=eval_dataloader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc7c1dd919b41779373bd844ea7578f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "generated_questions_mbr5_ddim20 = diffusion_model.generate(\n",
    "    dataset = eval_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=20,\n",
    "    eta=0,\n",
    "    mbr=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15155854506376149\n",
      "0.22320761914790413\n"
     ]
    }
   ],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr5_ddim20, eval_dataset, rev_tokenizer)\n",
    "print(sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"]))\n",
    "print(sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5461694549349757\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr5_ddim20, eval_dataset, rev_tokenizer)\n",
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]\n",
    "print(sum(rouge_l_f)/len(rouge_l_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some ways to help a teenager overcome depression ?\n",
      "How does anyone overcome depression ?\n",
      "How do I overcome depression ?\n"
     ]
    }
   ],
   "source": [
    "i = 51\n",
    "src_question = [rev_tokenizer[id.item()] for id in eval_dataset[i]['question1_input_ids']]\n",
    "src_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src_question))\n",
    "print(\" \".join(src_question))\n",
    "\n",
    "tgt_question = [rev_tokenizer[id.item()] for id in eval_dataset[i]['question2_input_ids']]\n",
    "tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "print(\" \".join(tgt_question))\n",
    "\n",
    "#print(\" \".join(generated_questions_mbr1_ddim200[i]))\n",
    "print(\" \".join(generated_questions_mbr15_ddim2[i]))\n",
    "#print(\" \".join(generated_questions_mbr5_ddim20[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7342301b021940aa883544b8a31e64eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "generated_questions_mbr5 = diffusion_model.generate(\n",
    "    dataset = eval_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=200,\n",
    "    eta=0,\n",
    "    mbr=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18039911311842954\n",
      "0.2751321938279151\n"
     ]
    }
   ],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr5, eval_dataset, rev_tokenizer)\n",
    "print(sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"]))\n",
    "print(sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5874787212343898\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr5, eval_dataset, rev_tokenizer)\n",
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]\n",
    "print(sum(rouge_l_f)/len(rouge_l_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_batch = next(iter(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "generated_questions = []\n",
    "bs = eval_batch['question1_input_ids'].shape[0]\n",
    "mbr = 5\n",
    "if mbr > 1:     # using MBR decoding\n",
    "    batch_questions = []\n",
    "    for cnt in range(mbr):\n",
    "        x_T = torch.randn(size=(bs,\n",
    "                                diffusion_model.config.max_position_embeddings,\n",
    "                                diffusion_model.config.word_embedding_dim))\n",
    "        final_hidden_state = diffusion_model.ddim_sample(x_T.to(device),\n",
    "                                              sampling_timesteps=200,\n",
    "                                              eta=0,\n",
    "                                              src_ids=eval_batch['question1_input_ids'].to(device),\n",
    "                                              src_attention_mask=eval_batch['question1_attention_mask'].to(device),\n",
    "                                              return_hidden_states=False,\n",
    "                                              verbose=False\n",
    "                                              )\n",
    "        sampled_ids = diffusion_model.rounding(final_hidden_state).cpu()\n",
    "        questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "        batch_questions.append([list(filter(lambda x: x not in ['[PAD]', '[START]', '[END]'], question)) for question in questions])\n",
    "    # batch_questions [mbr, bs, question]\n",
    "    for batch_ind in range(bs):\n",
    "        candidates = [one_generation[batch_ind] for one_generation in batch_questions]      # [mbr, question]\n",
    "        bleu_scores = torch.zeros(mbr)\n",
    "        for candidate_ind, candidate in enumerate(candidates):\n",
    "            for ref_ind, ref in enumerate(candidates):\n",
    "                if ref_ind != candidate_ind:\n",
    "                    bleu_scores[candidate_ind] += sentence_bleu([ref], candidate)\n",
    "        select_ind = torch.argmax(bleu_scores).item()\n",
    "        generated_questions.append(batch_questions[select_ind][batch_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_T = torch.randn(size=(batch_size, max_len, diffusion_model.config.word_embedding_dim))\n",
    "    final_hidden_state, hidden_states = diffusion_model.sample(x_T.to(device),\n",
    "                                        src_ids=eval_batch['question1_input_ids'].to(device),\n",
    "                                        src_attention_mask=eval_batch['question1_attention_mask'].to(device),\n",
    "                                        return_hidden_states=True,\n",
    "                                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_T = torch.randn(size=(batch_size, max_len, diffusion_model.config.word_embedding_dim))\n",
    "    final_hidden_state_ddim, hidden_states_ddim = diffusion_model.ddim_sample(x_T.to(device), sampling_timesteps=200, src_ids=eval_batch['question1_input_ids'].to(device),src_attention_mask=eval_batch['question1_attention_mask'].to(device), return_hidden_states=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# src_questions = bert_tokenizer.batch_decode(eval_batch['question1_input_ids_bert'], skip_special_tokens=True)\n",
    "src_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in eval_batch['question1_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(src_questions[sample_idx])\n",
    "    print(\" \".join(src_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# target_questions = bert_tokenizer.batch_decode(eval_batch['question2_input_ids_bert'], skip_special_tokens=True)\n",
    "target_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in eval_batch['question2_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(target_questions[sample_idx])\n",
    "    print(\" \".join(target_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sampled_ids = diffusion_model.rounding(hidden_states[-1])\n",
    "    generated_questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"sample_idx:\", sample_idx)\n",
    "    print(\" \".join(generated_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sampled_ids = diffusion_model.rounding(hidden_states_ddim[-1])\n",
    "    generated_questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"sample_idx:\", sample_idx)\n",
    "    print(\" \".join(generated_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_states_ddim[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_idx = 63\n",
    "for step in [1000,1900,1940,1980,1990,1993,1994,1995,1996,1997,1998,-1]:\n",
    "    hidden_state = hidden_states[step][sample_idx]\n",
    "    with torch.no_grad():\n",
    "        sampled_ids = diffusion_model.rounding(hidden_state)\n",
    "        sampled_seq = [rev_tokenizer[token_id.item()] for token_id in sampled_ids]\n",
    "        print(\"step:\", step)\n",
    "        print(\" \".join(sampled_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_idx = 63\n",
    "for step in [0,150,180,190,195,197,198,-1]:\n",
    "    hidden_state = hidden_states_ddim[step][sample_idx]\n",
    "    with torch.no_grad():\n",
    "        sampled_ids = diffusion_model.rounding(hidden_state)\n",
    "        sampled_seq = [rev_tokenizer[token_id.item()] for token_id in sampled_ids]\n",
    "        print(\"step:\", step)\n",
    "        print(\" \".join(sampled_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_hidden_state2, hidden_states2 = diffusion_model.sample(x_T.to(device),\n",
    "                                                           src_ids=train_batch['question1_input_ids'].to(device),\n",
    "                                                           src_attention_mask=train_batch['question1_attention_mask'].to(device),\n",
    "                                                           return_hidden_states=True,\n",
    "                                                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# src_questions = bert_tokenizer.batch_decode(eval_batch['question1_input_ids_bert'], skip_special_tokens=True)\n",
    "src_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in train_batch['question1_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(src_questions[sample_idx])\n",
    "    print(\" \".join(src_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# target_questions = bert_tokenizer.batch_decode(eval_batch['question2_input_ids_bert'], skip_special_tokens=True)\n",
    "target_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in train_batch['question2_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(target_questions[sample_idx])\n",
    "    print(\" \".join(target_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sampled_ids = diffusion_model.rounding(hidden_states2[-1])\n",
    "    generated_questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"sample_idx:\", sample_idx)\n",
    "    print(\" \".join(generated_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(eval_batch['question1_input_ids_bert'][0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer.convert_ids_to_tokens(eval_batch['question1_input_ids_bert'][59], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], target_questions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], generated_questions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bleu_score = []\n",
    "for target, generate in zip(target_questions, generated_questions):\n",
    "    bleu_score.append(sentence_bleu([list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], target))],\n",
    "    list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], generate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum(bleu_score)/len(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bleu_score = []\n",
    "for src, generate in zip(src_questions, generated_questions):\n",
    "    bleu_score.append(sentence_bleu([list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src))],\n",
    "                                    list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], generate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum(bleu_score)/len(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_dataset, rest_dataset = torch.utils.data.random_split(eval_dataset, [500, 12938])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generated_questions_mbr1 = diffusion_model.generate(\n",
    "    dataset = small_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=200,\n",
    "    eta=0,\n",
    "    mbr=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generated_questions_mbr1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_dataset[0]['question1_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[START]',\n",
       " 'Which',\n",
       " 'fruits',\n",
       " 'or',\n",
       " 'vegetables',\n",
       " 'should',\n",
       " 'be',\n",
       " 'eaten',\n",
       " 'regularly',\n",
       " 'to',\n",
       " 'get',\n",
       " 'vitamins',\n",
       " '?',\n",
       " '[END]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rev_tokenizer[i.item()] for i in small_dataset[0]['question2_input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bleu(generated_questions, dataset, rev_tokenizer):\n",
    "    \"\"\"\n",
    "    calculate BLEU metric\n",
    "    :param generated_questions: list[token_list]\n",
    "    :param dataset: pytorch dataset\n",
    "    :param rev_tokenizer: token_id to token dict\n",
    "    :return: {\"bleu\": val_list, \"self_bleu\": val_list}\n",
    "    \"\"\"\n",
    "    bleu, self_bleu = [],[]\n",
    "    for ind, sample in enumerate(dataset):\n",
    "        src_question = [rev_tokenizer[id.item()] for id in sample['question1_input_ids']]\n",
    "        src_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src_question))\n",
    "        tgt_question = [rev_tokenizer[id.item()] for id in sample['question2_input_ids']]\n",
    "        tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "        bleu.append(sentence_bleu([tgt_question], generated_questions[ind]))\n",
    "        self_bleu.append(sentence_bleu([src_question], generated_questions[ind]))\n",
    "\n",
    "    return {\"bleu\": bleu, \"self_bleu\": self_bleu}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr1, small_dataset, rev_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15005539419638644"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2198190329545948"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which is the better fruits to get eaten at critics ?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(generated_questions_mbr1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_rouge(generated_questions, dataset, rev_tokenizer):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    for ind, sample in enumerate(dataset):\n",
    "        tgt_question = [rev_tokenizer[id.item()] for id in sample['question2_input_ids']]\n",
    "        tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "        rouge_scores += rouge.get_scores(\" \".join(generated_questions[ind]), \" \".join(tgt_question))\n",
    "\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr1, small_dataset, rev_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.5, 'p': 0.5454545454545454, 'f': 0.5217391254442345},\n",
       " 'rouge-2': {'r': 0.09090909090909091, 'p': 0.1, 'f': 0.09523809024943337},\n",
       " 'rouge-l': {'r': 0.4166666666666667,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.434782603705104}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.434782603705104,\n",
       " 0.6666666616666668,\n",
       " 0.42105262659279785,\n",
       " 0.5185185135253774,\n",
       " 0.6666666617283951,\n",
       " 0.5999999950500001,\n",
       " 0.42424241928374656,\n",
       " 0.33333332839506175,\n",
       " 0.35294117148788934,\n",
       " 0.8888888839506174,\n",
       " 0.8235294069204152,\n",
       " 0.4347826039319471,\n",
       " 0.7058823479584776,\n",
       " 0.7407407357475996,\n",
       " 0.47999999500800006,\n",
       " 0.4705882303114187,\n",
       " 0.6666666616666668,\n",
       " 0.24999999545000007,\n",
       " 0.4444444395061729,\n",
       " 0.9729729679766254,\n",
       " 0.6153846108579881,\n",
       " 0.4999999950347222,\n",
       " 0.38461537988165684,\n",
       " 0.6249999950195313,\n",
       " 0.47619047129251707,\n",
       " 0.34782608204158794,\n",
       " 0.7142857093877552,\n",
       " 0.5555555511111112,\n",
       " 0.8571428522448981,\n",
       " 0.5454545406198348,\n",
       " 0.6666666616666668,\n",
       " 0.4285714236734694,\n",
       " 0.5333333283555556,\n",
       " 0.2399999953920001,\n",
       " 0.518518513580247,\n",
       " 0.8965517191914388,\n",
       " 0.35897435437212366,\n",
       " 0.6315789423822715,\n",
       " 0.31578946869806096,\n",
       " 0.44444444000000005,\n",
       " 0.5555555505555557,\n",
       " 0.5714285664399092,\n",
       " 0.7999999950080001,\n",
       " 0.3448275812128419,\n",
       " 0.3529411716262976,\n",
       " 0.37499999500000003,\n",
       " 0.4999999950347222,\n",
       " 0.47999999539200017,\n",
       " 0.555555550617284,\n",
       " 0.3703703657064472,\n",
       " 0.6249999950781252,\n",
       " 0.4999999950000001,\n",
       " 0.3243243197662528,\n",
       " 0.1999999952000001,\n",
       " 0.4615384565680473,\n",
       " 0.5384615334911242,\n",
       " 0.4705882311418686,\n",
       " 0.624999995,\n",
       " 0.42105262670360116,\n",
       " 0.7999999950000002,\n",
       " 0.7777777727777778,\n",
       " 0.44444443969135805,\n",
       " 0.5384615334615386,\n",
       " 0.399999995088889,\n",
       " 0.5217391254442345,\n",
       " 0.4999999954081633,\n",
       " 0.666666661701389,\n",
       " 0.7692307642603551,\n",
       " 0.3157894688088643,\n",
       " 0.639999995392,\n",
       " 0.27272726776859507,\n",
       " 0.9090909040955004,\n",
       " 0.374999995703125,\n",
       " 0.5263157844875347,\n",
       " 0.49999999531250006,\n",
       " 0.3333333283680556,\n",
       " 0.6315789423822715,\n",
       " 0.47058823029411767,\n",
       " 0.3333333283333334,\n",
       " 0.5454545404958678,\n",
       " 0.7272727224380167,\n",
       " 0.22222221728395072,\n",
       " 0.5882352892733564,\n",
       " 0.5263157844875347,\n",
       " 0.8749999950781251,\n",
       " 0.499999995138889,\n",
       " 0.6153846104142012,\n",
       " 0.5333333283555556,\n",
       " 0.36363635880165296,\n",
       " 0.4999999950255103,\n",
       " 0.5714285665306124,\n",
       " 0.6666666618666668,\n",
       " 0.5714285665306124,\n",
       " 0.6666666617687076,\n",
       " 0.7999999950000002,\n",
       " 0.34482758126040436,\n",
       " 0.44444443944444445,\n",
       " 0.29999999545000006,\n",
       " 0.3076923027218935,\n",
       " 0.6666666617687076,\n",
       " 0.4444444397805213,\n",
       " 0.6153846104142012,\n",
       " 0.39999999520000007,\n",
       " 0.5263157844875347,\n",
       " 0.4545454497107439,\n",
       " 0.4285714235714286,\n",
       " 0.7272727223140496,\n",
       " 0.6956521689981097,\n",
       " 0.7777777728395062,\n",
       " 0.41666666222222226,\n",
       " 0.999999995,\n",
       " 0.26086956022684316,\n",
       " 0.4615384565680473,\n",
       " 0.476190471473923,\n",
       " 0.5999999952,\n",
       " 0.39024389758477096,\n",
       " 0.555555550617284,\n",
       " 0.9285714235714286,\n",
       " 0.4210526269252078,\n",
       " 0.4999999950000001,\n",
       " 0.3076923029585799,\n",
       " 0.5217391254442345,\n",
       " 0.27272726776859507,\n",
       " 0.999999995,\n",
       " 0.5999999952,\n",
       " 0.37499999500000003,\n",
       " 0.5263157845983379,\n",
       " 0.2727272680991736,\n",
       " 0.3428571379591837,\n",
       " 0.3999999950222222,\n",
       " 0.7272727223140496,\n",
       " 0.5263157848199447,\n",
       " 0.6086956471833649,\n",
       " 0.6315789423822715,\n",
       " 0.35294117148788934,\n",
       " 0.4999999950500001,\n",
       " 0.6666666616888889,\n",
       " 0.5714285664399092,\n",
       " 0.5999999950000001,\n",
       " 0.7777777727777778,\n",
       " 0.639999995072,\n",
       " 0.5263157845983379,\n",
       " 0.5999999950000001,\n",
       " 0.7567567517604091,\n",
       " 0.39999999505,\n",
       " 0.7692307642307693,\n",
       " 0.5882352891349482,\n",
       " 0.48648648149013884,\n",
       " 0.624999995,\n",
       " 0.47619047129251707,\n",
       " 0.6086956471833649,\n",
       " 0.5263157845983379,\n",
       " 0.6999999952000001,\n",
       " 0.999999995,\n",
       " 0.2666666618666667,\n",
       " 0.6315789423822715,\n",
       " 0.7058823479584776,\n",
       " 0.5454545404958678,\n",
       " 0.7692307642603551,\n",
       " 0.2666666620222223,\n",
       " 0.6999999950500001,\n",
       " 0.4444444395061729,\n",
       " 0.3809523765079365,\n",
       " 0.499999995138889,\n",
       " 0.6666666618666668,\n",
       " 0.3809523762358277,\n",
       " 0.41666666180555556,\n",
       " 0.999999995,\n",
       " 0.8205128155161079,\n",
       " 0.39999999505,\n",
       " 0.7499999950347224,\n",
       " 0.5833333283333335,\n",
       " 0.6315789427146814,\n",
       " 0.3684210476869806,\n",
       " 0.45454544954545456,\n",
       " 0.5454545406198348,\n",
       " 0.6666666616735254,\n",
       " 0.7058823479584776,\n",
       " 0.3703703655418381,\n",
       " 0.5882352892733564,\n",
       " 0.39999999520000007,\n",
       " 0.27027026544923305,\n",
       " 0.7368421002770082,\n",
       " 0.5925925875994514,\n",
       " 0.6666666617283951,\n",
       " 0.7857142807142857,\n",
       " 0.34482758192627827,\n",
       " 0.5333333285333334,\n",
       " 0.26086956022684316,\n",
       " 0.8571428521428571,\n",
       " 0.3999999950222222,\n",
       " 0.33333332847222225,\n",
       " 0.42424241935720847,\n",
       " 0.8571428521428571,\n",
       " 0.7826086906616257,\n",
       " 0.4999999952000001,\n",
       " 0.7619047570068028,\n",
       " 0.9473684160664821,\n",
       " 0.6666666616666668,\n",
       " 0.6363636313636364,\n",
       " 0.5925925875994514,\n",
       " 0.5555555505555557,\n",
       " 0.8888888839506174,\n",
       " 0.2857142807142858,\n",
       " 0.3809523762358277,\n",
       " 0.42105262670360116,\n",
       " 0.3749999953125,\n",
       " 0.5263157848199447,\n",
       " 0.6666666616666668,\n",
       " 0.2857142807256236,\n",
       " 0.6999999950500001,\n",
       " 0.43478260415879016,\n",
       " 0.874999995,\n",
       " 0.6956521689224953,\n",
       " 0.7058823479584776,\n",
       " 0.8421052581717452,\n",
       " 0.5714285664285715,\n",
       " 0.4999999950000001,\n",
       " 0.4999999950500001,\n",
       " 0.5217391254442345,\n",
       " 0.37499999507812504,\n",
       " 0.6666666618666668,\n",
       " 0.5599999953920002,\n",
       " 0.470588230449827,\n",
       " 0.3999999950222222,\n",
       " 0.7999999950222222,\n",
       " 0.23076922579881665,\n",
       " 0.9523809473922903,\n",
       " 0.5384615334911242,\n",
       " 0.555555550617284,\n",
       " 0.7777777730246914,\n",
       " 0.7058823479584776,\n",
       " 0.5333333285333334,\n",
       " 0.4999999950000001,\n",
       " 0.22222221838134434,\n",
       " 0.749999995,\n",
       " 0.47619047120181407,\n",
       " 0.6363636314049588,\n",
       " 0.2666666618666667,\n",
       " 0.3333333285802469,\n",
       " 0.2857142807256236,\n",
       " 0.11111110617283973,\n",
       " 0.999999995,\n",
       " 0.9333333283555556,\n",
       " 0.999999995,\n",
       " 0.5185185136899864,\n",
       " 0.3888888841358025,\n",
       " 0.7142857093877552,\n",
       " 0.6315789424930748,\n",
       " 0.583333328888889,\n",
       " 0.59999999545,\n",
       " 0.6666666616780046,\n",
       " 0.6153846104142012,\n",
       " 0.7058823479584776,\n",
       " 0.4285714235714286,\n",
       " 0.7272727222727273,\n",
       " 0.41666666180555556,\n",
       " 0.7999999950000002,\n",
       " 0.4444444395061729,\n",
       " 0.8571428521428571,\n",
       " 0.749999995,\n",
       " 0.6315789423822715,\n",
       " 0.5161290273048909,\n",
       " 0.4615384568047337,\n",
       " 0.35294117148788934,\n",
       " 0.999999995,\n",
       " 0.4999999950255103,\n",
       " 0.5714285664285715,\n",
       " 0.434782603705104,\n",
       " 0.4999999950781251,\n",
       " 0.4999999950000001,\n",
       " 0.3888888840277778,\n",
       " 0.34285713789387756,\n",
       " 0.999999995,\n",
       " 0.37499999500000003,\n",
       " 0.5161290273048909,\n",
       " 0.4999999950000001,\n",
       " 0.8181818131818183,\n",
       " 0.5999999952,\n",
       " 0.37499999507812504,\n",
       " 0.7619047569160999,\n",
       " 0.26086956022684316,\n",
       " 0.3809523762358277,\n",
       " 0.7999999950000002,\n",
       " 0.8571428521428571,\n",
       " 0.6666666616666668,\n",
       " 0.2857142808163266,\n",
       " 0.999999995,\n",
       " 0.43243242746530314,\n",
       " 0.5555555508024692,\n",
       " 0.6249999950781252,\n",
       " 0.3076923027218935,\n",
       " 0.59999999545,\n",
       " 0.5714285668367348,\n",
       " 0.7619047569160999,\n",
       " 0.499999995138889,\n",
       " 0.5333333285333334,\n",
       " 0.4999999950347222,\n",
       " 0.999999995,\n",
       " 0.4999999950347222,\n",
       " 0.6315789423822715,\n",
       " 0.2777777727932099,\n",
       " 0.999999995,\n",
       " 0.5714285665306124,\n",
       " 0.37037036543209884,\n",
       " 0.8235294069204152,\n",
       " 0.37499999507812504,\n",
       " 0.39999999520000007,\n",
       " 0.5454545406198348,\n",
       " 0.6666666617283951,\n",
       " 0.6666666616666668,\n",
       " 0.6315789424930748,\n",
       " 0.8333333283333335,\n",
       " 0.47619047120181407,\n",
       " 0.2758620641141499,\n",
       " 0.5925925875994514,\n",
       " 0.4444444394513032,\n",
       " 0.5882352891349482,\n",
       " 0.4615384565680473,\n",
       " 0.7999999950000002,\n",
       " 0.476190471473923,\n",
       " 0.4210526269252078,\n",
       " 0.21428570931122462,\n",
       " 0.5714285664399092,\n",
       " 0.9523809473922903,\n",
       " 0.6249999950781252,\n",
       " 0.470588230449827,\n",
       " 0.3809523759637189,\n",
       " 0.6666666616666668,\n",
       " 0.8947368371052632,\n",
       " 0.33333332839506175,\n",
       " 0.39999999520000007,\n",
       " 0.39999999520000007,\n",
       " 0.999999995,\n",
       " 0.6666666616888889,\n",
       " 0.7142857092857143,\n",
       " 0.47619047129251707,\n",
       " 0.5714285664399092,\n",
       " 0.3199999950080001,\n",
       " 0.35294117148788934,\n",
       " 0.4444444397805213,\n",
       " 0.28571428094387763,\n",
       " 0.2666666618666667,\n",
       " 0.749999995,\n",
       " 0.999999995,\n",
       " 0.4444444395061729,\n",
       " 0.47619047129251707,\n",
       " 0.6666666617283951,\n",
       " 0.42105262670360116,\n",
       " 0.5882352892733564,\n",
       " 0.5714285665306124,\n",
       " 0.3999999950222222,\n",
       " 0.36363635886134077,\n",
       " 0.35294117148788934,\n",
       " 0.999999995,\n",
       " 0.624999995,\n",
       " 0.37499999507812504,\n",
       " 0.33333332839506175,\n",
       " 0.7058823480968859,\n",
       " 0.5882352892733564,\n",
       " 0.4545454495867769,\n",
       " 0.499999995138889,\n",
       " 0.4999999950000001,\n",
       " 0.4210526269252078,\n",
       " 0.5333333283555556,\n",
       " 0.7619047569160999,\n",
       " 0.6999999950500001,\n",
       " 0.7692307642603551,\n",
       " 0.7777777728395062,\n",
       " 0.4444444395061729,\n",
       " 0.33333332839506175,\n",
       " 0.6923076873372783,\n",
       " 0.6857142807183674,\n",
       " 0.9333333283555556,\n",
       " 0.999999995,\n",
       " 0.4347826037807184,\n",
       " 0.29999999520000004,\n",
       " 0.699999995,\n",
       " 0.33333332888888895,\n",
       " 0.7272727224380167,\n",
       " 0.6999999950500001,\n",
       " 0.823529406782007,\n",
       " 0.285714281122449,\n",
       " 0.7142857092857143,\n",
       " 0.7777777727777778,\n",
       " 0.6153846106508877,\n",
       " 0.4390243853896491,\n",
       " 0.7777777728395062,\n",
       " 0.6666666617283951,\n",
       " 0.4999999950347222,\n",
       " 0.2857142807142858,\n",
       " 0.3999999950000001,\n",
       " 0.4166666616666667,\n",
       " 0.3076923034319527,\n",
       " 0.5599999950720002,\n",
       " 0.3999999950222222,\n",
       " 0.5185185138545954,\n",
       " 0.42105262670360116,\n",
       " 0.9189189139225714,\n",
       " 0.4888888840098766,\n",
       " 0.3999999950222222,\n",
       " 0.6363636317355372,\n",
       " 0.3809523759637189,\n",
       " 0.5217391256710776,\n",
       " 0.13333332835555575,\n",
       " 0.999999995,\n",
       " 0.8235294067647059,\n",
       " 0.7368421002770082,\n",
       " 0.44444443944444445,\n",
       " 0.9444444394444445,\n",
       " 0.4615384568047337,\n",
       " 0.3809523762358277,\n",
       " 0.3529411716262976,\n",
       " 0.7058823480968859,\n",
       " 0.555555550617284,\n",
       " 0.999999995,\n",
       " 0.7619047569160999,\n",
       " 0.6999999950500001,\n",
       " 0.26086956025519853,\n",
       " 0.7777777727777778,\n",
       " 0.6315789423822715,\n",
       " 0.3333333283333334,\n",
       " 0.666666661701389,\n",
       " 0.5555555505555557,\n",
       " 0.30769230316568047,\n",
       " 0.6666666619135801,\n",
       " 0.2999999950500001,\n",
       " 0.6666666616666668,\n",
       " 0.45161289823100936,\n",
       " 0.4999999950000001,\n",
       " 0.5333333283555556,\n",
       " 0.3809523759637189,\n",
       " 0.39999999520000007,\n",
       " 0.999999995,\n",
       " 0.3333333284222223,\n",
       " 0.624999995,\n",
       " 0.6363636314049588,\n",
       " 0.37499999507812504,\n",
       " 0.27586206397146257,\n",
       " 0.888888883888889,\n",
       " 0.7741935433922998,\n",
       " 0.6999999950500001,\n",
       " 0.5882352892733564,\n",
       " 0.5454545408264463,\n",
       " 0.4999999950781251,\n",
       " 0.5263157848199447,\n",
       " 0.8181818132231407,\n",
       " 0.37499999507812504,\n",
       " 0.7499999950781251,\n",
       " 0.47999999539200017,\n",
       " 0.7999999950222222,\n",
       " 0.3478260821928167,\n",
       " 0.33333332835555557,\n",
       " 0.6666666616666668,\n",
       " 0.7199999950079999,\n",
       " 0.5882352892733564,\n",
       " 0.4999999950347222,\n",
       " 0.639999995072,\n",
       " 0.9142857092897959,\n",
       " 0.6153846104142012,\n",
       " 0.499999995138889,\n",
       " 0.38461537961538467,\n",
       " 0.21052631091412755,\n",
       " 0.6842105214404434,\n",
       " 0.24999999520000007,\n",
       " 0.470588230449827,\n",
       " 0.35294117148788934,\n",
       " 0.4166666619791667,\n",
       " 0.2941176420761246,\n",
       " 0.4166666619791667,\n",
       " 0.3529411719031142,\n",
       " 0.5999999952,\n",
       " 0.874999995,\n",
       " 0.5714285664285715,\n",
       " 0.9230769181065088,\n",
       " 0.6363636313636364,\n",
       " 0.8181818132231407,\n",
       " 0.5333333283555556,\n",
       " 0.3999999950000001,\n",
       " 0.4444444395061729,\n",
       " 0.6363636314049588,\n",
       " 0.7428571378612244,\n",
       " 0.28571428126984133,\n",
       " 0.6999999950500001,\n",
       " 0.4615384568047337,\n",
       " 0.3333333283333334,\n",
       " 0.3333333283333334,\n",
       " 0.4347826037807184,\n",
       " 0.6315789423822715,\n",
       " 0.2666666617555556,\n",
       " 0.8461538411834321,\n",
       " 0.5714285664285715,\n",
       " 0.6666666616666668,\n",
       " 0.699999995,\n",
       " 0.7368421002770082,\n",
       " 0.5333333283555556,\n",
       " 0.39999999520000007,\n",
       " 0.9565217341304348,\n",
       " 0.6315789424930748,\n",
       " 0.2857142807256236]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_l_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
