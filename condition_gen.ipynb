{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from transformers import BertConfig, BertTokenizerFast\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from models import BertForDiffusion, DiffusionLM, ConditionalDiffusionLM\n",
    "from data_utils import load_qqp_dataset_and_tokenizer_from_disk, QQPParaphraseDataset, load_split_qqp_dataset_and_tokenizer_from_disk\n",
    "from noise_schedule import get_named_beta_schedule\n",
    "from train_utils import train_conditional, evaluate_conditional\n",
    "from metric_utils import calculate_bleu, calculate_rouge\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataset args\n",
    "max_len = 32\n",
    "\n",
    "# training args\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda:1\")\n",
    "lr = 1e-4\n",
    "num_epoch = 30\n",
    "weight_decay = 0\n",
    "num_warmup_steps = 100\n",
    "\n",
    "# model args\n",
    "word_embedding_dim = 128\n",
    "# hidden_size = 768\n",
    "# num_hidden_layers = 12\n",
    "# num_attention_heads = 12\n",
    "# intermediate_size = 3072\n",
    "hidden_size = 512\n",
    "num_hidden_layers = 4\n",
    "num_attention_heads = 8\n",
    "intermediate_size = 2048\n",
    "max_position_embeddings = max_len\n",
    "\n",
    "encoder_type = 'from-scratch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 15672\n",
      "Training set size: 120940\n",
      "Evaluation set size: 13438\n"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset, tokenizer = load_split_qqp_dataset_and_tokenizer_from_disk(data_path=\"data\")\n",
    "\n",
    "# tokenized_qqp_train, tokenized_qqp_eval, tokenizer = load_qqp_dataset_and_tokenizer_from_disk(data_path=\"data\")\n",
    "\n",
    "rev_tokenizer = {v: k for k, v in tokenizer.items()}\n",
    "\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
    "\n",
    "# train_dataset = QQPParaphraseDataset(dataset=tokenized_qqp_train, random_swap=True)\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "# eval_dataset = QQPParaphraseDataset(dataset=tokenized_qqp_eval, random_swap=False)\n",
    "print(\"Evaluation set size:\", len(eval_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"T\": 2000,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 32,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 15672\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=len(tokenizer), hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, max_position_embeddings=max_position_embeddings, pad_token_id=tokenizer['[PAD]'])\n",
    "\n",
    "config.T = 2000\n",
    "# comment next line if using bit word embedding\n",
    "#config.word_embedding_dim = word_embedding_dim\n",
    "\n",
    "print(config)\n",
    "\n",
    "betas = torch.Tensor(get_named_beta_schedule(schedule_name=\"cosine\", num_diffusion_timesteps=config.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using learned word embedding\n",
      "set word_embedding_dim to: 512\n",
      "Diffusion model #parameters:\n",
      "47412736\n",
      "Diffusion model #trainable parameters\n",
      "47412736\n"
     ]
    }
   ],
   "source": [
    "diffusion_model = ConditionalDiffusionLM(\n",
    "    config=config, \n",
    "    betas=betas, \n",
    "    use_shared_weight=True, \n",
    "    lm_head_bias=False, \n",
    "    add_emb_noise=False, \n",
    "    conditional_gen=True, \n",
    "    encoder_type=encoder_type, \n",
    "    encoder_name_or_path='bert-base-uncased', \n",
    "    emb_type='learned',\n",
    ").to(device)\n",
    "\n",
    "print(\"Diffusion model #parameters:\")\n",
    "print(sum([p.numel() for p in diffusion_model.parameters()]))\n",
    "\n",
    "print(\"Diffusion model #trainable parameters\")\n",
    "print(sum([p.numel() for p in filter(lambda p:p.requires_grad, diffusion_model.parameters())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p:p.requires_grad, diffusion_model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_epoch*len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99afb48f8df5410482682d0703cd572f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 100\n",
      "mse  training loss=0.72279\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.35603\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.30105\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.27298\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.24881\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.23006\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.21545\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.20419\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.19804\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.19350\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.18554\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.18030\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.17496\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.16971\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.16446\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.16184\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.15944\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.15634\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.14635\n",
      "epoch: 2\n",
      "step: 100\n",
      "mse  training loss=0.15145\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.14411\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.14397\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.14099\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.13808\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.13624\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.13575\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.13196\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.13030\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.12611\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.12562\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.12429\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.12146\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.12231\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.12092\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.11770\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.11645\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.11496\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.10746\n",
      "epoch: 3\n",
      "step: 100\n",
      "mse  training loss=0.11288\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.11326\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.11060\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.10733\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.10783\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.10847\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.10517\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.10440\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.10387\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.10415\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.10284\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.10370\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.10080\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.09902\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.09793\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.09841\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.09631\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.09659\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.08913\n",
      "epoch: 4\n",
      "step: 100\n",
      "mse  training loss=0.09259\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.09446\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.09358\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.09295\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.09013\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.08905\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.09015\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.08939\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.08924\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.08769\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.08681\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.08636\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.08702\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.08391\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.08545\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.08388\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.08463\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.08506\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.07819\n",
      "epoch: 5\n",
      "step: 100\n",
      "mse  training loss=0.08143\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.08299\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.08184\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.08158\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.08114\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.07805\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.07983\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.07851\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.07836\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.07869\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.07727\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.07869\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.07628\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.07702\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.07427\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.07423\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.07467\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.07463\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.07004\n",
      "epoch: 6\n",
      "step: 100\n",
      "mse  training loss=0.07287\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.07205\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.07255\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.07318\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.07247\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.07096\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.07120\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.07162\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.06929\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.07144\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.06959\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.06969\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.06860\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.06824\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.06808\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.06768\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.06816\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.06858\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.06295\n",
      "epoch: 7\n",
      "step: 100\n",
      "mse  training loss=0.06645\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.06654\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.06472\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.06601\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.06532\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.06321\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.06440\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.06420\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.06409\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.06307\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.06361\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.06449\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.06448\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.06309\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.06252\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.06277\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.06102\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.06225\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.05801\n",
      "epoch: 8\n",
      "step: 100\n",
      "mse  training loss=0.06107\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.05957\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.06065\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.05920\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.05952\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.06053\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.05891\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.05990\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.05830\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.05945\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.05950\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.05991\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.05869\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.05958\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.05852\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.05927\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.05711\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.05714\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.05328\n",
      "epoch: 9\n",
      "step: 100\n",
      "mse  training loss=0.05588\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.05727\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.05595\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.05599\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.05674\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.05588\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.05603\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.05587\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.05632\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.05453\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.05409\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.05268\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.05329\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.05375\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.05561\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.05525\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.05271\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.05357\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.05079\n",
      "epoch: 10\n",
      "step: 100\n",
      "mse  training loss=0.05262\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.05232\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.05281\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.05316\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.05310\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.05323\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.05165\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.05180\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.05136\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.05148\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.05147\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.05080\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.05025\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.04969\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.05063\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.05141\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.05071\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.04954\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.04671\n",
      "epoch: 11\n",
      "step: 100\n",
      "mse  training loss=0.05081\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.04890\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.04948\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.04937\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.04887\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.04958\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.04907\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.05031\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.05005\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.04860\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.04846\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.04864\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.04789\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.04819\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.04860\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.04734\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.04803\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.04839\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.04554\n",
      "epoch: 12\n",
      "step: 100\n",
      "mse  training loss=0.04800\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.04721\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.04729\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.04626\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.04634\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.04603\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.04731\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.04673\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.04591\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.04773\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.04567\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.04593\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.04609\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.04514\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.04492\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.04595\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.04595\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.04497\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.04286\n",
      "epoch: 13\n",
      "step: 100\n",
      "mse  training loss=0.04514\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.04497\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.04440\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.04431\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.04460\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.04458\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.04414\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.04496\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.04406\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.04516\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.04415\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.04436\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.04434\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.04488\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.04299\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.04327\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.04401\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1800\n",
      "mse  training loss=0.04372\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "Evaluating...\n",
      "eval loss=0.04145\n",
      "epoch: 14\n",
      "step: 100\n",
      "mse  training loss=0.04357\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.04278\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.04349\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.04240\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.04274\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.04182\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.04296\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.04288\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 900\n",
      "mse  training loss=0.04191\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.04177\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.04249\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.04208\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1300\n",
      "mse  training loss=0.04353\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1400\n",
      "mse  training loss=0.04170\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1500\n",
      "mse  training loss=0.04162\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.04285\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.04175\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.04135\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03930\n",
      "epoch: 15\n",
      "step: 100\n",
      "mse  training loss=0.04189\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 200\n",
      "mse  training loss=0.04113\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 300\n",
      "mse  training loss=0.04169\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 400\n",
      "mse  training loss=0.04261\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 500\n",
      "mse  training loss=0.04035\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 600\n",
      "mse  training loss=0.04068\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 700\n",
      "mse  training loss=0.04081\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 800\n",
      "mse  training loss=0.04149\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.04075\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1000\n",
      "mse  training loss=0.04004\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1100\n",
      "mse  training loss=0.04111\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1200\n",
      "mse  training loss=0.04162\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.04027\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.04070\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.04116\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1600\n",
      "mse  training loss=0.04084\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00000\n",
      "step: 1700\n",
      "mse  training loss=0.04010\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.04036\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03775\n",
      "epoch: 16\n",
      "step: 100\n",
      "mse  training loss=0.04035\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.04050\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03949\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.04022\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.04035\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03948\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03986\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.04019\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03993\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03933\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03936\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03981\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03914\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03927\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03900\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03912\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03941\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03762\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03684\n",
      "epoch: 17\n",
      "step: 100\n",
      "mse  training loss=0.03903\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03881\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03817\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03852\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03905\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03841\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03804\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03790\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03873\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03852\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03878\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03827\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03910\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03820\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03868\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03724\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03798\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03767\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03585\n",
      "epoch: 18\n",
      "step: 100\n",
      "mse  training loss=0.03741\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03758\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03772\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03745\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03793\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03806\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03796\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03799\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03738\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03796\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03768\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03662\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03689\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03694\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03643\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03660\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03771\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03683\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03495\n",
      "epoch: 19\n",
      "step: 100\n",
      "mse  training loss=0.03688\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03737\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03629\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03784\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03745\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03662\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03638\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03650\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03593\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03663\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03651\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03724\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03704\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03614\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03667\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03711\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03596\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03629\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03395\n",
      "epoch: 20\n",
      "step: 100\n",
      "mse  training loss=0.03587\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03567\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03530\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03590\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03638\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03620\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03651\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03626\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03516\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03627\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03534\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03642\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03636\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03655\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03631\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03646\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03543\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03559\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03401\n",
      "epoch: 21\n",
      "step: 100\n",
      "mse  training loss=0.03565\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03525\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03639\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03499\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03538\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03528\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03534\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03584\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03437\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03594\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03541\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03468\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03480\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03545\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03564\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03445\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03551\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03490\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03359\n",
      "epoch: 22\n",
      "step: 100\n",
      "mse  training loss=0.03565\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03482\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03453\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03535\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03499\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03477\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03507\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03439\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03451\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03397\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03529\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03551\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03443\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03529\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03434\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03539\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03418\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03507\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03268\n",
      "epoch: 23\n",
      "step: 100\n",
      "mse  training loss=0.03481\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03456\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03336\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03431\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03421\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03398\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03447\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03443\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03464\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03404\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03473\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03503\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03460\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03386\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03467\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03450\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03435\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03437\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03259\n",
      "epoch: 24\n",
      "step: 100\n",
      "mse  training loss=0.03357\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03429\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03448\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03462\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03441\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03356\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03388\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03374\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03461\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03422\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03436\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03428\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03483\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03534\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03376\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03471\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03302\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03360\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03219\n",
      "epoch: 25\n",
      "step: 100\n",
      "mse  training loss=0.03391\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03378\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03412\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03341\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03332\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03393\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03351\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03391\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03392\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03428\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03400\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03364\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03440\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03414\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03416\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03339\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03297\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03383\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03234\n",
      "epoch: 26\n",
      "step: 100\n",
      "mse  training loss=0.03352\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03259\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03351\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03323\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03438\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03376\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03355\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03373\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03348\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03430\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03352\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03370\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03358\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03431\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03358\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03403\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03386\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03402\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03171\n",
      "epoch: 27\n",
      "step: 100\n",
      "mse  training loss=0.03380\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03416\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03388\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03368\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03365\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03436\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03340\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03380\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03425\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03359\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03357\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03327\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03353\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03382\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03305\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03342\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03312\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03405\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03212\n",
      "epoch: 28\n",
      "step: 100\n",
      "mse  training loss=0.03317\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03326\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03326\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03279\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03392\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03385\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03335\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03347\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03344\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03314\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03416\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03392\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03403\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03417\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03392\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03307\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03287\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03389\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03167\n",
      "epoch: 29\n",
      "step: 100\n",
      "mse  training loss=0.03281\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03310\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03327\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03417\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03357\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03288\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03320\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03282\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03373\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03379\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03311\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03358\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03259\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03392\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03323\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03367\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03395\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03362\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03190\n",
      "epoch: 30\n",
      "step: 100\n",
      "mse  training loss=0.03327\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 200\n",
      "mse  training loss=0.03306\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 300\n",
      "mse  training loss=0.03346\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 400\n",
      "mse  training loss=0.03333\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 500\n",
      "mse  training loss=0.03400\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 600\n",
      "mse  training loss=0.03389\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 700\n",
      "mse  training loss=0.03329\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 800\n",
      "mse  training loss=0.03230\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 900\n",
      "mse  training loss=0.03393\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1000\n",
      "mse  training loss=0.03390\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1100\n",
      "mse  training loss=0.03323\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1200\n",
      "mse  training loss=0.03355\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1300\n",
      "mse  training loss=0.03302\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1400\n",
      "mse  training loss=0.03348\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1500\n",
      "mse  training loss=0.03358\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1600\n",
      "mse  training loss=0.03314\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1700\n",
      "mse  training loss=0.03361\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "step: 1800\n",
      "mse  training loss=0.03365\n",
      "L_T  training loss=0.00000\n",
      "rounding  training loss=0.00001\n",
      "Evaluating...\n",
      "eval loss=0.03143\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "# loss_terms_dict_lst = []\n",
    "loss_terms_dict = defaultdict(list)\n",
    "loss_terms_weights = None\n",
    "verbose = True\n",
    "print_steps=100\n",
    "progress_bar = tqdm(range(num_epoch*len(train_dataloader)))\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    print(\"epoch:\",epoch+1)\n",
    "    # loss_terms_dict_lst.append(train_conditional(diffusion_model=diffusion_model, dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler ,progress_bar=progress_bar ,verbose=True))\n",
    "    device = next(diffusion_model.parameters()).device\n",
    "\n",
    "    if loss_terms_weights is None:\n",
    "        loss_terms_weights = {}\n",
    "        for term_name in diffusion_model.loss_terms:\n",
    "            loss_terms_weights[term_name] = 1\n",
    "\n",
    "    training_loss = {}\n",
    "    for term_name in diffusion_model.loss_terms:\n",
    "        training_loss[term_name] = 0\n",
    "    sample_cnt = 0\n",
    "    diffusion_model.train()\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "\n",
    "        if diffusion_model.model.encoder_type in ['frozen', 'fine-tune']:\n",
    "            # use input_ids and attention mask from bert tokenizer\n",
    "            # question1 as source, question2 as target\n",
    "            input_ids = data['question2_input_ids'].to(device)\n",
    "            attention_mask = data['question2_attention_mask'].to(device)\n",
    "            src_ids = data['question1_input_ids_bert'].to(device)\n",
    "            src_attention_mask = data['question1_attention_mask_bert'].to(device)\n",
    "            loss_terms = diffusion_model(input_ids=input_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            src_ids=src_ids,\n",
    "                                            src_attention_mask=src_attention_mask)\n",
    "        elif diffusion_model.model.encoder_type in ['from-scratch']:\n",
    "            # use input_ids and attention mask from custom tokenizer\n",
    "            input_ids = data['question2_input_ids'].to(device)\n",
    "            attention_mask = data['question2_attention_mask'].to(device)\n",
    "            src_ids = data['question1_input_ids'].to(device)\n",
    "            src_attention_mask = data['question1_attention_mask'].to(device)\n",
    "            loss_terms = diffusion_model(input_ids=input_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            src_ids=src_ids,\n",
    "                                            src_attention_mask=src_attention_mask)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        loss = sum([v.mean()*loss_terms_weights[k] for k, v in loss_terms.items()])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        bs = input_ids.shape[0]\n",
    "        sample_cnt += bs\n",
    "        # training_loss += loss.detach().cpu() * bs\n",
    "\n",
    "        for k, v in loss_terms.items():\n",
    "            loss_term = (v.detach().cpu().mean()*loss_terms_weights[k]).item()\n",
    "            loss_terms_dict[k].append(loss_term)\n",
    "            training_loss[k] += loss_term * bs\n",
    "\n",
    "        if verbose and step % print_steps == print_steps-1:\n",
    "            # print training loss\n",
    "            # training_loss /= sample_cnt\n",
    "            print('step:', step+1)\n",
    "            for k, v in training_loss.items():\n",
    "                print(k, ' training loss={:.5f}'.format(v/sample_cnt))\n",
    "            sample_cnt = 0\n",
    "            training_loss = {}\n",
    "            for term_name in diffusion_model.loss_terms:\n",
    "                training_loss[term_name] = 0\n",
    "\n",
    "    # evaluate_conditional(diffusion_model=diffusion_model, dataloader=eval_dataloader,)\n",
    "    device = next(diffusion_model.parameters()).device\n",
    "\n",
    "    if loss_terms_weights is None:\n",
    "        loss_terms_weights = {}\n",
    "        for term_name in diffusion_model.loss_terms:\n",
    "            loss_terms_weights[term_name] = 1\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    diffusion_model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in eval_dataloader:\n",
    "            \n",
    "            if diffusion_model.model.encoder_type in ['frozen', 'fine-tune']:\n",
    "                # use input_ids and attention mask from bert tokenizer\n",
    "                # question1 as source, question2 as target\n",
    "                input_ids = data['question2_input_ids'].to(device)\n",
    "                attention_mask = data['question2_attention_mask'].to(device)\n",
    "                src_ids = data['question1_input_ids_bert'].to(device)\n",
    "                src_attention_mask = data['question1_attention_mask_bert'].to(device)\n",
    "                loss_terms = diffusion_model(input_ids=input_ids,\n",
    "                                             attention_mask=attention_mask,\n",
    "                                             src_ids=src_ids,\n",
    "                                             src_attention_mask=src_attention_mask)\n",
    "            elif diffusion_model.model.encoder_type in ['from-scratch']:\n",
    "                # use input_ids and attention mask from custom tokenizer\n",
    "                input_ids = data['question2_input_ids'].to(device)\n",
    "                attention_mask = data['question2_attention_mask'].to(device)\n",
    "                src_ids = data['question1_input_ids'].to(device)\n",
    "                src_attention_mask = data['question1_attention_mask'].to(device)\n",
    "                loss_terms = diffusion_model(input_ids=input_ids,\n",
    "                                             attention_mask=attention_mask,\n",
    "                                             src_ids=src_ids,\n",
    "                                             src_attention_mask=src_attention_mask)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            loss = sum([v.mean()*loss_terms_weights[k] for k, v in loss_terms.items()])\n",
    "            bs = input_ids.shape[0]\n",
    "            test_loss += bs*loss\n",
    "\n",
    "        test_loss /= len(eval_dataloader.dataset)\n",
    "        if verbose:\n",
    "            print('eval loss={:.5f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6f20076bd0>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6nElEQVR4nO3deXxU1cH/8e9kmyRAEiCQsAQCIiICYY9BqVqjUSkutZVSKkqtFsX+VForqIBoNdQq2laUuqA+bRXUihsIYmQRiaCByL4viUA2MPs+c39/hAyMSSAJCYdwP+/Xa15D7px775nTPE++nu06LMuyBAAAYIiP6QoAAAB7I4wAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMMrPdAXqw+1269ChQ2rTpo0cDofp6gAAgHqwLEsFBQXq3LmzfHzq7v9oEWHk0KFDioqKMl0NAADQCOnp6eratWudn7eIMNKmTRtJVV8mJCTEcG0AAEB95OfnKyoqyvN3vC4tIoxUD82EhIQQRgAAaGFONcWCCawAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjWsSD8prLa6v3Ke1IkX4d210XRJ78iYIAAKB52Lpn5JONh/Rm8gEdOFJkuioAANiWrcOIv0/V1690W4ZrAgCAfdk6jDgcVe9uizACAIAphBFJZBEAAMyxdRgBAADm2TqMOFTVNULHCAAA5tg7jHiGaYgjAACYQhgBAABG2TuMVA/T0DECAIAx9g4j1cM0zBoBAMAYW4eRavSMAABgDmEEAAAYZesw4nAwZwQAANPsHUaOvZNFAAAwx95hhH1GAAAwzt5h5Ng7UQQAAHNsHUY8SCMAABhj6zDimcBKGgEAwBh7h5Fj70wZAQDAHHuHEZ5NAwCAcbYOI9V9I3SMAABgjq3DyPGlvWbrAQCAndk6jFRjAisAAObYOowwgRUAAPPsHUaqh2nMVgMAAFuzdxgRk0YAADDN3mGEpb0AABhHGBHDNAAAmGTrMFKNURoAAMyxdRipnjNikUYAADDG1mFEDNMAAGCcrcMI+4wAAGCevcOIg2fTAABgmr3DiOkKAAAAe4eRakxgBQDAnAaHkVWrVmn06NHq3LmzHA6HPvjgg1Oes2LFCg0ePFhOp1O9evXSG2+80YiqNj02PQMAwLwGh5GioiLFxMRozpw59Sq/b98+jRo1SldccYVSU1N1//3363e/+52WLl3a4Mo2NbIIAADm+TX0hGuvvVbXXnttvcvPnTtXPXr00LPPPitJuvDCC7V69Wo999xzSkhIaOjtm5RnAiujNAAAGNPsc0aSk5MVHx/vdSwhIUHJycnNfet6s1hPAwCAMQ3uGWmojIwMRUREeB2LiIhQfn6+SkpKFBQUVOOcsrIylZWVeX7Oz89vlrqxzwgAAOadlatpEhMTFRoa6nlFRUU1z42YNAIAgHHNHkYiIyOVmZnpdSwzM1MhISG19opI0tSpU5WXl+d5paenN2sd6RgBAMCcZh+miYuL0+LFi72OLVu2THFxcXWe43Q65XQ6m7tqJzwor9lvBQAA6tDgnpHCwkKlpqYqNTVVUtXS3dTUVKWlpUmq6tUYP368p/zEiRO1d+9e/fnPf9b27dv14osv6p133tEDDzzQNN/gNDg8D8ojjQAAYEqDw8i3336rQYMGadCgQZKkyZMna9CgQZo+fbok6fDhw55gIkk9evTQokWLtGzZMsXExOjZZ5/Vq6++anxZr8SUEQAAzgYNHqa5/PLLT7p9em27q15++eXasGFDQ291xjBMAwCAOWflapozhe3gAQAwz95hhIEaAACMs3cYqZ7AyjgNAADG2DqMVCOLAABgjq3DyPGlvQAAwBRbhxEW9wIAYJ7Nw0gVhmkAADDH1mGEHVgBADDP3mHk2Ds9IwAAmGPvMMKUEQAAjLN1GKlGxwgAAObYOox4dmBlnAYAAGPsHUYYpgEAwDh7h5Fj7/SLAABgjq3DSDVGaQAAMMfWYcRxbJyGfUYAADDH1mEEAACYZ+sw4mAxDQAAxtk6jFQjiwAAYI6tw0j1PiP0jAAAYI69wwj7jAAAYJytw0g1VtMAAGCOrcOIp2OELAIAgDH2DiMM0wAAYJzNw0j1pmcAAMAUW4eRahbLaQAAMMbWYcTzoDyyCAAAxtg6jIg5IwAAGGfrMOLZ9MxwPQAAsDNbh5FqDNMAAGCOrcOI50F59I0AAGCMvcOI6QoAAAB7h5FqDNMAAGCOrcMIO7ACAGCevcMIAzUAABhn7zBSPYGVcRoAAIyxdRipRhQBAMAcW4cRtoMHAMA8W4cRZrACAGCercOIp2eEgRoAAIyxdRipxjANAADm2DqMHN8OHgAAmGLvMMI+IwAAGGfvMOLZZ8RsPQAAsDNbh5HjSCMAAJhi6zDCIA0AAObZO4wwTAMAgHG2DiPVCCMAAJhj6zDiONY1wqZnAACYY+swAgAAzGtUGJkzZ46io6MVGBio2NhYrVu37qTln3/+eV1wwQUKCgpSVFSUHnjgAZWWljaqwk2JOSMAAJjX4DCyYMECTZ48WTNmzND69esVExOjhIQEZWVl1Vr+rbfe0pQpUzRjxgxt27ZNr732mhYsWKCHH374tCvfVMgiAACY0+AwMnv2bN15552aMGGC+vbtq7lz5yo4OFjz5s2rtfyaNWt0ySWX6Ne//rWio6N19dVXa+zYsafsTTkT2IEVAADzGhRGysvLlZKSovj4+OMX8PFRfHy8kpOTaz1nxIgRSklJ8YSPvXv3avHixbruuuvqvE9ZWZny8/O9Xs2BYRoAAMzza0jhnJwcuVwuRUREeB2PiIjQ9u3baz3n17/+tXJycnTppZfKsixVVlZq4sSJJx2mSUxM1MyZMxtStUap7hdhNQ0AAOY0+2qaFStW6KmnntKLL76o9evX6/3339eiRYv0xBNP1HnO1KlTlZeX53mlp6c3byXJIgAAGNOgnpHw8HD5+voqMzPT63hmZqYiIyNrPWfatGm69dZb9bvf/U6S1L9/fxUVFemuu+7SI488Ih+fmnnI6XTK6XQ2pGqN4mDKCAAAxjWoZyQgIEBDhgxRUlKS55jb7VZSUpLi4uJqPae4uLhG4PD19ZUkWYYna1RPYKVjBAAAcxrUMyJJkydP1m233aahQ4dq+PDhev7551VUVKQJEyZIksaPH68uXbooMTFRkjR69GjNnj1bgwYNUmxsrHbv3q1p06Zp9OjRnlBimulQBACAnTU4jIwZM0bZ2dmaPn26MjIyNHDgQC1ZssQzqTUtLc2rJ+TRRx+Vw+HQo48+qoMHD6pDhw4aPXq0nnzyyab7Fo3kWU1jthoAANiaw2oB3QL5+fkKDQ1VXl6eQkJCmuy6r365V39ZtE03DOysv/9qUJNdFwAA1P/vt62fTeN5UN5ZH8cAADh32TqMVCOLAABgjq3DCCt7AQAwz95hxLMdPH0jAACYYu8wcuydKAIAgDm2DiMepBEAAIyxdRhxsB88AADG2TyMVL3z1F4AAMyxdxg59s78VQAAzLF1GKlGGAEAwBx7hxHmjAAAYJytw8jxpb10jQAAYIqtw0g1hmkAADDH1mGEURoAAMyzdxg5NlBDxwgAAObYO4x4nk1jth4AANiZrcPIcaQRAABMsXUYYcoIAADm2TuMMEwDAIBx9g4jTGAFAMA4W4eRahZdIwAAGGPvMMKkEQAAjLN1GDm+HTwAADDF3mHk2AxWRmkAADDH1mEEAACYZ+swwjANAADm2TuMePYZIY4AAGCKrcMIAAAwz9ZhxMHSXgAAjLN3GBGraQAAMM3eYaR6zghTWAEAMMbWYaQaPSMAAJhDGAEAAEbZOoywAysAAObZO4wce2fOCAAA5tg6jAAAAPNsHUaO78Bqth4AANiZvcNI9T4jhusBAICd2TqMeJBGAAAwxtZhhO3gAQAwz95h5Ng7q2kAADDH3mGECawAABhn6zBSjSwCAIA5Ng8jTBoBAMA0W4eR48M09I0AAGCKvcPIsXeiCAAA5tg6jAAAAPNsHUZ4ai8AAObZO4wceyeLAABgjr3DiCeNEEcAADClUWFkzpw5io6OVmBgoGJjY7Vu3bqTls/NzdWkSZPUqVMnOZ1O9e7dW4sXL25UhQEAwLnFr6EnLFiwQJMnT9bcuXMVGxur559/XgkJCdqxY4c6duxYo3x5ebmuuuoqdezYUe+99566dOmiAwcOKCwsrCnqf1o8S3vNVgMAAFtrcBiZPXu27rzzTk2YMEGSNHfuXC1atEjz5s3TlClTapSfN2+ejh49qjVr1sjf31+SFB0dfXq1biIOMYEVAADTGjRMU15erpSUFMXHxx+/gI+P4uPjlZycXOs5H330keLi4jRp0iRFRESoX79+euqpp+Ryueq8T1lZmfLz871eAADg3NSgMJKTkyOXy6WIiAiv4xEREcrIyKj1nL179+q9996Ty+XS4sWLNW3aND377LP6y1/+Uud9EhMTFRoa6nlFRUU1pJr15xmmoWsEAABTmn01jdvtVseOHfXyyy9ryJAhGjNmjB555BHNnTu3znOmTp2qvLw8zys9Pb1Z6sZiGgAAzGvQnJHw8HD5+voqMzPT63hmZqYiIyNrPadTp07y9/eXr6+v59iFF16ojIwMlZeXKyAgoMY5TqdTTqezIVVrFDY9AwDAvAb1jAQEBGjIkCFKSkryHHO73UpKSlJcXFyt51xyySXavXu33G6359jOnTvVqVOnWoMIAACwlwYP00yePFmvvPKK3nzzTW3btk133323ioqKPKtrxo8fr6lTp3rK33333Tp69Kjuu+8+7dy5U4sWLdJTTz2lSZMmNd23aCR2YAUAwLwGL+0dM2aMsrOzNX36dGVkZGjgwIFasmSJZ1JrWlqafHyOZ5yoqCgtXbpUDzzwgAYMGKAuXbrovvvu00MPPdR036KRPPuMME4DAIAxDqsF/CXOz89XaGio8vLyFBIS0mTXXb0rR795ba36RLbRkvt/0mTXBQAA9f/7betn0wAAAPNsHUaOD9OYrQcAAHZm7zBy7J1NzwAAMMfWYQQAAJhn7zDCMA0AAMbZOox4ntpruB4AANiZvcMI+4wAAGCcrcMIAAAwz9ZhhO3gAQAwz95hxDNOY7YeAADYmc3DSNU7WQQAAHNsHUYAAIB5tg4jnjkjrKYBAMAYe4cRhmkAADDO1mHkeN8IAAAwxeZhpAqjNAAAmGPrMHJ8mIY0AgCAKfYOI8fe6RkBAMAcW4cRAABgnq3DSPUOrPSMAABgjr3DiOkKAAAAm4eR6gmsdI0AAGCMrcMIAAAwz9ZhxHFsoIZ+EQAAzLF3GPEM05itBwAAdmbrMAIAAMwjjIgdWAEAMMnWYYRhGgAAzLN3GGECKwAAxtk6jAAAAPMII5KyC8pMVwEAANuydRjZejjfdBUAALA9W4cRl9ttugoAANiercOIg0flAQBgnL3DCFkEAADjbB5GSCMAAJhm7zBiugIAAMDmYYQ0AgCAcbYOIz6kEQAAjLN1GCGLAABgnq3DCAAAMM/WYYTVNAAAmGfvMGK6AgAAwOZhhDQCAIBx9g4jJ/SN5BaXG6wJAAD2Ze8wckLPyMHcEnMVAQDAxmwdRnxOCCOWZa4eAADYma3DyIkBpKTCZa4iAADYmK3DSKX7eBo5xDANAABGNCqMzJkzR9HR0QoMDFRsbKzWrVtXr/Pmz58vh8OhG2+8sTG3bXInzhlxuRmnAQDAhAaHkQULFmjy5MmaMWOG1q9fr5iYGCUkJCgrK+uk5+3fv19/+tOfNHLkyEZXtqmd+GwawggAAGY0OIzMnj1bd955pyZMmKC+fftq7ty5Cg4O1rx58+o8x+Vyady4cZo5c6Z69ux5WhVuSiduM/KftWnG6gEAgJ01KIyUl5crJSVF8fHxxy/g46P4+HglJyfXed7jjz+ujh076o477qjXfcrKypSfn+/1ag7ndWzt+fd36bnNcg8AAHByDQojOTk5crlcioiI8DoeERGhjIyMWs9ZvXq1XnvtNb3yyiv1vk9iYqJCQ0M9r6ioqIZUs97aBgc0y3UBAED9NetqmoKCAt1666165ZVXFB4eXu/zpk6dqry8PM8rPT29GWsJAABM8mtI4fDwcPn6+iozM9PreGZmpiIjI2uU37Nnj/bv36/Ro0d7jrnd7qob+/lpx44dOu+882qc53Q65XQ6G1I1AADQQjWoZyQgIEBDhgxRUlKS55jb7VZSUpLi4uJqlO/Tp482bdqk1NRUz+v666/XFVdcodTU1GYbfgEAAC1Hg3pGJGny5Mm67bbbNHToUA0fPlzPP/+8ioqKNGHCBEnS+PHj1aVLFyUmJiowMFD9+vXzOj8sLEySahwHAAD21OAwMmbMGGVnZ2v69OnKyMjQwIEDtWTJEs+k1rS0NPn42HpjVwAA0AAOyzr7HxGXn5+v0NBQ5eXlKSQkpMmum11QpmFPfu75ef+sUU12bQAA7K6+f7/pwgAAAEYRRgAAgFG2DiPtW7HpGQAAptk6jPj4OE5dCAAANCtbhxEAAGAeYQQAABhFGAEAAEYRRgAAgFGEkRNUuNymqwAAgO0QRk7w8qq9pqsAAIDtEEZOsDe7yHQVAACwHcIIAAAwijByAgd7oAEAcMYRRk5QUFphugoAANgOYeQEuzILTVcBAADbIYycYG8OE1gBADjTCCM/sjurwHQVAACwFcLIj1z79y9NVwEAAFshjPxIhcsyXQUAAGyFMAIAAIwijAAAAKNsH0ZCg/xNVwEAAFuzfRjxYddVAACMsn0Y8SWNAABglO3DiIMH0gAAYBRhxHQFAACwOduHET+GaQAAMMr2YWTcxd1NVwEAAFuzfRipbWlvcXmlgZoAAGBPtg8jtdmTxdN7AQA4U2wfRnxYTQMAgFG2DyM/i+lU45glHpYHAMCZYvswEuTvW+PY9S98ZaAmAADYk+3DCMM0AACYZfswUtd28Fn5pWe4JgAA2JPtw0hdxr261nQVAACwBcJIHXZlFZquAgAAtkAYAQAARhFGAACAUYQRAABgFGFEUt9OIbUej56ySHNX7jnDtQEAwF4II5L8fOvea2TWp9vPYE0AALAfwoikWT8fYLoKAADYFmFEUt/OtQ/TVFu08fAZqgkAAPZDGKmHSW+tN10FAADOWYQRAABgFGGknsor3aarAADAOYkwUk/3/JehGgAAmgNh5JiP7730pJ9/vi3zDNUEAAB7IYwc079r6CnLzF62U7nF5WegNgAA2EejwsicOXMUHR2twMBAxcbGat26dXWWfeWVVzRy5Ei1bdtWbdu2VXx8/EnLn83+kbRLjyzcbLoaAACcUxocRhYsWKDJkydrxowZWr9+vWJiYpSQkKCsrKxay69YsUJjx47V8uXLlZycrKioKF199dU6ePDgaVfehG/2HzVdBQAAzikOy7KshpwQGxurYcOG6YUXXpAkud1uRUVF6Q9/+IOmTJlyyvNdLpfatm2rF154QePHj6/XPfPz8xUaGqq8vDyFhJx8g7LTET1l0SnLhLd26ttH45utDgAAnCvq+/e7QT0j5eXlSklJUXz88T/GPj4+io+PV3Jycr2uUVxcrIqKCrVr167OMmVlZcrPz/d6nS1yCstMVwEAgHNKg8JITk6OXC6XIiIivI5HREQoIyOjXtd46KGH1LlzZ69A82OJiYkKDQ31vKKiohpSzWYXPWWR7vlvikorXKarAgBAi3dGV9PMmjVL8+fP18KFCxUYGFhnualTpyovL8/zSk9PP4O1rJ/FmzLUZ9oS09UAAKDFa1AYCQ8Pl6+vrzIzvffcyMzMVGRk5EnPfeaZZzRr1ix99tlnGjDg5E/JdTqdCgkJ8XqdCXN/M+S0r5FdUKYDR4qaoDYAANhDg8JIQECAhgwZoqSkJM8xt9utpKQkxcXF1Xne008/rSeeeEJLlizR0KFDG1/bZnZNv5MHqtokLt7m9fOwJz/XZX9boSPMLQEAoF4aPEwzefJkvfLKK3rzzTe1bds23X333SoqKtKECRMkSePHj9fUqVM95f/6179q2rRpmjdvnqKjo5WRkaGMjAwVFhY23bdoQhd2algvzL9W7a31+O6ss/P7AQBwtmlwGBkzZoyeeeYZTZ8+XQMHDlRqaqqWLFnimdSalpamw4cPe8q/9NJLKi8v1y9+8Qt16tTJ83rmmWea7ls0ofuuPN90FQAAsJUG7zNiwpnaZ0SSyipduuDRhk1MvfyCDnrihn6Kahfs2atk/l0X6+Ke7ZujigAAtAj1/fvtdwbr1CI4/XwbfM6KHdka+fRy9Yls4zl29kc8AADODjworxazft6/Uedtzyjw/NsSaQQAgPogjNTilqGnv8navNX7lF9a0QS1AQDg3EYYqYWPj+O0r/H5tiwNeOwzFZZVNkGNAAA4dxFGmlm/GUu1Pu0H09UAAOCsRRipQ4Bf0zXNz19coyJ6SAAAqBVhpA7vTax7R9nGuGjGUo19+Wu1gJXUAACcUYSROgzoGqb9s0Ypql1Qk10zee8RXfnsSs/PeSUV9JgAAGyPMHIKI8/v0KTX25tT9RC9knKXYmZ+potmLK213IepB3XTi1/pcF5Jk94fAICzDWHkFNzuph9WySup0K9f/drzc21DN/fNT9WGtFw9/vHWJr8/AABnE8LIKbibYY5HzMzPtCEt1/PzZ1sz5XZbenPNfm36Ps+rbH5pBcuDAQDnNLaDP4XRMZ31zrffN+s9fv/vFPXrEqLNB/MlVT3XptpXu4+o34ylurBTiD69b2Sd1yirdMnH4ZC/L/kSANCy8KC8etiVWaCubYN14fSGPUCvqe156jr51rIhW3mlWzEzP1NokL++fvhKAzUDAKCm+v795j+j6+H8iDYKCvDV67cPM1qPCW98U+vx/UeKVFLhUkZ+6RmuEQAAp48w0gChwf5G779qZ7b25RRp9mc7tHxHlqKnLNLjH29VbvHxZ+CUlLuUkUcoAQC0HAzTNIBlWeoxdbGx+9dHeOsA5RSWa8WfLld0eCvP8cWbDqtjG6eGRrczWDsAgJ0wTNMMHA6HUqdfpS5hTbcRWlPLKSyXJF3+zAodyq3ao2R3VoHu+e96/WJusiRpxY4spRw4aqyOAACciDDSQGHBAYq/sKPpatTLnf/3rSTpu/Tjy4UP55Xo9te/0c0vJcuyLOWXVtR1uhe329LH3x1S+tHiZqkrAMC+WNrbCOGtnaarUC9bDuXL5bb0x3e/8xybt3qf59/jXl2rNXuO6H93j9D767/XW+vS9JPzO+h3I3vocF6pro/prEB/X0nSwg0HPdfZP2vUmf0iAIBzGnNGGqGk3GV8me+ZcPuIaD12/UWSpD+/951nvxXCCACgPur795uekUYICvDVN4/EKyOvVE98slXr9p+b8y/eWLNfb69LU4CfT415MjmFZbIsqUObltFLBAA4exFGGqlDG6c6tHHqnYlxkqR3v03Xq1/uU9rRYpVUuAzXrumUVbpVVunW9owCz7FKl1tD//K5JGn7E9d4hnJ+zO225FPLJm0AAJyICaxN5JdDo7T0gZ9o88wE9ezQ6tQntGB/eHuD59/Pfb5TRwrL9PpX+xQ9ZZHW7MmRJOUVV2j4U5/rwRPmqwAAUBvmjDST6CmLTFfBmE2PXa0hT3yucpdb0vE5Jv/++oB2ZhTo8RsuksNBjwkAnOuYMwJjpn+4xRNEpKpn++zLKdK0DzZLkkac117X9u9U47w5y3frb0t3qHv7YN17RS/9cmjUGaszAMAchmnQ5BZuOOj181XPrdJd/07x/Hz3f9fri+2ZXmUsy9Lflu6QJB04UqwH39uovJIKlVW6tGjjYa9N2nZkFOi5ZTtVWFbZjN+ibruzCuu9PwsA4NToGWkm/75juO6fn6ojReWmq3JWuvs/63XbiGj9fHAX7cosVMqBH2qUiZn5mdfP+2eN0oEjRUp4fpUkKbe4XLE929e5zf0bX+1TaaVbEy87r971yswvVcc2zjqHkbYeytd1//hSQf6+2vbENfW+LgCgboSRZjLy/A769tF4rdyZrUUbD+ux6y9SWaVbg59YZrpqZ4WySrdeXrVXL6/aW+9z8oordNnfVnh+fjP5gN5MPiBJev32YerQxqn/fH1AvxreTX07heixj7dKkkbHdFZphUu7Mgt0Tb9Onms989kO3TS4iwZ3aytJWrjhez2w4Dt1bOPUHZf20O9rCTGrdmVLUo0VUykHjmrSfzfosev7eu4BAKgfJrCeYdkFZRrzr2T9YmhX3X3ZeWf9g/fOJpf2Ctfq3Tn1Kjuke9tae1v+fcdwjTy/g6a+v0lvr0uTJC174CeyJP36lbXKKSzzlP3f3SM0pHtbr/P/tXKPEj/dLsl787eYmZ8pr6SixvEfc7st7coqVO+I1kziBXDOYwLrWapDG6e++NPlnp+3Pp6ggY8vU3mlu+6TIEn1DiKSag0iUtXzenp1bK3NB/M9x656blWtZU8MJtVOzA8b0n5QVLtghbd2eoLIyby//ntNfqdqqfO9V/TSnxIuOOU5AGAHhBHDggP8tOOJa5RVUKbc4grtyCxQWYVLliX9+X8bTVfvnFNa4fYKIifz3LKdKi6v1AMLqgLE7SOi9b/133s+v+nFNfJxSHsTvXtCCssqtTurUDFdQ716P6qDiCS9sHz3ScNIWaVL5ZVutQn0r1ddAaAlI4ycBRwOhyJCAhUREqgLItt4jke1C9bSLRm66yc9VVbp1hXPrDBXSRvanlHgCSJS1fb4P+a2pOXbs7yO9Zux1PPvSVecp0qXpf87NrflRKUVLuWVVCgiJNBzrLzSracWb/Pca+NjVyuEQALgHMfS3rNY3Hnt9dj1F6lzWJC6tQv2HI+/sKPBWuHHJrzxTZ2fzVm+R/9atbfWRwT0mbZEsU8l6e+f75LbbSk1PVe9H/3UK/RsTM/zOmfBN2mKnrJI+3KKquafZBboxGlfuzIL9PSS7TWGjSqP7fviclsqqGVZcmmFS+vTfpDbbSm3uFxzlu/WwdySen3/hqp0ubUvp6hZrg2gZaJnpAV6ZFRffb4t69QF0SI89/lOPff5zlo/c1mWZny4WW8mH9CtF3fXv7+u6mGprZcs8ef9NfX9TZKkjPxSzfr5AK3Zk6MdGQV6eukOudzHQ8snf7hUP/vnavUIb6Wl9/9Et7++Tl/vPapHR12or/ce0efbsvTW2jR9NeWnkqqWPMc+laTYHu204Pdxnuss2XxYJRUu3TSoqySpoLRC2QVl6tmhdZ3f965/p+iL7Vn6+68G6oaBXRrWWADOSaymaUFmfrxFeSUVevaXMVq9O0cPvrtRf/3FAA3qFqYBjx3fk6O108/YhmA4e3QJC2pw74afj0OVJ4SWP13dW9f276Qrn13pVa5zaKAO5ZV6fr7n8vOUcFGkbnt9nXKLK7T4/41U3861/99q9aMSBnQN1Uf3XlprmayCUr3zTbpuGRqljicMYwFoWer795swco44/5HFqnBV/U+5f9YoHcot0UffHdJvLu6u3OJyudyWpr6/SWv2HJEkTb22j2eJKtAc3psYp9Agf89qpaHd2yq3pEK7swo9ZfbPGqWMvFKVVLi0/XC+krZn6bMtGcovrQrTfj4ObX/iGvn5+uhIYZmnd2fWku0aFt1OA7qGasuhfHVtG6QR54XXWo/ySrf+tnS7urVvpVsv7l5rmU82HtLsz3ZqzrjB6tWxtVbsyNaw6LayLKltq4BazzlaVK7SCpc6hwU1qn0sy5LD4dDmg3k6cKRYowZ4709zOK9EWw/l66d9Op7xZeCWZanSbcnfl5F8nB7CiM2kpufq8Y+36NGf9fVs4lWbNXtytGJHtv509QUqKqvUzXPXKOGiSL20Ys8ZrC1QJbZHO63dd/TUBevhuxlX6+VVe7RqZ45+HdtNY4d303/XHtAjCzd7yiz+fyPVJSxI9769Xnuzi3Qwt0QxXUP13fdVc3NaO/3k5+tQbvHxeTVP3tRP42K764eicn2z/6haO/00ole4p4fnu+lXKzT4+CTjCpdbfj5V4aGuELE+7Qfd9to6TRvdV39+r2rV3Ju/Ha7LeneQVLWJ3s0vJUuqmiP26m3D6tUG0z/crABfHz36s76eY4VllaqodNcZqmpz939StHxHltZMuVLt6jjP7bY0+Z1U9esSqqv6Ruir3Uf0y6Fd5e/rowqXW+WVVXOD+nUJVWmFS0nbsnRpr3Cvtqqv9KPFCgv2b/LVZbnF5Qrw81FwQO0zFizLUsqBH9SzQ+s626E+LMtSdkFZs/byFZVVKjjAt17BtazSpR0ZBerXOVQ+Ps0bdAkjaJD0o8Wa+fEW3Tmyp3p0aKXxr63T9owC09UCWoTJV/XWsq2Z2nQwr84ynUMD1b19KyXvPVJnmc0zE/S7N7/R13u9A1q7VgE6WlSuvp1CdDivRFOu7aO2wQF6M3m/yivd+ma/9746W2YmaO2+I9qXU6wnPtnqOf73Xw3Uv1bu1dGicmXkVw2z3TCws/7w015q5fSTQw5FhgZ6glan0EB9+ecrVOm2FOjvK6lqAvIf3/1OG9JylXa02Ou+Ab4+unlIF729Lt3r+PUxnfXRd4ckHQ9vSzYf1ufbsvSXG/sp0N9XWfmlWrolQyN6heu8Dq3lcltK3nNErZy+uunFNQrw9dHOJ6/1XPNQbom2Z+Tr8t4dtXRLhgrLKr0erpmVX6rnPt+l31zcTRd1Dq3R1vtzinT5sblXM0b3VUigv24e0tWrzNyVezTr0+1q7fTT5pkJnuMHc0u0cke2fj64iwL9fbU7q1C7swp1Vd8Ipab/oIFRbeV77I+8ZVl65IPNemtt1SaL/xw7SKNjOmv59iwdzC3Rby7uruLyyjoDkVRzqX9mfqke/3irbo3rrot7tte2w/m69u9fSpL2PnWddh3rfYwIcWr2sp26eXBXDegaqqyCMt35f99q08E8WZb08HV9dNdP6v+4jMYgjOC0ZRWUaviTSTWO33pxd/Xs0EozP96qGwZ21oephwzUDoAJwQG+Ki6vuTqsIe64tIdeW71PkvTQNX301yXeQ8a7n7xWvR75tMZ5/xg7SIs2HlJkSKDnURAnev+eEQpv5dRnWzP0l0XbPMf/c0esfvPa2lPW6/9+O1zTP9ysXh3b6I5Le2jsK197Pvt/P+2lz7Zmev1H2sTLztND11xQ607aW2YmKHnPET28cJOyCmpuoFibTqGBGtQtTNkFZfpm/w8aF9tNF/dsrz+8vUGStPCeEbIkzflit5KObSnQv0voSUPwqTgc0nX9O2nGz/o2S88NYQRNonrb9P/dHacOrQP19d4jumlwF/n7+uhoUbnaBvtr4n9StHRLptoE+qng2Fj//+6OU68ObRQS5CeHw6FXVu3Vk4ur/p/DsOi2Nf5LDgBg1sJ7RmjQSYb5G4MwgiZhWZaKyl1q7ay7CzG3uFyvf7VfNw3q4un23Pp4wkm7Hcsrq8bVR8z6Qhn5pXogvrei2gVpULe2bO4GAAaMj+uux2/o16TXJIzAiPzSClVUutW+tbNe5UsrXMouKFPUCZu6fZeeq3KXW5UuS8N7tFNOYZkWbjioW4ZGKTigatza6eejtfuO6lcvf13jmr8c0lXvpnxf4zgAoG43D+6qZ2+JadJr1vfvN+u20KRCAv3rHUQkKdDf1yuISFJMVJiGRbdT3Hnt5etTtVX+xMvOU7tWAQr091Wgf9WM8Yt7tlfy1J/q0l7Hl3QOi26rv/0yRo+N7qv7489X8tSf1rjn+mlXaVh0VVfkhmlXeWbJX9a7g35ybDUDANhNMy+sOSl6RnBOWLM7R/O/SdeM0X1rhKG8kgpVuNzycThUVulSp1DvfSHyiiu08WCuRpwXLl8fhw7mlmjh+u91y9Ao3fFm1czzTqGBeuk3Q/TYR1u06WCenv1ljAL9fTW4e5heXL5HV17YUbsyC/X4J1sV26Od7rmil26bt05S1ZLN6n8DwNlq0hXn6cGEPk16TYZpgCaQVVCqN9fs16+GdavRg9OYa73x1X71jmij0TGddd7DVTPwO7Zx6tP7RnpC1CcbD+netzYowM9H5ZVVz5R5b2Kchka30/9Svtcf3/1Oz48ZqPzSCk3/cItuHxGtmwd31XOf79QX23lMAIDGeeLGfnVuDNhYhBHgLJddUKZlWzN1w8DOalXHBOHU9FwVlVXqkhOGolxuy7OHQVFZpde5hWWVyi4o0w/F5eoUGqhOoUGa9N/1+j63RP83Ybiuem6l2rUK0PaMAkWGBGpYj3bqEhakuSv36NXxQ3XlhR31ycbDmvK/jXrgqt7qExmi3JJy3fvWBq96Lbl/pLYdztcDC77TeR1aKTjATx3aOGsNQ7+9pIfmfbVPY4d30w9F5erfNVR/W7pDktS+VYC6tg3ybDp2ovDWAcopLG94wwJolCduuEi3xkU36TUJIwBqcLstORx17wx6MqUVLvWZtkQ9w1vpiz9dXmuZ4vJK/ffrNF19UYS6tg32hKbaVLjcnu3GP9hwUOGtnV57QWyZmSBL0qbv87Q7u1C/GhYlf18fVbrcuuVfycouLNOCu+LUyuknp1/VddbuO6qPUg/pm/1HNaR7Wy3ccFBSVXjam12ktXuPqHNYkG4e0lV+Pg4NfHyZJOlftw5RSblL9y9IVZewIA3v0c5zriTdPiLa8zTlGwZ21s8Hd9VfP92urYfz9YshXbXp+zztyPTeJHB4dDvdfkm0Hnpvowp+9Kyos3V5e4c2TmXXc08MnHuW/+ly9Qhv1aTXJIwAaHE2H8zTz/65WtN/1le/vbTHaV+vPuGr+hkxP1bhcmvVzmwN6d5WYcEn3wq8wuXWjowCuS1LBaWVGnFe+1MGvtIKlydElVa4FXRspVh2QZm2Z+TrkvPC5bYsrdlzRAO7heloYbmKy116eul2Tb6qt/p3CfW6R3ZBmdq1CpDLbenAkSLll1aoTaC/vv+hWMN7tFdrp582fZ+nWUu26amb+qt7+1YqKXdp3f6jKiitUHhrpy7u2V4l5S75+zqUXVimTqFBKi6vVHmlW1/tPqLu7YPVMcQpX4dDH6Ye0sjzwzXm5a91tMi7B2tU/066ok9H3Ty4i9xWVfsUllXqzTX79c8vduuy3h30y6FdNap/JzkcDu3NLpTD4VCXsCAF+Pno7XVp+mp3jmbfMlB+Pg4dyivRpX9dXqMNVz90hef4/+6O09iX1yq2Zzv9+45YSVU9i08v2a41e47oos4hendinOau3Kt/JO2SJKU8Gi+X29Lwp6o2d9z6eIL2Zhfp1S/36ta4aEW3D9btr3+jTQfz9OioCzXhkh763/rvdcUFHXXf/A1as+eItsxMUCunn2Z+vEWvf7VfT97Uz+sRBPtnjVJqeq4y80u1P6dIvj4ODe7eVl3DgtQxJFArd2brtnnr1DuitV4cN0S/nLtGPxRX/e8x+5YYDegaqic+2abbR0TrF3PXqKzSrZUPXq4vd+Uor6RCH6YeVKC/rzae0Lv4QHxvXdc/UlkFZQoL9ldeSYVkScu2ZWrLoXzNviVG0z/coi+2Z+mKCzpo+Y5sXdsvUi/9ZshJf2cbgzACAGh2pRUuvZvyvX7ap6P8fR0K8PU5aXirK/zVR25xucKCA7Qzs0CtnH7q8qOHFNbn2pZlKe1osbq1C/aULSqrlL+vjwL8al9gemIvXn19uStb0e1bNXiumWVZKiirVEgTP4fHlGZd2jtnzhxFR0crMDBQsbGxWrfu5CsF3n33XfXp00eBgYHq37+/Fi+uuXUuAKDlCfT31a0Xd1eXsCB1bBN4yl6k03kCcfW1e0e0qRFE6ntth8Oh7u1beZVt5fSrM4hIatTTi0ee36FRk94dDsc5E0QaosEtvGDBAk2ePFkzZszQ+vXrFRMTo4SEBGVl1T6Lf82aNRo7dqzuuOMObdiwQTfeeKNuvPFGbd68udbyAADAXho8TBMbG6thw4bphRdekCS53W5FRUXpD3/4g6ZMmVKj/JgxY1RUVKRPPvnEc+ziiy/WwIEDNXfu3Hrdk2EaAABanmYZpikvL1dKSori4+OPX8DHR/Hx8UpOTq71nOTkZK/ykpSQkFBneUkqKytTfn6+1wsAAJybGhRGcnJy5HK5FBER4XU8IiJCGRkZtZ6TkZHRoPKSlJiYqNDQUM8rKiqqIdUEAAAtyFn5bJqpU6cqLy/P80pPTzddJQAA0EzqfsZ7LcLDw+Xr66vMzEyv45mZmYqMjKz1nMjIyAaVlySn0ymns/4PWwMAAC1Xg3pGAgICNGTIECUlJXmOud1uJSUlKS4urtZz4uLivMpL0rJly+osDwAA7KVBPSOSNHnyZN12220aOnSohg8frueff15FRUWaMGGCJGn8+PHq0qWLEhMTJUn33XefLrvsMj377LMaNWqU5s+fr2+//VYvv/xy034TAADQIjU4jIwZM0bZ2dmaPn26MjIyNHDgQC1ZssQzSTUtLU0+Psc7XEaMGKG33npLjz76qB5++GGdf/75+uCDD9SvX7+m+xYAAKDFYjt4AADQLJp1O3gAAICmQhgBAABGEUYAAIBRDZ7AakL1tBa2hQcAoOWo/rt9qumpLSKMFBQUSBLbwgMA0AIVFBQoNDS0zs9bxGoat9utQ4cOqU2bNnI4HE123fz8fEVFRSk9PZ1VOo1A+zUebXd6aL/Go+1OD+3XMJZlqaCgQJ07d/ba9uPHWkTPiI+Pj7p27dps1w8JCeGX6jTQfo1H250e2q/xaLvTQ/vV38l6RKoxgRUAABhFGAEAAEbZOow4nU7NmDGDJwQ3Eu3XeLTd6aH9Go+2Oz20X/NoERNYAQDAucvWPSMAAMA8wggAADCKMAIAAIwijAAAAKNsHUbmzJmj6OhoBQYGKjY2VuvWrTNdpWa1atUqjR49Wp07d5bD4dAHH3zg9bllWZo+fbo6deqkoKAgxcfHa9euXV5ljh49qnHjxikkJERhYWG64447VFhY6FVm48aNGjlypAIDAxUVFaWnn366Rl3effdd9enTR4GBgerfv78WL17c5N+3KSUmJmrYsGFq06aNOnbsqBtvvFE7duzwKlNaWqpJkyapffv2at26tW6++WZlZmZ6lUlLS9OoUaMUHBysjh076sEHH1RlZaVXmRUrVmjw4MFyOp3q1auX3njjjRr1aWm/uy+99JIGDBjg2SgqLi5On376qedz2q7+Zs2aJYfDofvvv99zjPar22OPPSaHw+H16tOnj+dz2u4sYdnU/PnzrYCAAGvevHnWli1brDvvvNMKCwuzMjMzTVet2SxevNh65JFHrPfff9+SZC1cuNDr81mzZlmhoaHWBx98YH333XfW9ddfb/Xo0cMqKSnxlLnmmmusmJgY6+uvv7a+/PJLq1evXtbYsWM9n+fl5VkRERHWuHHjrM2bN1tvv/22FRQUZP3rX//ylPnqq68sX19f6+mnn7a2bt1qPfroo5a/v7+1adOmZm+DxkpISLBef/11a/PmzVZqaqp13XXXWd26dbMKCws9ZSZOnGhFRUVZSUlJ1rfffmtdfPHF1ogRIzyfV1ZWWv369bPi4+OtDRs2WIsXL7bCw8OtqVOnesrs3bvXCg4OtiZPnmxt3brV+uc//2n5+vpaS5Ys8ZRpib+7H330kbVo0SJr586d1o4dO6yHH37Y8vf3tzZv3mxZFm1XX+vWrbOio6OtAQMGWPfdd5/nOO1XtxkzZlgXXXSRdfjwYc8rOzvb8zltd3awbRgZPny4NWnSJM/PLpfL6ty5s5WYmGiwVmfOj8OI2+22IiMjrb/97W+eY7m5uZbT6bTefvtty7Isa+vWrZYk65tvvvGU+fTTTy2Hw2EdPHjQsizLevHFF622bdtaZWVlnjIPPfSQdcEFF3h+vuWWW6xRo0Z51Sc2Ntb6/e9/36TfsTllZWVZkqyVK1dallXVVv7+/ta7777rKbNt2zZLkpWcnGxZVlUY9PHxsTIyMjxlXnrpJSskJMTTXn/+85+tiy66yOteY8aMsRISEjw/nyu/u23btrVeffVV2q6eCgoKrPPPP99atmyZddlll3nCCO13cjNmzLBiYmJq/Yy2O3vYcpimvLxcKSkpio+P9xzz8fFRfHy8kpOTDdbMnH379ikjI8OrTUJDQxUbG+tpk+TkZIWFhWno0KGeMvHx8fLx8dHatWs9ZX7yk58oICDAUyYhIUE7duzQDz/84Clz4n2qy7Skts/Ly5MktWvXTpKUkpKiiooKr+/Vp08fdevWzav9+vfvr4iICE+ZhIQE5efna8uWLZ4yJ2ubc+F31+Vyaf78+SoqKlJcXBxtV0+TJk3SqFGjanxH2u/Udu3apc6dO6tnz54aN26c0tLSJNF2ZxNbhpGcnBy5XC6vXy5JioiIUEZGhqFamVX9vU/WJhkZGerYsaPX535+fmrXrp1XmdquceI96irTUtre7Xbr/vvv1yWXXKJ+/fpJqvpOAQEBCgsL8yr74/ZrbNvk5+erpKSkRf/ubtq0Sa1bt5bT6dTEiRO1cOFC9e3bl7arh/nz52v9+vVKTEys8Rntd3KxsbF64403tGTJEr300kvat2+fRo4cqYKCAtruLNIintoLnE0mTZqkzZs3a/Xq1aar0qJccMEFSk1NVV5ent577z3ddtttWrlypelqnfXS09N13333admyZQoMDDRdnRbn2muv9fx7wIABio2NVffu3fXOO+8oKCjIYM1wIlv2jISHh8vX17fGjOnMzExFRkYaqpVZ1d/7ZG0SGRmprKwsr88rKyt19OhRrzK1XePEe9RVpiW0/b333qtPPvlEy5cvV9euXT3HIyMjVV5ertzcXK/yP26/xrZNSEiIgoKCWvTvbkBAgHr16qUhQ4YoMTFRMTEx+vvf/07bnUJKSoqysrI0ePBg+fn5yc/PTytXrtQ//vEP+fn5KSIigvZrgLCwMPXu3Vu7d+/md+8sYsswEhAQoCFDhigpKclzzO12KykpSXFxcQZrZk6PHj0UGRnp1Sb5+flau3atp03i4uKUm5urlJQUT5kvvvhCbrdbsbGxnjKrVq1SRUWFp8yyZct0wQUXqG3btp4yJ96nuszZ3PaWZenee+/VwoUL9cUXX6hHjx5enw8ZMkT+/v5e32vHjh1KS0vzar9NmzZ5Bbply5YpJCREffv29ZQ5WducS7+7brdbZWVltN0pXHnlldq0aZNSU1M9r6FDh2rcuHGef9N+9VdYWKg9e/aoU6dO/O6dTUzPoDVl/vz5ltPptN544w1r69at1l133WWFhYV5zZg+1xQUFFgbNmywNmzYYEmyZs+ebW3YsME6cOCAZVlVS3vDwsKsDz/80Nq4caN1ww031Lq0d9CgQdbatWut1atXW+eff77X0t7c3FwrIiLCuvXWW63Nmzdb8+fPt4KDg2ss7fXz87OeeeYZa9u2bdaMGTPO+qW9d999txUaGmqtWLHCa4lgcXGxp8zEiROtbt26WV988YX17bffWnFxcVZcXJzn8+olgldffbWVmppqLVmyxOrQoUOtSwQffPBBa9u2bdacOXNqXSLY0n53p0yZYq1cudLat2+ftXHjRmvKlCmWw+GwPvvsM8uyaLuGOnE1jWXRfifzxz/+0VqxYoW1b98+66uvvrLi4+Ot8PBwKysry7Is2u5sYdswYlmW9c9//tPq1q2bFRAQYA0fPtz6+uuvTVepWS1fvtySVON12223WZZVtbx32rRpVkREhOV0Oq0rr7zS2rFjh9c1jhw5Yo0dO9Zq3bq1FRISYk2YMMEqKCjwKvPdd99Zl156qeV0Oq0uXbpYs2bNqlGXd955x+rdu7cVEBBgXXTRRdaiRYua7Xs3hdraTZL1+uuve8qUlJRY99xzj9W2bVsrODjYuummm6zDhw97XWf//v3WtddeawUFBVnh4eHWH//4R6uiosKrzPLly62BAwdaAQEBVs+ePb3uUa2l/e7+9re/tbp3724FBARYHTp0sK688kpPELEs2q6hfhxGaL+6jRkzxurUqZMVEBBgdenSxRozZoy1e/duz+e03dnBYVmWZaZPBgAAwKZzRgAAwNmDMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMCo/w/CRXgpIDrZgwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_terms_dict['mse'])\n",
    "# plt.ylim((0, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6f0038e650>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_terms_dict['rounding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ts = diffusion_model.word_embeddings.weight.data.cpu()\n",
    "init_ts = torch.nn.Embedding(len(tokenizer), diffusion_model.config.word_embedding_dim).weight.data\n",
    "\n",
    "trained_ts = trained_ts / trained_ts.norm(dim=1, keepdim=True)\n",
    "init_ts = init_ts / init_ts.norm(dim=1, keepdim=True)\n",
    "\n",
    "trained_cosine = torch.matmul(trained_ts, trained_ts.T)\n",
    "init_cosine = torch.matmul(init_ts, init_ts.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1444e-05)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = len(tokenizer)\n",
    "(trained_cosine.sum() - V) / (V * (V - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15734.2949)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_cosine.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(diffusion_model.state_dict(), \"checkpoints/conditional_from_scratch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['alphas_bar', 'alphas_bar_prev'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diffusion_model.load_state_dict(torch.load(\"checkpoints/20221015_2026\"))\n",
    "diffusion_model.load_state_dict(torch.load(\"checkpoints/20220906_0519\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774ffe570ac14eb89694d8fcef592931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.07535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0753, device='cuda:1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_conditional(diffusion_model=diffusion_model, dataloader=eval_dataloader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc7c1dd919b41779373bd844ea7578f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "generated_questions_mbr5_ddim20 = diffusion_model.generate(\n",
    "    dataset = eval_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=20,\n",
    "    eta=0,\n",
    "    mbr=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15155854506376149\n",
      "0.22320761914790413\n"
     ]
    }
   ],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr5_ddim20, eval_dataset, rev_tokenizer)\n",
    "print(sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"]))\n",
    "print(sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5461694549349757\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr5_ddim20, eval_dataset, rev_tokenizer)\n",
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]\n",
    "print(sum(rouge_l_f)/len(rouge_l_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some ways to help a teenager overcome depression ?\n",
      "How does anyone overcome depression ?\n",
      "How do I overcome depression ?\n"
     ]
    }
   ],
   "source": [
    "i = 51\n",
    "src_question = [rev_tokenizer[id.item()] for id in eval_dataset[i]['question1_input_ids']]\n",
    "src_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src_question))\n",
    "print(\" \".join(src_question))\n",
    "\n",
    "tgt_question = [rev_tokenizer[id.item()] for id in eval_dataset[i]['question2_input_ids']]\n",
    "tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "print(\" \".join(tgt_question))\n",
    "\n",
    "#print(\" \".join(generated_questions_mbr1_ddim200[i]))\n",
    "print(\" \".join(generated_questions_mbr15_ddim2[i]))\n",
    "#print(\" \".join(generated_questions_mbr5_ddim20[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7342301b021940aa883544b8a31e64eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "generated_questions_mbr5 = diffusion_model.generate(\n",
    "    dataset = eval_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=200,\n",
    "    eta=0,\n",
    "    mbr=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18039911311842954\n",
      "0.2751321938279151\n"
     ]
    }
   ],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr5, eval_dataset, rev_tokenizer)\n",
    "print(sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"]))\n",
    "print(sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5874787212343898\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr5, eval_dataset, rev_tokenizer)\n",
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]\n",
    "print(sum(rouge_l_f)/len(rouge_l_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_batch = next(iter(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "generated_questions = []\n",
    "bs = eval_batch['question1_input_ids'].shape[0]\n",
    "mbr = 5\n",
    "if mbr > 1:     # using MBR decoding\n",
    "    batch_questions = []\n",
    "    for cnt in range(mbr):\n",
    "        x_T = torch.randn(size=(bs,\n",
    "                                diffusion_model.config.max_position_embeddings,\n",
    "                                diffusion_model.config.word_embedding_dim))\n",
    "        final_hidden_state = diffusion_model.ddim_sample(x_T.to(device),\n",
    "                                              sampling_timesteps=200,\n",
    "                                              eta=0,\n",
    "                                              src_ids=eval_batch['question1_input_ids'].to(device),\n",
    "                                              src_attention_mask=eval_batch['question1_attention_mask'].to(device),\n",
    "                                              return_hidden_states=False,\n",
    "                                              verbose=False\n",
    "                                              )\n",
    "        sampled_ids = diffusion_model.rounding(final_hidden_state).cpu()\n",
    "        questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "        batch_questions.append([list(filter(lambda x: x not in ['[PAD]', '[START]', '[END]'], question)) for question in questions])\n",
    "    # batch_questions [mbr, bs, question]\n",
    "    for batch_ind in range(bs):\n",
    "        candidates = [one_generation[batch_ind] for one_generation in batch_questions]      # [mbr, question]\n",
    "        bleu_scores = torch.zeros(mbr)\n",
    "        for candidate_ind, candidate in enumerate(candidates):\n",
    "            for ref_ind, ref in enumerate(candidates):\n",
    "                if ref_ind != candidate_ind:\n",
    "                    bleu_scores[candidate_ind] += sentence_bleu([ref], candidate)\n",
    "        select_ind = torch.argmax(bleu_scores).item()\n",
    "        generated_questions.append(batch_questions[select_ind][batch_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_T = torch.randn(size=(batch_size, max_len, diffusion_model.config.word_embedding_dim))\n",
    "    final_hidden_state, hidden_states = diffusion_model.sample(x_T.to(device),\n",
    "                                        src_ids=eval_batch['question1_input_ids'].to(device),\n",
    "                                        src_attention_mask=eval_batch['question1_attention_mask'].to(device),\n",
    "                                        return_hidden_states=True,\n",
    "                                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_T = torch.randn(size=(batch_size, max_len, diffusion_model.config.word_embedding_dim))\n",
    "    final_hidden_state_ddim, hidden_states_ddim = diffusion_model.ddim_sample(x_T.to(device), sampling_timesteps=200, src_ids=eval_batch['question1_input_ids'].to(device),src_attention_mask=eval_batch['question1_attention_mask'].to(device), return_hidden_states=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# src_questions = bert_tokenizer.batch_decode(eval_batch['question1_input_ids_bert'], skip_special_tokens=True)\n",
    "src_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in eval_batch['question1_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(src_questions[sample_idx])\n",
    "    print(\" \".join(src_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# target_questions = bert_tokenizer.batch_decode(eval_batch['question2_input_ids_bert'], skip_special_tokens=True)\n",
    "target_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in eval_batch['question2_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(target_questions[sample_idx])\n",
    "    print(\" \".join(target_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sampled_ids = diffusion_model.rounding(hidden_states[-1])\n",
    "    generated_questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"sample_idx:\", sample_idx)\n",
    "    print(\" \".join(generated_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sampled_ids = diffusion_model.rounding(hidden_states_ddim[-1])\n",
    "    generated_questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"sample_idx:\", sample_idx)\n",
    "    print(\" \".join(generated_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_states_ddim[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_idx = 63\n",
    "for step in [1000,1900,1940,1980,1990,1993,1994,1995,1996,1997,1998,-1]:\n",
    "    hidden_state = hidden_states[step][sample_idx]\n",
    "    with torch.no_grad():\n",
    "        sampled_ids = diffusion_model.rounding(hidden_state)\n",
    "        sampled_seq = [rev_tokenizer[token_id.item()] for token_id in sampled_ids]\n",
    "        print(\"step:\", step)\n",
    "        print(\" \".join(sampled_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_idx = 63\n",
    "for step in [0,150,180,190,195,197,198,-1]:\n",
    "    hidden_state = hidden_states_ddim[step][sample_idx]\n",
    "    with torch.no_grad():\n",
    "        sampled_ids = diffusion_model.rounding(hidden_state)\n",
    "        sampled_seq = [rev_tokenizer[token_id.item()] for token_id in sampled_ids]\n",
    "        print(\"step:\", step)\n",
    "        print(\" \".join(sampled_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_hidden_state2, hidden_states2 = diffusion_model.sample(x_T.to(device),\n",
    "                                                           src_ids=train_batch['question1_input_ids'].to(device),\n",
    "                                                           src_attention_mask=train_batch['question1_attention_mask'].to(device),\n",
    "                                                           return_hidden_states=True,\n",
    "                                                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# src_questions = bert_tokenizer.batch_decode(eval_batch['question1_input_ids_bert'], skip_special_tokens=True)\n",
    "src_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in train_batch['question1_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(src_questions[sample_idx])\n",
    "    print(\" \".join(src_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# target_questions = bert_tokenizer.batch_decode(eval_batch['question2_input_ids_bert'], skip_special_tokens=True)\n",
    "target_questions = [[rev_tokenizer[id.item()] for id in ids] for ids in train_batch['question2_input_ids']]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"idx:\", sample_idx)\n",
    "    # print(target_questions[sample_idx])\n",
    "    print(\" \".join(target_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sampled_ids = diffusion_model.rounding(hidden_states2[-1])\n",
    "    generated_questions = [[rev_tokenizer[token_id.item()] for token_id in sampled_id] for sampled_id in sampled_ids]\n",
    "for sample_idx in range(batch_size):\n",
    "    print(\"sample_idx:\", sample_idx)\n",
    "    print(\" \".join(generated_questions[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(eval_batch['question1_input_ids_bert'][0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer.convert_ids_to_tokens(eval_batch['question1_input_ids_bert'][59], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], target_questions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], generated_questions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bleu_score = []\n",
    "for target, generate in zip(target_questions, generated_questions):\n",
    "    bleu_score.append(sentence_bleu([list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], target))],\n",
    "    list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], generate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum(bleu_score)/len(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bleu_score = []\n",
    "for src, generate in zip(src_questions, generated_questions):\n",
    "    bleu_score.append(sentence_bleu([list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src))],\n",
    "                                    list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], generate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum(bleu_score)/len(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_dataset, rest_dataset = torch.utils.data.random_split(eval_dataset, [500, 12938])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generated_questions_mbr1 = diffusion_model.generate(\n",
    "    dataset = small_dataset,\n",
    "    rev_tokenizer=rev_tokenizer,\n",
    "    sampling_timesteps=200,\n",
    "    eta=0,\n",
    "    mbr=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generated_questions_mbr1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_dataset[0]['question1_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[START]',\n",
       " 'Which',\n",
       " 'fruits',\n",
       " 'or',\n",
       " 'vegetables',\n",
       " 'should',\n",
       " 'be',\n",
       " 'eaten',\n",
       " 'regularly',\n",
       " 'to',\n",
       " 'get',\n",
       " 'vitamins',\n",
       " '?',\n",
       " '[END]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rev_tokenizer[i.item()] for i in small_dataset[0]['question2_input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bleu(generated_questions, dataset, rev_tokenizer):\n",
    "    \"\"\"\n",
    "    calculate BLEU metric\n",
    "    :param generated_questions: list[token_list]\n",
    "    :param dataset: pytorch dataset\n",
    "    :param rev_tokenizer: token_id to token dict\n",
    "    :return: {\"bleu\": val_list, \"self_bleu\": val_list}\n",
    "    \"\"\"\n",
    "    bleu, self_bleu = [],[]\n",
    "    for ind, sample in enumerate(dataset):\n",
    "        src_question = [rev_tokenizer[id.item()] for id in sample['question1_input_ids']]\n",
    "        src_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], src_question))\n",
    "        tgt_question = [rev_tokenizer[id.item()] for id in sample['question2_input_ids']]\n",
    "        tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "        bleu.append(sentence_bleu([tgt_question], generated_questions[ind]))\n",
    "        self_bleu.append(sentence_bleu([src_question], generated_questions[ind]))\n",
    "\n",
    "    return {\"bleu\": bleu, \"self_bleu\": self_bleu}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dingyizhou/anaconda3/envs/gen/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleu_dict = calculate_bleu(generated_questions_mbr1, small_dataset, rev_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15005539419638644"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_dict[\"bleu\"])/len(bleu_dict[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2198190329545948"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_dict[\"self_bleu\"])/len(bleu_dict[\"self_bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which is the better fruits to get eaten at critics ?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(generated_questions_mbr1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_rouge(generated_questions, dataset, rev_tokenizer):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    for ind, sample in enumerate(dataset):\n",
    "        tgt_question = [rev_tokenizer[id.item()] for id in sample['question2_input_ids']]\n",
    "        tgt_question = list(filter(lambda x:x not in ['[PAD]','[START]','[END]'], tgt_question))\n",
    "        rouge_scores += rouge.get_scores(\" \".join(generated_questions[ind]), \" \".join(tgt_question))\n",
    "\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rouge_scores = calculate_rouge(generated_questions_mbr1, small_dataset, rev_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.5, 'p': 0.5454545454545454, 'f': 0.5217391254442345},\n",
       " 'rouge-2': {'r': 0.09090909090909091, 'p': 0.1, 'f': 0.09523809024943337},\n",
       " 'rouge-l': {'r': 0.4166666666666667,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.434782603705104}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rouge_l_f = [d['rouge-l']['f'] for d in rouge_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
